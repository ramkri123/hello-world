{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ab543f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "       \n",
       "    \n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"LLM Bootcamp\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "           \n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "    \n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "   \n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "    \n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "<center><img src=\"https://d4x5p7s4.rocketcdn.me/wp-content/uploads/2016/03/logo-poster-smaller.png\"/> </center>\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9916f",
   "metadata": {},
   "source": [
    "# ASR: Automatic Speech Recognition\n",
    "\n",
    "Automatic speech recognition is also referred to as computer speech recognition, text-to-speech (TTS), and so forth. \n",
    "\n",
    "Collectively, these synonymous terms refer to the conversation of the spoken language -- as sound waveforms -- into the corresponding text of that language. Sometimes, it may include the ability to an implicit translation to another language text.\n",
    "\n",
    "As of circa 2023 end, the state of the art models use transformers. In particular, we will use the `Whisper` family of models for the lab below. These models have shown a steady improvement. While the best of these is a rather large - and sometimes slow on consumer-grade hardware, there have been various distillations and optimations to create speedier variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "314f0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955f3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from svlearn.multimodal import AutomaticSpeechRecognition, AsrModel\n",
    "\n",
    "asr = AutomaticSpeechRecognition(AsrModel.WHISPER_LARGE_V3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b88e2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It's April the 8th, Saturday morning, a beautiful morning. It's more than 50 degrees, which is a really nice thing in California these days. This is our fourth session. The first session was sort of basic and introductory. The second two sessions, we did some real stuff. So I'm going to start by recapitulating what we have done so far. The first thing, broadly, is that we are in the world, when it comes to natural language processing, the one statement that goes with it is, it's transformers, transformers all the way right just about anything you want there's a transformer for it for doing to recapitulate we did a search engine using ai semantic search engine and we noticed that we used a transformer architecture a very interesting architecture we used a siamese network and in Siamese network, we would pass in the way we would create semantic embeddings, train the semantic embeddings by using a transformer. We will recap that and the whole thinking behind that. These transformers produce embeddings, and those embeddings is what we use to do care nearest neighbors. And then we realized that we could do approximate care nearest neighbors and so forth. So we quick, breezy stuff. We'll do it a little bit more in detail shortly. Then we went into the architecture of the original transformer, which is mentioned in the paper. Attention is all you need that paper is landmark paper and in a moment we'll go we'll review that paper too but basically it started the whole transformer revolution this architecture that was proposed in attention is all you need came out in december 2017 by the time people returned from their Christmas holidays and the New Year's holidays, people could sense that the world has changed effectively. Right after that, in very quick succession, a whole lot of things began to come out. There came the BERT, which is an architecture that we did in considerable detail. It's a bi-directional language model. It uses masking of words, random masking of words and asking the transformer to infer what words have been masked. Again, we'll do it in a quick succession. Then came a few months later, then came the sentence bird, which we use, which was literally the foundation of the whole AI-based semantic search. And it pretty much was the next big breakthrough in search engines. People have been doing linguistic search or keyword search for a very, very long time. The theory was really well established. There are mature libraries like Lucene, Apache Lucene, as the core engine behind even larger open source projects like the Apache Solar and the Elasticsearch, which put even more layers of functionality on top of it and make it quite easy to use. They put a whole distributed application stack on it. Now, then came the fact that you could do semantic embeddings and you could immediately see what a vast difference it makes in the quality of the responses. Instead of being keywords driven, this understands the intent. What are you trying to say. It can search for content that is very relevant, but that contains none of the words that you actually mentioned in your text. And it helps you do question answers and so forth. So you could already see the beginnings of things like with these transformers of what we do. ChatGPT is a prompt, it's a question and answer, roughly speaking, framework today. You can see the beginnings of all those ideas with the coming in of transformers and with sentence word. One of the things we did in the lab is use sentence word for QA, question answers. One of the demos that Junaid made was to show how you can load a corpus of diabetes documents in a very short corpus, yet you could ask enlightening questions. What are some of the things you should do to avoid diabetes? And it comes up with very sensible answers, things that were simply not possible with traditional search engines like Elasticsearch. So that, if anything, we started a course with that. The first lab we did was that. And the purpose was to show you how radically the world changes the moment you use transformers. Today, quite a few things have happened. First is this transformers, and using transformers, the large language models, have become bigger and bigger and bigger. As they become bigger, we find that their capabilities also get bigger. They start not only doing things with greater accuracy, but they develop what are called emergent abilities. For example, transformers at the scale of bird, bird base and bird large, they're not able to do arithmetic. You cannot say if I purchased 10 apples yesterday and then today I ate two of the apples, but then my friend gave me six more apples, how many apples am I left with? It wouldn't be able to answer that question. But as the models get bigger, we notice that surprisingly, they are able to answer these questions. Now, sometimes they answer these questions somewhat wrongly, but they try to answer these questions. Now, sometimes they answer these questions somewhat wrongly, but they try to answer these questions. Then came other sort of small set of breakthroughs, a series of breakthroughs. There is the notion of the chain of thought, COT. What it does is it gives you a way to give little breadcrumbs to the transformers, to the large language models, to reason through. And when you give these little breadcrumbs using the chain of thought process, then all of a sudden they're answering the arithmetic questions with fairly good accuracy. accuracy they're almost always right and so with all of this we realize a larger theme with transformers we are realizing especially in conversational transformers or which are becoming quite dominant these days how or what input you give the response seems to be or what input you give, the response seems to be dependent on that, obviously, but the quality of the response often depends on how you pose the question. So prompt engineering or prompt construction has become pretty much a cottage industry. On the web, you find a lot of websites literally devoted to the ways you can create prompts. I just ordered a couple scenes, a screen. What are we seeing? Are we seeing my drive? Yes, we are. So let's say that lost landscape visualization. So there is this paper, which I would encourage you all to read. perhaps given time I would like to cover it. But I'll just give you a very quick preview of this paper. This paper brought out these beautiful diagrams of how the lost landscape looks and with a lot of pictures here, but what we are going to do is see it in real life. So there is this website called lostlandscape.com. I would suggest you go and look at it. And when you do look at it, you will see that these, let me go to one of them. Oh, we are not able to play with this. There was a way to play with this. Explore. All right, guys. So it is loading. Just to recap, what is a loss landscape? It shows you the loss with respect to the parameters of the model. Now you know that these deep learning models, they have hundreds, thousands, or millions, and now million looks too kiddish almost, kindergartenish. These days, you're talking billions, and now trillion. But GPT-4 is supposedly a trillion parameter model, right, from what we know. We obviously don't know all the details about it, but we do know that it is so. Now you realize that when you look at this, there is a clear minima. Do you see at the very bottom, that pink place where the mouse is? There is a very clear minima. But at the same time, the lost landscape has this hills and valleys. And if you look inside it, and you imagine that you start somewhere up in the mountains, a random point, and you try to gradient descent, it is quite likely that you might get stuck in one of those little ridges out there or in one of those little depressions, little valleys along the way, way up in the mountains still. And there is no guarantee that you will necessarily go to the minimum, the global minimum. You can get trapped in local minimum. And so this used to be considered a major problem with neural networks. Today, we don't think of it so much like that, because we have developed pretty good techniques to escape out of those local minima and make our way towards the global minima, or at least something that's close to the global minima. You don't have to be at the absolute global minimum, but you do get to something that is practically useful. But you see that these deep neural networks have a very fascinating and rich loss landscape. Now, with these landscapes, the next thing is the concept of attention. Before attention came, people used to do natural language processing mostly with LSTMs, which are related to recurrent neural networks that are autoregressive or GRUs and so forth. Now, going back to what we said, so let's take the quintessential case. You want to translate from English to let's say French right or to Hindi or something there are two phases to it you take the input and you encode it you encode the sentence into a hidden representation an abstract representation that abstract representation has a certain amount of memory associated to it and then the decoder decodes it and forward generates the French representation or the Hindi representation and once you have an abstract representation a hidden representation you could do many things for example you could translate it into French you could translate it into Hindi but you could also do things that are similar but not probably so obvious you could translate it from plain text to poetry, right? To a sonnet or to a haiku. You could translate it into, I don't know, things like that. You could do it from one style to another. You could do a summarization, translate it into fewer words and so on and so forth, right? So you could do many things with a hidden representation. The trouble is when you use RNN style models, because data is input sequentially, the hidden state overwhelmingly represents what it has learned from the last few tokens. And it tends to forget what was learned from the earlier tokens. It sort of begins to erase. Very much like human beings, you realize that when we learn composition or diction in schools, I don't know if it is taught. We'll ask the young man here, Abdullah, whether it's still true. One of the first things we are taught is to write short sentences for clarity. There has to be a reason to use long sentences. Why is it? There are people, for example, there was a great British writer, Matulay. His sentences were proverbially long. It could start from the beginning of a page and it could go to the end of the next page as one sentence. He was famous and yet apparently it was wonderful writing sometimes. We don't practice that anymore. One of the reasons is our attention spans are smaller, but more than that, it is far more likely that given our short amount of memory or attention span, by the time you reach the end of that sentence, you have forgotten exactly who it is that we are talking about. Right. So same thing happens with this recurrent neural network based models like LSTMs and so on and so forth. the neural network based models like LSTMs and so on and so forth. So the other problem with those architectures were that they could not be, they could not really benefit from the whole deep learning revolution. You couldn't create deep layers of recurrent networks. One or two layers is about as deep as you went. Like not that was another, the third problem was in the computer vision space, already the people were benefiting from the accelerators, hardware accelerators, the graphic processing units and the tensor processing units. So already you were having all these companies like NVIDIA getting rich. So already you were having all these companies like Nvidia getting rich. So, but in the NLP world, there was no use because that parallelism couldn't be so well exploited. It was a recurrent neural network. You had to pass in one token at a time. Now the sequence of words had to be given sequentially. What transformer architecture did, or the big thing it brought is, it took the idea of attention, which had already been discovered, it was discovered in 2014, and people realized that attention is a useful concept. They were hybridizing the attention concept with recurrent neural networks and LSTMs. And when they would hybridize the two ideas, they would superimpose attention on those, already they were seeing much better results. Right? Then, so the general belief at the time was that yes, RNNs and LSTMs are the workhorses, but there are ways to put it on steroid and attention is one way to put it on steroid. And then came the paper of 2017, which basically said, you know, surprisingly, you can do NLP, you can do an encoder-decoder model architecture simply with attention heads. You don't need RNNs or LSTMs or GRUs at all. That's a pretty remarkable statement because that was the choke point. That was what forced you into a sequential way of thinking, right? And that is what prevented you, in some sense, from benefiting from hardware acceleration. So when you could put things through the attention head, it could all go in parallel. Not only that, you could have multiple attention heads. So you could look at that whole process of encoding from different perspectives in a way. One was more syntactically closer. One was looking more, I mean, to use common language, though it's not really like that. It's much more abstract. But sort of a rough intuition would be that one attention head is more dramatically sensitive. Another is more focusing on some other aspect of the input. And so they are working all in parallel, all of these heads, multi-head, attention, they're all working in parallel. And then, of course, you throw in the usual feedforward networks and a few residuals here and there, right? They're like peanut, like I said, one good way of thinking about them, or I think about them, somewhat wrong, or a bit tongue-in-cheek, but it works for me, is I think of the feedforward layers always as the peanut butter and jelly that you put in sandwiches, right? Why do we need them? Just like you don't want to eat a sandwich without them in the morning, you need them because obviously they are the ones that insert non-linearity, right. So if you want to have ultimately the universal approximator function behavior, you need feedforward networks to introduce your non-linearities. And you do that to create then better representation learning in the classic sense. They will take the input that goes into the feedforward and basically try to create higher order representations out of it. So that the subsequently have benefits from the higher order approximation. So that was the attention, that was the architecture or the basic idea of the transformer, which was the original transformers are encoder decoder architecture. It has an encoder that produces a state that goes into the decoder. The decoder is autoregressive in some sense. It takes the hidden state, produces the first word, then that first word is fed back and along with the hidden state to produce the second and so on and so forth. And it goes on producing word after word after word, right? So remember, remember these transformers are if you use the full auto the um the transformer the full architecture of the transformer you know that suppose you're generating text out of it whether you're doing a translation or whatever it is one word emitted at a time. You see that, right? You sort of generate one word at a time and you move with that. Just pause and think what it means. We are all so mesmerized with the latest developments of the large language models today, okay? Or people are, as you know, Microsoft wrote a paper called Sparks of Artificial General Intelligence. They claim they are seeing in GPT-4, which they have been playing around with a few. It's a pretty large paper. I believe it weighs in at 154 pages. Most of them are examples of prompts and their responses. It's a very easy read, but really enlightening read. But somehow, it does make the researchers ask the question, are we seeing sparks of AGI? I don't think so, frankly. I feel that we are not seeing sparks of AGI but we are certainly in a land where we are seeing these large language models produce things surprisingly at the next level to the kind of task specific performance we were seeing so forth. If you look at the so-called call zero-shot learning that we do, there is always sort of a trick to it. For example, we noticed how with sentence embeddings, we could create, or with the classifiers, we could do, so zero-shot classifier, how does it work? You train a classifier to find some labels, A, B, C. But then how do you do, then you say classify into PQR. But what's the hidden trick we use? We know that A, B, C, and PQR we can embed into a latent space. And then instead of, if you predict A, we just find, okay, amongst PQR, which is it nearest to, right? The statement, because the statement text is also going to go into the same embedding space. You go into the embedding space and instead of looking for proximity to ABC, you look for proximity to PQR, right? And that's a trick you use. So behind all this zero-shot learning, there's always a like there's always a trick like for example for the clip paper uh did i cover the clip paper in this one no so we'll do that maybe today we'll do that so what you do maybe today i'll use literally for clip so what we what they did is they said this is zero shot learning we didn't need image and its labels to train to train a transformer to learn from images and its labels to train a transformer to learn from images. True, they didn't have to use an army of people to set and label images. But, you know, an army of people have already done that. Because every time you create a web page, as you know, as a web developer, good practices, you put an image and you put a caption there. Alt, A-L-T, I-M-G, S-R-C is blah, A-L-T is blah, right? You're putting a caption there, alt, text to it. And so what Clip did is it just harvested all of these images with captions. And so it had label data. So when you, and they call it pre-processing or pre-training before you do the thing. But in reality, you end up with label data. So there's always a little bit of a trick. Now with this very large language model, so you are beginning to see emergent abilities, like the ability to do arithmetics and so forth. We are not really training for that. So it is a little bit of an extension. But we have to ask this question. Let's see, as I said, a transformer produces one word at a time in this autoregressive framework. And the next word it produces is conditional on the words it has produced and the context it was given the input. It is duly conditioned on both, right? So it is one word at a time machine. A one word at a time machine has not thought out the entirety of what it is going to say. And yet intelligent creatures, like for example, would you ever talk seriously right or perhaps to your boss or to your spouse right by first thinking one word and then thinking okay what should i say next and then the the thing you would get is watch what you're going to say next right so uh that's not intelligence has this thing that there is a there's a conceptualization and out of the conceptualization there is a verbalization of the concept with at this moment at least there is no evidence that it is a truly conceptualizing what it is going to say in one shot and then translating it into words. You may argue that it is so, but at least there's no clear evidence that it is. So to me, it is still one word at a time machine, right? And that is why I'm skeptical that these things are AGIC or even have any, but certainly they can fool us that they are convinced, they're far more intelligent than the transformers we are used to. And they are, like when they say they are sparks of AGI, well, not quite wrong either in the sense that it is impressing us. It's making us feel. And, for example, in artificial intelligence, there was this concept of the, what is it called, the Chinese room experiment, right? The evidence of weak. So first people thought that a strong intelligence, AGI, I mean, general intelligence has two categories, AI. The belief in strong intelligence means you have to show thinking. We don't even know what human thinking is. How can we conceptualize machines to think? There was historically, by the way, a company in Silicon Valley literally called Thinking Machines, a very high flying machine with Minsky and the great AI gurus sitting there. Noble laureates came and worked there. That company is no more, it broke into two pieces. The hardware part became Sun, the software, the machine learning part went into Oracle, and then Oracle ended up eating up Sunny also. So now both of those groups are in Oracle, except that they're different. The machine learning group is sort of, it is embedded, limited, it's embedded into the kernel, into the database kernel. And Sun is still doing well, reasonably well. So the weaker form of intelligence that people put, AI, is that if you put a curtain and you ask questions, and if a human being is asking questions from one side of the curtain and cannot tell whether the other side of the curtain is a human or a machine, or is fooled into the belief that it's a human, or you say that the machine has achieved a weak form of artificial intelligence. So we are in a very interesting place at this moment. So anyway, this is a small digression, and I'll move past this quickly. We're in a very interesting place. I don't know if you know that every single day, I think Amazon mentioned that their, what is that called, Alexa. Alexa receives dozens or hundreds of proposals, marriage proposals, right? From lonely people who are absolutely sure that these wonderful, wise, empathetic, loving answers could only come from somebody with a beating heart, with a large heart. Right? People want married, aren't they? Yeah, that's right. Even now, people have gone crazy with child abuse. They are committed suicide because of chat GPT before some response. VINOD MARUR, Oh, really? Praveen is mentioning something very interesting, for those of you who are remote, that similar behavior is happening with chat GPT to the extreme that people, they're engaging with chat GPT. And when they didn't get the same amount of a passionate response from chat GPT, a romantic response, they are committing suicide, right? So it speaks to how much these machines are fooling us into the belief that they are intelligent, right? So that whole question of weak intelligence, are we already getting them? The question is, who's asking the question? If you have a researcher ask the question, in two minutes, they come to the conclusion this thing is done, not even close. I remember when Chad GPT came out, I asked a question, which it turns out a lot of other people asked. I asked a question, and actually somebody here in our audience, Chanda asked this question, not me. He asked this question, why seven, not a prime number? And it immediately answered to Chandler that seven is not a prime number because of course, a prime number is divisible by other numbers, is not divisible by other numbers, except one itself. And seven is of course divisible by three and nine, right? Which is utter nonsense, right? So you see this one word at a time coming out, a sensible word. If you don't know arithmetic, if you don't know what frames are, it looks very logical, isn't it? But it was wrong. And now you ask the same question and it gets it right. So obviously there's a human in the loop somehow improving its training. No, more often they say it's memorizing. It sort of memorizes. That's the other. Yeah, because they ask people this one, the goat, grass, and that. Initially, first they did not give the right approach. They fixed it. They fixed it, yes. So, yeah, Prabin gave another beautiful example. Remember the story that a man has a goat, a lion, and some food, right? Now, it has to protect all three and cross a river. The goat will eat the grass, and the lion will eat the goat, right? So how do you cross it without either of the two mishaps happening? And there's a way to go sequence to go back and forth. When you give it a chat GPT, in the beginning, it got it wrong. It's a logical question. But after some time, it got it wrong. It's a logical question, but with, after some time it started giving the right answer. So two, two ways of looking at it. One is that somebody notices the wrong answer. There's a human in the loop. Now they're fed in the right answer as part of the training data, because it's going through continuous training and it brought, brings up actually a very deep question. Are these machines memorizing the answers? Because they have so many parameters, they have a trillion parameters. Can they effectively memorize? Now it turns out one can mathematically show that they don't memorize. At least they don't brute force memorize, but something else happens. See, if you train them on a certain kind of data, right they are making the condition on that the weights have shifted the parameters have learned from that what will happen if you ask a very similar question or that question it is predicting the next word at a time and the correct behavior should of it should be that given the weights, it should produce a response more or less like what it has been trained on, right? Because that would be the path of least error that would give you the least value of the last function, isn't it? And so that brings up the whole question that given the way the weights are, effectively, for unique questions, if it produces exactly the answer that is correct, right? Almost it looks like regurgitation. Has it regurgitated? Has it memorized? How can it regurgitate unless it has memorized? This again came out, I think one of you here were pointing out that somebody put in the source code of Samsung or something, a snippet of the source code and then it emitted out the rest of the source code, isn't it? I clicked it like someone else should have been able to see. I don't understand. What was the input? They basically like inputted the source code. And what I heard was um so instead of telling them that it leads somewhere else oh so in response to another person it regurgitated the source code yeah this is it right because source code is so highly structured that when you give long constructions of source code it has no choice but to adapt its ways learn learn from it. But then when you give it any prompt that looks similar to that, it has no choice but to essentially try its best to reproduce it from its ways. Even though it hasn't memorized it, it has in some sense optimized itself in such a way that it will emit it out. Right? Yeah. So that's why they're binding in some places. Like Walmart, they said they're binding in certain places. Yes. ChatGPT. Yes. So companies, Praveen is mentioning, companies are banning it in the use of ChatGPT in corporate environments. The last thing they want is people to feed it more and more of their secrets. So, Albert, go ahead. ahead so you know going back to the assignment so you have this short sentence so that's basically the end of the presentation so is that similar to the attention also that we talked about the same we talked about attention the tools of attention or? Yes. Yeah, Albert's question is that we gave short sentences to these transformers and then it created these embeddings. How is that related to attention? The way it is related to attention is, attention was the mechanism used to see the relationship between the words, the semantic relationship. And because you're embedding it in a space where two sentences that are semantically similar should be nearby, right? So what does it mean that their attention heads should emit out a vector that are close to each other? And so attention is the mechanism that is used to get semantic understanding and project it into an embedding space close to each other. Right? All right, guys. So with all of that, we are in a place that transformers are on a roll at this particular moment, right? And for better or for worse, we are doing that. Now, what we did is I'll go through the very quickly, the theory that we went through we did the initial original transformer then we said that when you see transformers in the wild you notice an interesting pattern some transformer architectures they use only the encoder part of the classic the attention is all you need transformer the full encoder decoder architecture. BERT family in particular utilizes the encoder aspect of it, is predominantly encoder based. The GPT family is predominantly the decoder part of it. Are we together? And that is why they are one word emitting at a time. Then what's the trade-off between these two? Let's think of a trade-off between using and .. And then, of course, there are architectures, transformers, that use both. For example, BARD uses both. Not Burt, BART, B-A-D, uses, is it Bart? Bart, yeah, uses Bert and so on and so forth. So there are many architectures, there's a mix. What's the trade-off between these two? I don't think I quite spoke on that, so I'll speak on that. See, when you use Bert, if you remember, how did we train Bert? When we reviewed the BERT paper, we noticed that the germ, the main crucial idea was, you take the encoder password, the transformer, and then, and of course you make it bigger, you have six heads, 12 multi heads and whatever, multi attention heads and all of that. But at the end of it, the embedding was that, the word would get embodied, tokenized and embedded. It would go through the embedding and even the embeddings were learned. You force the transformer to learn the embeddings. Then there was a segment embedding saying which of the two, you give two segments at a time, two sentences at a time, which of the two sentences a word belongs to. And the third was position embedding. What is the position of this word in the sentence? So position embedding comes from the classic transformer idea. Now you concatenate all of these three to create the full embedding of a token. The actual token embedding plus segment embedding plus position embedding is the complete embedding for that token. Now you take a sentence with all its tokens, you put a special token CLS at the beginning and a special token SEP between the two sentences, right, and then you shoot it through the bud, right, through the attention heads and some glue, some peanut butter and jellies there, feed forward networks there, and then you get the output state. Now the whole question is how do you train it? You need some task to train it. So the task that they picked was a masked language model. What it means is, that's a really mouthful of a word for something quite simple. You would do what children are taught to do in schools, fill in the blank, right? What do we do? We know that the capital of California is blank. And now you have to guess what the capital of California is. We have all been through these exercises. And so it's like teaching that. It's a very effective educational exercises. And so it's like teaching that. It's a very effective educational methodology. And that's exactly what they did. They would take a vast corpus of documents and randomly suppress about 10, 15% of the tokens. They would put a mask. And then they would say, ask the transformer, what was that? What was this? Guess which word was actually there and for it to guess what word it is there it's a classic thing that you can only do with attention because it needs context remember the when we did the theory of attention attention is based on what attention is there is a context that forces you to put differential weights on differential tokens of the input. That is the gist of the attention. To recap again, when you are walking on a hike, you're enjoying the mountains, you're enjoying the stream going by in the trail, and you're talking to your friend. And all of a sudden you spy you hear a sound and all of a sudden the sky has disappeared and the stream has disappeared right and the only thing your attention is for where is that where is that predator isn't it so you know your whole visual field is narrowed to something else a search for something else. So the context determines your attention, where you put emphasis, and that is attention. That's the main idea of attention. Now, when you put a mass word guessing, it's a classic use of attention. You have to pay attention to the rest of the words to a differential extent. You have to understand the semantics of the words in some sense to be able to tell what word was masked. So when you train it like that, what can you tell? This task is not something that you would actually use in real life. Maybe you would use, sometimes you do OCR in some words, and you may try to guess what word it is. But in defect, you do such real things as, is this document legal document or sports document? Right, or something like that. Use it for classification, use it for summarization, use it for all sorts of things. It turns out that when you train bird for this it is able to do very good classification for classification it's good because to classify text what do you need you need to pay attention to all the tokens that are fed in right or for entailment like what does one follow the other? You need to pay attention to both the sentences and move forward with that. So for that, the encoder architecture works pretty good. Whereas if you're doing generative tasks, like generating a passage of text, right? You realize that the autoregressive is better because you use the decoder part of it and it keeps on emitting word after word after word and keeps generating for you so that is one way of separating the two parts out see when you are trying to generate one word after the other chat you probably want to heavily emphasize the decoder because it will keep emitting word after word after word. It hasn't been, BERT hasn't really been specifically trained for such tasks. Isn't it? So that's the difference. Which is this classification in the encoder part that everything is open and afraid? That's right. You said that EPP focuses on that. And what other model? But it focuses on the encoder part. See, what happens is we talked about. And again, this is a recap. I mean, you can use GPD also for decoding. But I'm saying a bird, for example, is very good at finding mass width because it literally is what it's trained on, on whether these two sentences are similar. Similarity analysis, perfect for the encoder part of the architecture, because literally that's what you trained it on. Because what is the CLS token? What do you do? You feed it into a classifier. You can ask it a question, give it the probability that these two are similar or tell what is a mass word. It's literally very close to what it was trained on, sentiment analysis, isn't it? Whereas the decoder part, what is it well trained on? Translating from English to French, generating the text. Right? That is where decoders are good. So that's a very rough way of doing it. Of course, you can sort of hack one into the other and use it, but that's a simple dichotomy. Both use transformer, but they focus on only the encoder part because once they use transformer they transform both encoder and decoder. No actually if you look at the BERT architecture, decoder is practically not there. It's only the encoder. But they use all transformers. No they don't use the whole encoder-decoder at all. Look at the bird. You'll see mostly is the encoder sitting there. See what do you do? You take this word segment and all of that pair, shoot it through the attention heads, throw in your feed forwards, maybe more attention heads feed forward, but ultimately sandwiches of those, right? And then at the top of it, what do you do? You get a hidden representation, the CLS and you feed the CLS straight into the classifier. That's it. They don't use the Vanilla Transformer but there are certain transformers that use the whole of the transformer. But the most common ones that you hear about, they're actually using one or the other right so you can ask a quick question hey for the gpt don't they need some kind of encoding before they get to decoding part or oh yes yes the tokenizing and embedding of course you do and that is learned see remember that even in the decoder right decoder is nothing but the whole encoder, right? The input is not coming from generally, like in the classic architecture, input is not coming from this part. User, it is actually coming from the hidden state and just a trigger, you know, just trigger. The trigger will produce the first word. You feed the first word back into the as input along with the hidden state and then it produces a second word. Now you take the first and second word feed it back into it, right? And like you keep moving forward like that. Now when you only use the decoder what you do is you actually it is nothing but the encoder but with the loop like whatever output it produces, you feed back in. Yeah, that makes sense. Because when I was reading the attention is all you need paper, I did not understand. Thank you for clarifying that because they've added on the output embedding, they've added a mass multi head retention. Yeah. I was thinking, why did they add a mast on this side? Right. The multi head. So, yeah. Yeah. Is that why they have added it that is right the mass multi-head is simply to mask the words that it shouldn't pay attention to you know the future the words that are that are not yet you know the the like what it has translated is just one word so far right so that is it so So that is your classic architectures. Now we went. Then why do we say that it's not encoder and decoder and only decoder heavy? GPT. Why don't we do this? Let me do the GPT. I've done BERT. Were you there in the BERT session? When we did. I was there. I was there. So BERT you saw it. It's encoder heavy. Yes, that I understood. Why don't I do this? I just realized that I have not done the GPT architecture in detail. Why don't I do that? Because that will clarify all the doubts. Oh, thank you so much. Let me do it. And if you're planning to do that later in some other session, that's also okay. I don't want to detail this session. It's okay. We will do that. Either today or tomorrow or either tomorrow, today or the next time, we'll definitely do that. Is generator and destructor the same as importer and decoder? Generator and? Destructors. Destructors. I've not heard of destructors. Like in GAN. Oh, GAN. GAN are generators and discriminate. Discriminate. Yeah. So what happens is that, okay, so question here is, what's the relationship between the generator and the discriminant or discriminator of GAN to encoder-decoder? Actually, you don't think of it like that. You don't try to map one to the other. I'll explain to you what it means. See, in a GAN, this is a quick digression. A discriminator is just a classifier. Let's say that the basic quintessential idea is, suppose the generator is basically a counterfeiter. Who has to counterfeit the currency of a country whose currency it has never seen let us say that you all of a sudden decide that you want to count it's a really good idea to counterfeit bhutani's currency right but you have never seen the bhutani's currency right So what happens is that you have a discriminator which is watching out, is the police in some sense. And when you give it in the beginning, you start training both of them. Now, let's say that even the cop is rather naive. So you first give it a genuine Bhutanese currency and it makes some random probability guess. It says it's a genuine or fake. That produces a loss error function. There's an error there. And you back propagate the loss. And so what will happen? The discriminant weights will shift and learn something from it. Right. What about the generator loss? It also figures out that something is up. Right. It also shakes a little bit. But then the generator asks to produce a currency. It has no idea what Bhutanese currency looks like. It produces some random noise. It will make a picture like it will just give that and say this is it. Right. So, I mean, to make it a little bit more colorful, not that it does that, it produces random noise in the beginning, but let's say that it puts a peacock on a paper and says, this is Bhutanese currency, right? Then even that naive cop knows that this cannot be Bhutanese, so it gives it a probability and it shoots it down. Now you back prop the error, right? Now what happens? Suppose it said it's fake. The discriminator has now gotten a very small loss. It's happy. There isn't that much of things to propagate. It's sort of, it's feeling happy with the small losses. Right, it says my weights are doing well for this fake thing. I caught the fake, right? And it's patting itself on the back. But what is the discriminator telling? I got it wrong. I mean, what the generator thinking? Oops, I got caught. So it will change its ways. So next time it won't produce a peacock. It will produce maybe a horse or something like that. I mean, it will slightly change. It will generate a different... But the amazing thing about gradient descent and backdrop is very quickly the generator without ever seeing Bhutanese currency will learn what a Bhutanese currency looks like. And it will start producing it and giving it to the discriminator. And the discriminator will also gradually smarten up because it's not so naive cop anymore. It has learned to tell the difference between the genuine one and the fake one. Why? Because it's also getting genuine instances and the times that it gets right, it's told it's right. The times it gets wrong, it's told it's wrong. Right? And so forth. So it, and likewise for the fake ones, you randomly give genuine fake, genuine fake, and you train the discriminator to tell the difference apart. And so it's a cat and mouse game. The generator learns to produce more and more realistic fakes and the discriminator as a classifier gets better and better at telling them apart. So they have their own encoder and decoder feedback loop within them? Yeah, they have a feedback loop, but you don't think of it as encoder-decoder because generator is not producing a hidden representation. When you encode, you expect an encoded vector coming out of it. In a way, the generator does, but then the real data, it is trying to imitate the real data rather than some encoded thing. So it's literally trying to imitate which is the purpose of GANs right GANs can produce infinitely many human faces that don't exist why because you have trained it to yeah you have trained the GAN to recognize human faces so if you give it a horse it will say no right so what happens gradually the generator begins to get smarter and it produces more and more human-like faces. But once the GAN has been trained, what's the whole point of a generator? A generator, it becomes a generative model that can produce infinitely many faces after that. Isn't it? It can go on producing faces. And that's an amazing thing for something that started with just knowing nothing at all right and now when we talk about segment the meta paper the segment anything is that also based on that the which paper the segment the segment anything paper that metaa just released. Oh, no, I have to read that paper. I'm sorry, I haven't yet. I'll read that. But thank you for pointing. Actually, let's post it to our Slack network. I'll definitely read it to my team. Okay. So that's sort of a summary. Now let's go to semantic search. What do we do in semantic search? How do we train to produce meaningful embeddings? You deliberately take paired triplets, right? You take a corpus of triplets. You take three sentences, right? Two sentences are genuinely together. They're similar. Right? And the third sentence is deliberately some random sentence from the corpus. Right? And so you say that the first sentence is a reference sentence. The second is similar. And the third is dissimilar. Known to be dissimilar. Then what you do is you pass it through a transformer to create the hidden embedding, the embedding. And then what do you do? You look at the cosine distance between the reference and the similar statement. And you look at the cosine distance between the reference and the dissimilar state. And then you put a few classifier layers. And what do you want your classifier layers in the softmax to do? You want the classifier layers to minimize, I mean, to maximize the cosine similarity between the reference and the similar statement and minimize the cosine similarity between the reference and the dissimilar statement. Isn't it? Minimize the similarity. Cosine is a measure of similarity. So maximize the distance, minimize the similarity. And so you can therefore easily construct the loss function associated with it. And that is the basic idea. You see how simple that idea is. And yet that simple idea helps you create embeddings for sentences in such a way that basically you have, in one moment, it creates a revolution in AI search. In fact, the first big step forward since the days of keyword search, right? And that's the beauty of transformers. Like once you get the transformer idea right sometimes big ideas big changes can be done with what in hindsight looks like simple you know ways of looking at it now to complete that discussion of similarity uh so like semantic similarity we we took this idea that see isn't it ideal if we trained a bird to take two sentences two segments and pass it simultaneously together to have cross attention between them right it's a cross encoder effectively and let's say that you put a classifier the probability that you produce the larger potentials that you produce, should be proportional to how similar these two statements are. Quite literally, you can train it on that ideally. So why don't we use that for similarity analysis? So suppose I have a corpus of documents, of sentences. I get a query sentence. Why can't I look for similarity against all of this corpus of let's say, a billion documents and find out which ones are the most similar, which ones produce the most. The problem is inference from a transformer is expensive. And for every query, if you have to do a billion comparisons through a transformer, you'll go bankrupt and the cloud providers will be smiling, right? So you don't want to do that. So the idea was the intermediate stage, and that is the whole point that the sentence bird does, it says that, no, what we do is by training the sentence bird or like two sentence by using this triplet siamese network we just create embeddings right embeddings that are semantically closer if sentences are closer and what we store are embeddings so now when i get a query i convert the query into an embedding and i'm not searching into a cartesian space i don't need a transformer for that i just need to do cosine distance comparisons, which are way, way cheaper than transformer inferences. So that is good. Then we go one step further. We know that when you do embedding, something is lost. I mean, it is never as precise or accurate as doing direct inference through a transformer like that. You lost something. But you lose a little bit more because even to do vector comparisons against or cosine comparisons against a billion documents is not the wisest idea. So then we brought in the idea of approximate nearest neighbors, which somehow bucketizes all the points or it partitions the embedding semantic space into sort of partitioned areas so that when you get a query, you go into that bucket and you just look for or you search for neighbors only in that bucket. The trouble with that, and we use all of these ideas, quantization of space and many, many ideas, we use resolution colors and quantizations and so forth so there are many ways of doing it and i believe i covered about five ways of doing a and n's what the engines bring you is speed you don't have to do this massive a billion comparisons you do a far fewer number of comparisons and you get the neighbors. But what do you lose? If you look at search performance in terms of performance and recall, I mean, sorry, precision and recall, these two measures, precision being every single search result is really the relevant one. And recall being every relevant search results that you know exists in the database, in the corpus, did show up in your result. So that's a measure, that would be a measure of recall. How many of the most relevant ones, genuinely relevant ones, you lose both in ANN. Why? Because you're searching only a neighborhood of things. And so you miss out some things that are just beyond the boundary. And that may be very highly relevant, especially if your query happens to fall literally next to a boundary. Let's say that you happen to have a query that lands you in San Diego and you start looking for neighbors, but you can't cross over to Mexico. So all your neighbors will be Californians, including the guy living in Northern California, Eureka, right? Whereas you missed out your neighbors just down the border. What's that city called? Tijuana. Tijuana. You missed your neighbors in Tijuana, who, by the way, are coming back and forth and helping you out with your, I don't know, backyard upgrade or your home improvements or whatever. It happens. Or you cross over there all the time to get, I don't know, medicines, and U.S. medicines are so expensive. So whatever it is, you miss the neighbors. So that's the problem with ANN search. So there are two ways to address this issue. One is, and I use this metaphor of sand dollars and sand. If you're looking for sand dollars and sand, one way you can do is you can scoop a larger bucket of them. You want 10, just scoop enough sand that you expect at least 10 of those sand dollars to be there, but you also end up picking up a lot of sand. Now you have a problem. Two things, because you got scooped up more, does it mean, is it computationally more expensive? A little bit, but because ANNs are so cheap, you can still go and scoop more. Now you scooped 50 results to get 10 really good ones, 10 sand dollars problem is how do you sift through the sand now easy because now you can go back to the gold standard what was the best absolute best way of doing similarity analysis not by embedding but by going back to the original transformer and doing cross encoding, right? Two sentences at a time. So take a key query and every of the 50 results, pass it through, look at the probability, similarity measure, and then rank it. You re-rank the search results based on that. And then you pick the top 10, right? That's why you use a cross encoder for re-ranking. And when you do that, think about it. You got now, hopefully, the best of both worlds. You got speed. At the same time, you got your recall, hopefully, if you brought a big enough bucket, all the sand dollars are there, right? And a precision. Hopefully, when you re-rank, only things that are genuinely relevant were in the top 10. So both your precision and recall gets sort of sufficiently restored. And that is the whole architecture of putting the search engine, semantic search engine together. Now, one reason I started this workshop with the semantic search is observe a few points that now that you're familiar with it, that we should see. We had to put many ducks in a row to make it work, isn't it? Yet each of the ducks are things that in hindsight look simple. Now, when I present it to you the way I presented it to you, I hope it almost looks intuitive that yes, that makes sense makes sense would you agree guys yeah you would also observe the fact that it was multiple users of transformers we use a siamese network of transformers to create embeddings those embeddings we put it into a FAS or ANN engine, scan, FAS, whatever, ANN engine, ANN database, ANN index, retrieved it. But then once again, we use transformers to do cross-encoding. Now, this also opens up the path, an interesting path, by the way, which I didn't cover. See, suppose the word is, you just get one word embeddings, like, for example, you get the word cat. Right. Are you talking about computer-aided tomography? Are you talking about cat, the cute animal? Right. So there's not enough context. So sometimes you can have a situation where AI search may underperform just keyword search. Great. And so one of the things people sometimes do is they use sort of a sparse vector presentation, or they just use ordinary keyword search results. They pipeline it. Remember I said you scoop a bucket of sand, but you also scoop a bucket of sand from elsewhere, from let's say your but you also scoop a bucket of sand from elsewhere, from let's say your keyword search engine, right? Because you know that ultimately you have the gold standard, the cross encoder sitting there as a gatekeeper, and it will re-rank it, and it will make sure nonsense doesn't go through, right? So that is also sometimes done. Yeah, Albert, your question now. Can we, every piece of the puzzle, not every piece, I take that back. Quite often the crucial pieces are independent ideas using transformers, right? Like in this semantic search, the embedding is through transformers, the cross encoding is a transformer, right? So that transformers play roles in two places, the Siamese network as well as the cross-encoder. That is it. The last thing I would say is, guys, people often tend to ignore the last step, the cross-encoding and the use of ANN. They often, a lot of people, a lot of vendors, they're giving you so-called semantic search engines or AI search engines, and all they're doing is vector search. That is not the whole way. Why? Because you will have problems with precision and recall. Not to mention that if you're doing direct vector search, you also throw performance out of the window. It will be slow. All right, guys. So I'll stop with that. And let's take how much of a break? 15 minutes, 20 minutes break? 10 minutes. Okay, let's take a 10 minutes break. I'll have my coffee and we'll start after that. So guys, we did do a review of the theoretical landscape we covered. We said transformers, transformers all the way, that's our mantra. You can use it to solve lots of problems by creating a chain of transformers and other things that you know. We took as a classic example, we took the search. Now, in a subsequent lab, we are going to do more interesting examples, but we took a break from search. After doing search, I wanted us to become skilled with the tools we are using. In particular, the tool set that we want to use for transformers, and that has become a defective standard, is the Hugging Face library. Now the Hugging Face library, Now, the Hugging Face library, interestingly, is written in Rust. Makes it pretty fast. As you know, Rust is a language that tries to have all the speed of C, but none of its security or memory leak issues, security vulnerabilities. So it's a pretty robust library. And what it does, amongst other things, is it gives you a uniform, easy interface to transformers, irrespective of whether they are written in PyTorch or TensorFlow. The two dominant deep learning libraries. A big thing from that. Now, people use different, they sort of bounce back and forth within TensorFlow and PyTorch. In my case, if you guys folks remember, this same workshop, long, long ago, I used to teach with TensorFlowflow right but somewhere along the line i switched over to pythorch simply because personally i find it to be more pythonic more more research friendly if you want to do some experimentation and things it is just nicer tensorflow is practically dead yeah yeah yeah so So TensorFlow has had a peculiar, you know, anyway, this is on records, I should say. It has had an interesting history. So apparently inside Google, so I'm told, they created a neural architecture. And the code base was rather, let's say, left a lot of scope for improvement. So they said, we are going to redo it. And this time, we're doing it in open source. So they came out with TensorFlow 1.0, except that perhaps the same people who created the previous thing created this. It was very low level. So it is like you want to ride a bicycle. Here is the chain. Here are the wheels. Here is the seat. Here is a post go build it right and then right there right like ikea like ikea yeah exactly like it became very popular and simply immediately took over the world like it was considered a necessary evil in many ways because you it was really a very powerful very framework, but you had to do a lot of things by hand. So that created the open source community to step forward and create the Keras, which is basically a layer of sanity over TensorFlow 1.0, right? And it's a high-level abstraction that did the simple thing simply, right? Then TensorFlow team adopted Keras as a de facto interface for 2.0. And TensorFlow was sort of two-phase, they would first make the execution graph and then you would run the execution graph. It made debugging harder, like you can't put breakpoints and so forth. It wasn't either execution, PyTorch is either execution. So then obviously a lot of us moved to PyTorch and then TensorFlow also introduced eager execution, tried to catch up. I don't know maybe a lot of people say that it's more production ready but I have never looked back from PyTorch. So what really happens is you take PyTorch. I mean, I'll tell you what I do. I take PyTorch models. I then forward generate into Onyx and TF, this thing, sir, what is it called? The RT, RT serve. What is this called? Titan, not Titan. The NVIDIA server, the model server, inference server, there is this thing, I'll remember, Triton or something. You load it onto that, and you run it, and it runs blazing fast. So that's what my team does. So anyway, this question arose, which of the two to pick? Anyway, HuggingFaces is a layer over both. It can work with both. Now the HuggingFace API has three core libraries. I would say four core libraries, four libraries, but technically three libraries. One is the datasets library. The datasets library is just wonderful. People produce data in all sorts of formats. This guy has CSV, that guy has JSON. It has this naming convention for the features and that for label, the target variable and this and that. In some sense, it brings a method to the madness. And it gives you a simple one-liner to get data sets, load data sets. We are going to learn about that data set library and see how convenient it is to use that library. Amongst its beautiful features are, so, you know, if you have a library to deal with data sets, it should have the following good characteristics. First, there should be a large repository of data sets that you can access. And we will see that it has access as of today morning, I believe it was 29,000 datasets it could access. Those datasets included text, they included audio, they included video, they included all sorts of things. Rich, rich library for you to play with. And we'll play with one of those libraries today, one of the data sets today. We will take the problem of classification and in classification, the simplest problem that are the quintessential poster child problem you solve is sentiment analysis. But we'll go beyond just positive and negative sentiment. We'll have a spectrum of six sentiments, sorrow, sadness, anger, so on and so forth. All of those emotions we'll have. So for that, we'll use the emotions data set. And we'll see how easy it is to use the data set. So there's a rich thing. Secondly, the API should be dead simple. Let us say that it is dead simple, but a work in progress. Not everything is obvious. I mean, some things you have to work a little bit harder. It makes common things dead simple, but a work in progress. Not everything is obvious. I mean, some things you have to work a little bit harder. It makes common things dead simple, but when you try to do something else, you have to work a little bit longer or, you know, poke through the documentation. The third advantage it has is reading from standard file format, CSV, JSON should be dead simple. It is, right? Just like pandas library. And the fourth is, it should have interoperability with other libraries. So the datasets, for example, the most common dataset loading framework that we know of in data sciences, the data frame, right? Pandas data frame. And it is absolutely interoperable with it. The data frame, right? Panda's data frame. And it is absolutely interoperable with it. And because Panda's data frame itself is interoperable with many things, for example, with Spark data frame, now you can start chaining the thoughts together, right? You could generate data in Spark, by Spark, and then gradually have a bridge over to this data set. And so you can do all that. Of course, you can write it as a file and then load it in data set, or you can directly with a couple of lines of code do a translation straight to the data set and save it. So those are the advantages of using the data set core library. Its internal representation of the data set is also pretty good. I believe it's Arrow. It uses Arrow as its native format, unless you give it something else. Arrow is a code based written in C++. It's a very efficient and high performance data format and gaining a lot of adoption. So one news that you may not be aware of is Pandas 2 came out. I don't know if you noticed that. Was it Pandas 2 or Pandas next version? I think it's Pandas 2. Pandas 2, right? Pandas 2 came out and guess what? Pandas 2 has support for Arrow as a backend, makes it much faster. And pretty much it sort of decreases the gap between the Pandas and polar. Come again? What is arrow? Arrow. So see when you represent data in memory what used to happen is pandas would typically use numpy arrays. NowPy arrays is far better than using Python data types because Python data types less dictionaries are horrendously slow. It's an interpreted language. So one way that people used it is they know that it's very hard to beat Fortran. So the Fortran's basic linear algebra package, blast and all of that, Fortran arrays are super duper fast. NumPy is a Python library on Fortran and C. So when you use, when you create your data structure in NumPy, you're basically using Fortran and so forth. And obviously because of that, it was a de facto standard. Python basically, Pandas basically was a simple usable interface on top of many for manipulating NumPy arrays, NumPy data structures. And it brought in the mindset of DataFrame. DataFrame is a mindset that has pervaded statisticians and data scientists for ages, since antiquity. Antiquity being in computer science world, maybe two decades, right? Two, three decades. So it's been there. So NumPy though has certain limitations. Data, when you represent it in columnar format, you can do it in a much more compressed way, right? Because for example, taking project, one of the common things you do is you get a data set with many features, but you want to drop some features and keep only some. When you do it with a matrix, you literally have to have another matrix ready for it. Whereas when your data is stored columnarily, it's very easy to pick a few columns. Do you see that? And so columnar data structures have been known in the analytics world to be the most efficient format of data storage for analysis. You see that for that reason. Aggregation along columns, sum, totally sweet, isn't it? If data is stored along columns, you have the locality of reference. The whole thing can be loaded into the CPU cache, and in one zip, you compute the sum or aggregation or whatever things that you're doing on a feature. So for that reason, columnar formats are better. Arrow is a columnar format. Pandas 2.0 supports columnar format. I believe dataset, correct me guys if I'm wrong, datasets to the best of my knowledge is native columnar arrow format. Again, speaks to its efficiency. Now, so that is one of the core libraries of Hugging Face. The other core library of Hugging Face is the tokenizers. It's part of the transformer library, but we consider it as a core part of it, the tokenizers. What in the world are tokenizers? Who would enlighten us? What are tokenizers? Say that again? Did someone respond? What are tokenizers? . Similar, yeah. Those are lexist parsers. They do the tokenization. See, guys, tokenization, the word comes from obviously if you are doing compilers, et cetera, lexing parsing, the words. So let's put it this way. You have text. But if you look at text, there are infinitely many texts. But you need to break it down into constituent parts, a smaller vocabulary. Those smaller vocabulary components are called tokens. Now, for example, when you write a language, let's say a language like C, what does your compiler do? The first part that it does is there's a lexa parser. It will take your text, chop it chop, chop, chop into tokens, the tokens that it understands. Because the grammar admits only so many words in the vocabulary. And each of the tokens must belong to it. Otherwise, it will throw a compiler error. Isn't it? And not only that, then there are syntactic rules on how you can put those things together into that. And that's the grammar of the language. Now, in NLP, you don't really go so much's the grammar of the language. Now in NLP, you don't really go so much into the grammar of the language, but you do still tokenize the text into constituent parts. The question is, what are those parts? And those parts are tokens. Now, what is a token though? It turns out that the answer to that question is a little bit more interesting, more interesting than you think. You may think word, you may think sentence is a token, but there are languages that don't exactly admit the notion of a sentence, right? They don't quite have very strict definitions of sentence boundaries. have very strict definitions of sentence boundaries. Then well not that then what? Words? Well word looks like a logical way to break it up and it's somewhat. The trouble with words is that there are two problems with words. Words are many. The English vocabulary is almost 5 million or 3 million unique words, growing 5 million perhaps. Most of those words are rarely used. The most common words are about, I don't know, 10 to 20,000 words, isn't it? So you know, guys guys that you can learn any language and be reasonably fluent with that language by the time you have learned the first 200 words right you can be reasonably conversant with it by the time you memorize only a thousand words you're doing very well you're pretty much at the high school level or so. And by the time you do 5,000 words, by the way, I remember that to come to graduate school, you had to take the GRE exam. And there is a guide to GRE called the Barons. And it used to have a vocabulary of 5,000 words. And if you really knew all those 5,000 words, it would absolutely ace your GRE verbal section. Right. So it just shows you that 5000 words puts you into the graduate school category. Isn't it? In that language. So there is, in other words, a long tail distribution here. Most often used words are few. So what do you want to do? Do you really want to use a vocabulary of 5000 words? Because in machine learning, you have to do something called one hot encoding for categorical variables. A word is a categorical variable. It's not a measure. It's not a number 3.65. It is like you have to identify an animal, parrot, cat, dog, horse, for animals. It's a categorical. So any token is a word. Which word is a question you can ask. But if there are three million words or five million words, you will have to one-hot encode it into a vector with five million dimensions. What's wrong with 5 million dimensions? If nothing else you'll be buying a lot of hard disk space and memory to load it, isn't it? It's wasteful. And computationally it's just terribly wasteful to do matrix computations with you know five five five hundred five million dimensions especially when that's such a sparse sparse matrix the it's not good but there is actually a further problem certain languages are what is called augmentative or there's a word for it in linguistics what it means is you you can you you have a concept in India. They use the word sandhi. And Sanskrit is very common. So you can have an entire sentence made up of just one word because you have conjoined all the words together. So then the reader is supposed to know where the boundaries of where to tokenize it, how to mentally tokenize it into word parts which individually have meaning. And that is a clue. Finland has the same issue. Japanese has the same issue. You can go on concatenating words together into a word of arbitrary length. And I believe German has a similar issue. Oh okay, nice. So Praveen is mentioning that OpenAI has a tool for tokenization that it will do. So but today we'll focus on the Hugging Faces tokenizer library and but yeah actually why don't we take this as a homework guys explore the OpenAI's tokenizing library also. So whenever you have an NLP library they will always have tokenizers to break it up into pieces. Spacey for example has a tokenizer. Spacey was what we used in the last iteration of this course, a very popular, very good library. But now they are upgrading themselves to be transformer compatible. spaCy 3 is transformer integrated. So therefore, it has value if you're using spaCy. Very fast library. And so every library, NLTK has its tokenizers and so on and so forth, but we'll focus on the hugging face tokenizers. A very good exercise is to see how other tokenizing libraries work. So tokenizer is the second piece. But from this fact that words can be conjoined, we get an idea that maybe the fundamental unit of the vocabulary shouldn't be word, but the sort of atomic word pieces, word segments or word parts that should be. And I'll give you a further hint. Suppose you use the word, just look at the word tokenizer itself. There is token plusizer. Theizer word can be used as a suffix to many other words. Can you think of what other words? Stabilizer. Yeah, and so forth. Each one of you can think of a word which answer ISA. So you can really think that it's actually what does ISA do? The doer of it. The maker of tokens, the maker of stability. So you should really take them as two different words, token and ISA, word parts, right? And likewise, singular, plural, what if I add a S to it? Tokenizer, tokenizers, token, tokens, right? So the plural itself gives you a clue that you don't want the singular and the plural both to be sitting in the vocabulary. It's a waste. So you could have a word part that is a word segment that is just the suffix s and the suffix eiser. So that is beginning to give you a sense of how better to construct a vocabulary. We'll go through that exercise today. The third part of the Hugging Face core library is the transformer themselves, the transformers, the models. Now there is a rich variety of transformers there, but here is the interesting thing. Because a transformer, if you remember remember we said but you could use bird for classification you could use bird for figuring out fill in the blanks finding the missing the mass word you could use bird for entailment whether the second segment sentence actually makes sense as a follow-up to the first thing right for question answer things like that you can do but model for anything. So how come one transformer model can do all these things? And the answer to that, so if you look at the cross encoder, for example, you give it two sentences and it produces the probability that they are similar, right? You give it a classification task, you give it something, and you ask, what is the sentiment? Then you don't give it the second sentence. You just pad it up with empty, but you're asking the bird to produce a probability for an emotion. It is producing, let's say a soft max probability for some emotion, positive, negative, anger, hate, anger, sadness, joy, whatever, surprise, whatever. How could it do that that you're using the same transformer architecture for many things and so the way to think about it is this see what you do is and i'll give you the big picture you just look at the transformer without the head right so a headless transformer right now it's a little bit of a gory image. Don't think of headless people like zombies walking around. Right? But it's somewhat like that. It's like a zombie walking around. It's very capable of something. But something is the head. You need to screw in a head, appropriate head, for it to do anything. head. You need to screw in a head, appropriate head for it to do anything. So one very easy example that I think of is, you know, you get electric drills, this called these electric drills. Can you do anything with a drill? You can't do anything with a drill till you put a drill bit into it, isn't it? Now, based on what drill bit you put into it, it will either, you know, you can use it for a whole variety of purposes. Do you see that? So that, for me, that visual sort of metaphor makes idiom. I don't know. Is it idiom? Is it metaphor? I think metaphor would be the right word, isn't it? So that metaphor is what I carry in my head. It's a drill without the bits. Right? And you need to attach different bits to do different things. So that different thing is called the head. So now how in the world does that make sense? See what happens is that the transformer, ultimately what did it do? It created a hidden state representation. All the tokens that go through all of those multi heads and so forth. Take BERT, and what do they do? They all create their vectors. The vectors get produced. And of course, the CLS token, the special token, also produces its vector. So each token has a vector representation. You can call it a tensor representation. But let's say vector representation, to be intuitive. Now, these vector representations are there. After that, let's say you're doing a classifier. You just take the vector coming out of the CLS and feed it into a classifier. Now, how do you do a classifier? Well, forget about transformers. How would you do a classifier? Maybe one example is you would just take a softmax, right? And into the softmax layer, you would feed in the input vector and output come because it's a soft max, it would produce probabilities for each of the cat probabilities, this dog probability is this horse probability is this and whichever is the highest, you say that's it. It's a cat, isn't it? And soft max, of course, exaggerates a bigger number. So that's a classifier. Now, that is the simplest classifier you can put, softmax. Or if it is a binary classification, you can just put a logistic classifier there. Right? Logistic or something like that. Now comes the next part. You could do even better. You could say I will take that and actually feed it through a few layers of my peanut butter and jelly. What are those? Feed forward layers. You may say that, well, let me learn some even higher orders of representation from the output of the transformer body. And then last layer is the sortness. So you can play games with it. You can do fancy stuff with it. And it's up to you how fancy you can get. And who determines what is the right way to do it? How do we determine which is the right way to do it? Like, should I just use the softmax? Or while you're at it, why not throw 10 more layers of feedforward and then put a softmax? How would you decide which one to go with? How would you decide which one to go with? Yeah the results speak. So what you have to do is you have to look at the measure, goodness of a model measure, something like either accuracy or F1 score or precision or recall based on the context. You pick that measure and you basically the number, the head, the shape of the head, how many layers there are, whatever, etc. It is a hyper parameter of the model, isn't it? To put three layers or five layers, to put no layers, just a softmax, one layer, just softmax. That is a decision that is a hyper parameter of the model. You have to do it and figure out what works best. Ultimately, who decides? The data decides. The validation set decides which is the right approach to deal with it. And then again, you can get fancy, you can have residual links and so on and so forth. You could do all of that, up to you but the bottom line is i will just use the word in general classifier head to classifier similarity head let's say it's cosine similarity head to do cosine similar and so on and so forth you can apply a screener head to that so that is your transformer library with a catch each of these transformers what are they expecting? They're expecting input vectors, embeddings themselves. They're not expecting words as in language text. So what are they expecting? Token embeddings, tokens and embedded already nicely indexed and given to you. They don't expect the word CAT. embedded already nicely indexed and given to you. They don't expect the word CAT, they expect 49 hot encoded to whatever 49 becomes in the vocabulary, given the vocabulary. Assuming that the cat word corresponds to the token 49, token ID 49, you usually call it the input ID 49. So that is the architecture. So it will be a sandwich of three things. Transformer will be the model will be in the center at the beginning. First, the text comes in, input doesn't go and hit the transformer. Input goes and hits the tokenizer. It gets tokenizer to input IDs and encoding and so forth. And then it gets hit the transformer. Transformer takes that and further adds positional embeddings, segment embedding and whatnot. And so forth. It does that. And the tokenizer has already added some things more. It has added, actually, segment embedding and attention layers, it's already added. Position embedding the transformer will add, shoot it through the attention heads and it's feed forwards and so forth. And out will come a hidden representation. I call it, I tend to call those words in a more general sense embeddings. Not many people would use that word embedding in such a general sense, but to me,'s embedding embedding is a vector in a latent space, right in a hidden space. It is a, these are vectors, each word becomes a vector in its own space. Each token becomes and that goes into the final head that you screw up for whatever purpose you want. Yes, they are. They are tokens. And we'll see it right now. So today is the practice day. So see, guys, now let's get into the labs. I give you the big picture because I wanted you to have this context in mind as we get into the labs. Now, over these three core libraries, namely the dataset, the tokenizers, the models, right? You realize that last Saturday, we sort of danced our way through the rose garden. We didn't have to deal with any of those. What did we use? We used pipeline. You remember, here is a pipeline for classification. Here is a pipeline for this sentiment analysis. Here is a pipeline to do a named entity resolution. Here is a pipeline to do whatever it is. There's so many. I believe the last I checked, there were almost 16 or 20 pre-cooked pipelines that Hugging Faces comes in. So those are the common tasks. So now that you look at it like that, what are those pipelines done? When somebody has that pipeline, what has he done? Somebody has sat down and put these things together for you. somebody has sat down and put these things together for you. Isn't it? So last time we used the pipeline cycle. Now your first response should be, when you solve a practical problem, always baseline with a pipeline because it will give you a rough and ready answer of how much accuracy you are getting or whatever your measure is, accuracy, F1 score, precision, recall, whatever matters, how well it is doing, start with a given pipeline, right? And now you can change the models. You can try different models as arguments to the pipeline. You can do that. You will get some idea. There is actually a significant degree of adaptability there, flexibility there. Because in the pipeline, remember, we just gave the task name. What is the task? It's a sentiment analysis. But if you look carefully at the pipeline API, you will notice, and that is what we are going to do today, dig into the details, you will notice that it also gives you the choice of specifying the tokenizer and specifying the model and the classifier. So all the pieces you can specify. What it does is it glues them together for you properly because that's a bit of code. When you do it by hand you have to struggle with it. No, no, for everything, all transformers sentence forget about sentence transform for the timing that was only for search i'm saying all transformers right for all tasks so pipelines is so they are god-given pipelines or rather hugging faces given pipelines and their pipelines we will learn to build our own pipeline today. So with that, what people have done, as I said, is they have contributed a lot of models that have been trained on something. So when they give a model, it is, what is it? It's a neural architecture, but it is also the weights because ultimately when you train a neural network, it's a fancy way of saying is, I have magically determined the best set of weights that will get your job done, some job done, isn't it? So it is no different from some fairy just moving a magic wand and telling you these are the best weights. Equivalent, so long as it works. But what you get are the weights of a model. Those set of weights there is a more technically what you say it is a checkpoint. The word checkpoint is often used as that you're downloading a checkpoint when you people say that I'm downloading a model in reality what they're doing is or more precise way of saying it is a bird is the model let's say but but checkpoint is bird uncased let us say the base uncased so what it means is i take the bird base model means bird has a larger model with a lot more. I believe BERT base has six attention heads of 12 and the BERT large has 12 or 24. I forget the exact numbers, but one is much bigger than the other. Lots of attention heads and layers. So the BERT base is the basic model or BERT large is the bigger architecture. But given the architecture, you have trained it on some data, right? When you say that I've trained it on a corpus, BERT base uncased, what does it mean? You have trained it with input in which everything has been turned to lowercase. Uncased means you don't distinguish between small and big cases in languages that have small and big cases. For example, Hindi doesn't have small and big cases. I think many languages don't have it. For example, in my view, Hebrew doesn't have it. Middle Eastern languages, none of them have it. Sanskrit doesn't have it. Indian languages also tend not to have it. But European languages tend to have small and big cases, upper cases. So it'll bring it down. Now, typically when it says uncased, usually these models are also like BERT, for example, that BERT, BASE, uncased. If you read the documentation, so associated with all these transformers, there's a convention. If you read the documentation, so associated with all these transformers, there's a convention. We call it the model card. A model card is a standard. Think of it as a billboard, right? Or a reference card that tells what the model is, right? And how it's been trained. So it will say, we have used word base, we have used uncased. And the corpus we trained it on is English corpus. Why? Because English is the dominant language in the data set that is accessible or publicly available. Other languages are far less represented. Now somebody told me that actually French is more spoken in the world than English. Is that true? That's what the French say. Is that true? That's what the French say. Right. But for some reason, the French corpus is not, doesn't seem to be as big. And by the time you go to a corpus of like Tibetan, now you're getting into trouble. Smaller corpuses. How many of you in the room know Tibetan? No. OK. So you look at the model card. And what has happened is when you download, what you're effectively downloading is not just the architecture part. You're downloading actually the weights that has already been trained. That's why you call these models, these checkpoints, the checkpoints represents a word that we use, pre-trained. You download a pre-trained model. Why? Because training a model from scratch is horrendously expensive. So what I'm alluding to is the concept of transfer learning. So we'll start there today. So the top five sources of knowledge is Mandarin, Chinese, Spanish, English. We ended with, I'll just recap for a moment. We ended with visual QA. Let me remind you what we did. We gave it a, Let me remind you what we did. We said that transformers are capable of great things. Amongst the great things they're capable of, you can give it a picture like this and you can say, what is this animal? And it will tell you it's a snow leopard. Isn't that amazing? You can give it any picture. And how easy was it with the hugging face pipeline? Now think about this line, guys. After you do what we are going to do today, you realize that it's just powerful to invoke a model with one line and be able to use it. Isn't it? Image classifier can immediately tell you what this image is. And right away it tells you it's a snow leopard with 93% probability. And it gives another 5% probability to just leopard between 93 and a half plus five, almost six. No, it is almost 99% probability. And then jaguar, lynx, cheetah, which look like leopards, are far, far less. How very accurate it is. The other example we took is you can give it a picture and ask it questions about the picture. Look at this code. We are creating another pipeline, one line pipeline for visual question answer. You give it an image and you ask, what is the invoice number in the image? Right. Now, the old way used to be that you would do some segmentation, right? You would zoom into that area, put a box around it, then do specific recognition of that particular OCR or something like that. But now you're not giving it any hints. You're saying, what is the invoice number? I don't know how many of you are impressed, but I am, that it can tell you that the invoice number is this, US001. Right? Why don't we run it and see? Let's do that. Let me run this. And what is the name? Oh, sorry. Name pipeline. I have to go to the top and run everything. One second. I think the link to the photo has expired for the Snow Leopard. Oh. When I ran that, it it had an issue why is it working for me hang on let me see oh no i have replaced it with that so i have to give you guys the new notebook okay i'll give you the new notebook so at this moment just point to any picture and you'll see okay so this is that now let me run this. Oh, it turns out by the way, I'm using a new machine. So I too am downloading it. And what would just happen here? Did it fail or do something weird? If you provide it without word boxes and the pipeline will but is not okay on this machine guys I have to install the libraries, give me a moment. Why don't I do that. I have to go and install these libraries. So why don't I take a rain check on it because I need to go to the server and run this command I'll show it to you after lunch. I will do that. This is the new machine, by the way, that Sukhpal helped me build. So my thanks to Sukhpal. By the way, Sukhpal has also built a machine for Abhijeet. Desktop. Really beautiful. You should see his machine. I'm envious. Now i want to exchange mine with his right so 4090 is just yeah yeah and now he's beginning to hate his mac it's way better right so yeah you can see what is the total amount and it says $154 and 0.6 cents. Like in the invoice, what is the amount it's able to answer, but do you notice that it's well, if you look at the image, if you look at the image, there is no word. There's an amount column, but it is not obvious in the amount column there are many amounts isn't it so it takes some level of inference to know that this is the one that you have to answer right then you can ask what is the invoice date and the invoice date it gets wrong actually so is the limitation invoice date is 11 slash 02 slash 2019 and it interprets the slash as one. So there's a little bit of an error here, which speaks to the OCR library behind it. Then you say, who is it billed to? Now, this is an invoice. Billed to is John Smith right here. And it figures that out. And you can go on asking questions. In the afternoon, I'll install the library and you can go on asking questions in the afternoon i'll install the library and you can see who is serving the invoice if you look at this it's not so obvious who's serving the invoice right there's a big invoice there's an address it doesn't say from right but it infers what it is from and we didn't train it i don't know if you're impressed i am that one didn't train it then like somebody repaint pre-trained yes somebody trained it and so that speaks to the power of transfer learning you benefit from other people's hard work people's hard work. Go ahead. I was just saying that in that case, isn't that this whole transformer or the model is going to go towards a global repository where you have models trained by and it's being trained by everybody and that's where... Yeah, Hugging Face is that repository. Yeah, yeah, that is that. that is literally what hugging face is is the right now we are downloading that right versus it's more towards those models are being hosted also so they're hosted so one of the things we will learn in this course is when you build your own perfect model how to contribute it back to hugging face so that others can benefit from it. So see what happens is when you train your own model and you upload it, your model becomes pre-trained model for someone else and they can do transfer learning. They can use it and that is the HuggingFace hub. I think as a bonus, like following up on that So if I look at something like a chat GPT, you're not downloading that. It's kind of. No, that is the problem. See, the problem that has happened is. OK, so for those of you who are remote, let me repeat the question. The question is. Chat GPT, we are not downloading. And it's a sad, sad day that we have come to this. It started with GPT-3, the open AI all of a sudden realized that they are sitting upon a gold mine and they made frankly in my view lame excuses. They said oh the model is so big it won't fit into the people's machines. Hey let people decide whether they can fit it on their machines or not. But in any case, they didn't allow it. ChatGPT4, at least ChatGPT3, they sort of opened it and say how it was done. ChatGPT4, they have written a paper that's more like a white paper. Technical details are rather sparse, right? So they're drawing the curtains on it. It's pretty terrible, actually. And I would strongly say that given a choice between an open source model and one of these proprietary commercial models, always use the open source. At least encourage it. Support, sponsor open source. If you have to give $100, if you have $100 to give, give it to the community. Don't give it to these sharks, frankly. So what I was going to ask is, I think to Saran's point, so for Huggins Face, you download the model, you train it, and then we load it back again. If you want to, yeah, somebody else can use it. In this open AI, it's kind of almost like this cloud. So you basically, you're like training their own model. No, so what they have done is they have pre-trained, they have a pre-trained model. When you use ChatGPT, you're just exercising the pre-trained model. But if I load my data, then I find- Cue it, yes, exactly. So that's the second part. We'll learn about the transfer level so then what happens is when you load your data you you get you have your private sort of variant which has your your fine tuning attached to it but. the fact that like for example a source code got leaked so how does chat uh how does gpt3 work where if i put my data into like you know one chat will that transfer to someone else's chat or is it just strictly locked in yes if you're using the public chat gpt yeah almost surely so there is a lovely xkcd cartoon i saw i just loved that one So it had a picture of some people having a conspiracy to overthrow the government, right? So they all say, let's meet at the docks. So the cop goes to this chat, this thing, and just says, we need to overthrow the government. Where are we going to meet? And Jack Gifford says, at the docks. And he says, gotcha. You see how it can be abused. So that is the huge societal impact of these things. And that's a point that I think I made in the past that the social and economic and political aspects need to catch up. They need to get real and catch up because nobody can stop this scientific movement. It's moving very, very, very fast. Again, I give you guys the background that when the steam engine was created, not many people understood how steam engines work, except that they knew how to make it work. Thermodynamics came much later, but it caused the industrial revolution, caused much benefit, cheap clothes, amongst other things, fast transport, steam engines, locomotives, but it also led to horrendous suffering. Children were slaved, chained to the machines, textile machines to work on them. The whole London was filled with smoke. People lived in hovels, tiny little hole in the wall places. And there was tremendous suffering. And then came sort of the socialistic counter-move movement to reform the whole thing. Law and order was brought in and things improved. We are in the same age. Unless we are careful, it's an industrial capture. Those people who own these big models and who are not willing to share it anymore. Now everyone is realizing that these models have changed the world, put the world upside down and they are going to make trillions. The rest of us will just be users with a begging bowl. Oh, can I please use your model? They say, okay, give us $1,000. Give us all you have. Reminds me of a joke. One guy, if I may, goes to a psychiatrist or psychotherapist or somebody and says, doctor, I feel that everyone is after my money, right? I can't help it. I have this feeling all the time. He says, no problem. I'll cure you for that. He says, okay, how much will you charge? So the doctor says, how much do you have? We are entering that world now anyway guys so this is a recap of the last time and i'll make it work on the server it's a new machine i set up but let's go back to new territory now today we are going to start with number the concept of transfer learning so transfer learning. So transfer learning is the basic idea, and I'll dwell upon it. As you see, there's nothing to do here, the code. So now the thinking is, if you want to solve a problem properly, what you do is, you see if the problem has already been solved, right? If it has been solved, somebody has checkpointed a model, you download that model, model checkpoint. Use it, right? You're done, isn't it? You're done. On the other hand, if you can't find your task, solution to your task, no, so even if it is done, you may say that maybe the performance can be improved because I have a special data set, my own data set, with which I can further train it, make it better. Are we sure? So then taking a model which already has some semantic understanding of the problem, and just it has already been trained through, let's say, a thousand epochs. And then you take your different data set and you take your data set and then you train it for a few more epochs. The secondary part is called fine tuning. Implicit to fine tuning is you don't want to fine tune on the original data set. Because the assumption is that they were careful enough to reach a saturation point of performance before they checkpointed it. Usually is true, not always true, but then but you take your own data set and then you train it. So what will the weights do? They will go through some minor perturbations and they'll readjust and they'll get fine tuned. That's why the word you use is fine tuning because you don't expect the weights to get radically transformed, isn't it? Weight weights is to completely look different you expect them to go through small perturbations the first one is called pre-training it's called pre-training pre-training is what you bring down yeah so transfer learning is made up of two steps pre-training and fine-tuning and now comes the interesting part of pre-training is the expensive part, usually, because you have to train it for a general problem with a vast corpus of data on usually a large cluster, right? Right? Or you have to go to Sukhpal, he'll build you a nice machine. Okay. So, and then you can train it, and then you can checkpoint it. That's your pre-trained model. Then you can train it and then you can checkpoint it. That's your pre-trained model. But fine-tuning is good because, see, you don't need so much data. Sometimes small data sets, for example, in the medical world, you're trying to classify a disease. How many cases will you get? Very few. You're trying to do a medicine compatibility with certain genomic types. You will get a data set of 100 patients, 100 gene blueprints, genomic print. So then it turns out it's enough because you just fine tune it at that particular moment. Of course, you can do small data augmentation strategies and do it and you can get away with it. And we did that in the deep learning course, deep learning foundations, you'll see. You learn to tell the difference between a weeping willow and a pepper tree. But when you do that, you will take a model that can basically classify things, but it can tell the difference between a house and a tree and a truck and so forth, general set of problems. But then you will, and it will even be able to tell apart the pepper tree from the whipping willow, but with a certain accuracy. But then you fine tune it. And when you fine tune, the accuracy goes up. Are we together? Or whatever metric you want to use, but in this case, accuracy makes sense. The accuracy will go up. Right. Now in our today's labs, precision required, but in this case, accuracy makes sense. The accuracy will go up. Right? Now in our today's labs, what we'll do in the afternoon is we will take the example of a sentiment analysis. A sentiment analysis. In sentiment analysis, you will see that a pre-trained model will be able to classify at about 50, 60% accuracy. Now, 60% accuracy, when you are looking at six classes, is pretty good because randomized, its accuracy should not have been that much, right? It should have been something like 17, 18% accuracy at most. So to go from 16, 17 or so or approximately or 16% to go from there to about 60 plus, transformer is doing a good job. It's not a dummy baseline classifier. It's not a nonsensically classifier. It is doing something good. But it is, you can do better. So how will we do better? We will take the emotions data set and we'll do the fine tune. So we'll do both stages of it, right? With the pre-train and then we'll fine tune it, right? And if we can't finish it today, then we'll finish it in the next lab, right? By the way, guys, we are moving a little slower than I had planned. So as I said, I'm adding two labs, two extra days to this course so that we make sure that we cover all the territory that we want to cover. So instead of six days, now it's an eight-day workshop. I see another question. So we talked to the media, and they have this thing called Nemo. And so they said we could actually take it and then bring it on our own. Is that what they call fine-tuning? That would be the final fine-tuning step if you give them a pre-trained model and say they have, I don't know about Nemo that much, but if they're holding on to specific domain specific data sets and they're not sharing it with you but they're saying we'll find you no no it's like um it's almost like a general purpose compute completely and so this one like you can basically then we use our own data in our own um you know cluster cluster and then train that uh oh that's fine too that's fine yeah so basically they're giving you a gpu cluster as a service you're basically saying no it's a model actually oh it's a model it's like a speech recognition oh yeah that so that is that so there is a pre-trained model and then you can fine-tune it with your data that's literally that and that is transfer learning literally so guys we got the idea of transfer learning right now let's move a bit faster we will take into let's go into one example now hugging face is something let's go to the hugging face i'll quickly go there to show you what the hugging face api looks like the and by now guys you know there are two things that I'll tell you. The recipe for success in this field are two. First is the statement of Karpathy. He says, really be friends with your data. Know your data well. Right? And I say something similar. I say that, see, when you, in America, of course, we have an audience that probably didn't date, but in America, there's a convention. Most of the world is a convention that you find your better half through dating, right? So when you meet somebody on the first date, you're on the guard. You're just trying to make a positive impression. You don't really learn much about the other person, right? You just see like big red flags, then you don't go learn much about the other person. You just see big red flags, then you don't go on the second date or something like that. You just get a gut feel for it. It's only after a long association that you know the person, isn't it? After many, many meetings. I think there's a human study that says it takes about 50 or 60 encounters before we genuinely trust somebody. Something to that effect. I don't know the exact stats. 50-60 hours of association before we trust somebody. Same is true for data. Think of data. Whenever you look at data, think of it as a date. You're not likely to have a lot of success if you completely ignore your data. It's the same way. You want to know, you need to know the data really well, because it is the data that data science is about. All your prediction models, everything is about the data. So start by knowing data. It is the guiding light. It will tell you what to do and what not to do. And you will notice that data can be, you can learn through exploratory data analysis. But even more, when you build a model, you look at the way the model is making mistakes. By studying the mistakes you'll actually find, especially when you're dealing with label data, the mislabelings, because every data has errors. You can use, sometimes you can trust the data, sometimes you can trust your model. And you can use your models to find out where the data has errors and fix the errors by hand. Now relabel it. So remember your models are also great relabeling tools or data correction tools. So do that. We'll do that. And of course, we have talked a lot about exploratory data. The second thing that you need to really be familiar with is if you want to do carpentry, what should you be most familiar with? The tools and the material, right? You should be able to look at a wood and feel and tell, is it soft wood? Is it hard wood? Is it cross-grained? How am I going? Right? The strength of the material. And you should know your tools, right? You, for example, right, should, like, you should know what the drill bits are, what each of the components in your library, in your sort of tool set are, not to know it. So for example, using the back of a drill to hammer a nail is a terribly bad idea. I have seen people do that. And silly as it looks, I have seen people do that in programming. Using these libraries, they use something. Yes, you could use it. But really, so don't do that. Know the libraries. And today is about knowing the library. We'll go below the surface of just using a pipeline and getting a little bit deeper. So how do we do that? First of all, there is the datasets. Very good, we can explore the datasets guys. It has already been categorized by the tasks. Do you notice that? Like all of these tasks, they have already been categorized by text classification. You want a dataset on text classification? Here we go. Text classification. You can go and do that, right? And one of the things that I would suggest is, sort it by most downloaded. The Glue and the Super Glue, the IMDB. What in the world is the IMDB? Movie database. That's right. So Rotten Tomatoes is along the same lines. So the highly downloaded ones are a good place to start. Why do they call it tasks? Because these are things that you do, right? Classification is a kind of task. Feature extraction or question answering, entailment, and so on and so forth. These are the kinds of things you can do. The word often used is tasks. So you can pick the data sets. I will just take one particular data set in here. In here, emo, right? And you notice that emotion dataset. When you go to the emotion dataset, do you notice that there is this dataset, Dairy AI Emotion 2.4? Let's go and look into this dataset. What does it look? So you preview the data, guys, so that you know what you're working with. Good thing with Hugging Face is instantly you get to preview the data. You see this string. You have a text and you have a label. And the label has meaning. Where is the meaning coming from? It is coming from the features, the description of the features. So now notice that there is a data set card. It's one of the nice things, conventions followed in open source, that you cannot upload a data till you give some descriptions to it properly. That's a practice. So when you look at this, you will see that it has a lot of things associated with it there's a paper you can go read the paper right uh and so on and so forth but i would get too much into it labels and they explain the labels do you see what does zero does zero mean? Joy, one, love, two, anger, three, three, four, surprise, surprises, this thing. So remember guys, zero, one, two, three, four are not degrees. They are not numbers. They shouldn't be treated as numbers. They should be treated as categorical, right? Even though they are written as numbers, one minute, even though they are written as numbers, you should treat them as categorical, right? Even though they are written as numbers, one minute, even though they are written as numbers, you should treat them as categorical. And this is very common. People in databases, they store things as numbers. Why? Because it takes less space. But then they have a dictionary to translate from the number back to a label. Remember, categoricals must be treated as categoricals. If you treat them as numbers, all hell breaks loose. Albert? What is the corpus here and what is the short sentence? So the entire data set is your corpus and each document is one text and text is the document. Those are your documents. Each document is some text string. So those are your documents. Each document is essentially one text. So that's the short sentence .. Yeah. And then the labels is what any good classifier should. So it is very evident that it has to be used as a classifier, in particular classifying into emotional states. So it's a sentiment analyzer. Any kind of sentiment analyzer should be using this because it's labeled data. Right? This is it. Use this data set, you can do that edit data, or train in auto train. So one of the nice things is, you can click on train with auto, right, so you'll have to tune in. so i'll come to other aspects of it if you have a hugging face api like a account and i advise you to have a hugging face account it's very useful right if you are in this world you know you have to know where the meeting place is right so get familiar with it and so i won't at this moment go into it. There's paper with code. When you go to paper with code, you will see that there's an emotional, there's a, and people have written some things to it. I won't go into that, the homepage. There'll be a paper. It's always good to read the paper from which this data, data set originally came. Like I said, know your data and what other people have done. Like people create data because they're doing some research. You want to go and read that research paper just to get the reason, even though it's historic, go read it. We are going to deal with this particular dataset. Let's go back now and where were we? In the paper with the data set with the model that we might have published or something? No, they give you just the dataset and we'll come to the models in a moment. Where was I? Transfer learning we did. Oh, sorry. Give me a moment. Too many notebooks open. NLP, Visual, QA, transfer learning, data sets, and pre-trained transfer learning. I'll reopen it. So what we will do today is we will start with the data sets, not books. It's a worthy thing to do. Today, actually, I'll extend, we'll start a lunch a little bit later because we have been doing a lot of talking, which is good, but we need to cover some territory. So look at the simplicity of using the data sets. Make sure that the data sets is installed, pip install data sets if you have not installed it. It's part of Hugging Faces. And the first function we'll use is list data sets. It will tell us how many data sets are where on our local machine or in hugging face in the hub. Right. So when you do that, as of today morning, there are 27,985 data sets. So anyone of you who are asking this question, I don't know where can I get data? You probably have an answer. So you have an answer, right? So this one is going to a hugging phase and then finding the number. Yes, it's finding the number. Isn't it lovely? And for you, it's just one line code. Now, let's say that number. Isn't it lovely? And for you, it's just one line code. Now, let's say that you want to load a data. Look at the sheer simplicity of it. Give it the name. What was the name of the data? Emotion. Right? You load it and it goes and gets the emotion data set. Do you see that it gets the emotion dataset? Load emotion. This is the calling the API then? Yeah. And it's getting the data. So the name emotion will be used in the website. Yeah, it's there in the dataset. So then you look at this dataset dictionary, which... And so some datasets are given like pretty pretty much they won't let you tamper with it it's there then you have the data set like if you just say down do you see it's downloading right and by the way i don't think i ran it on this machine so might as well run it see how quickly it should run through, because this is a new machine. So you and I will have the same experience. This is it. And let's see, maybe the number has changed since morning. Oh, number has actually changed. It's now 28,000. Do you see how fast evolving Hugging Face is? I literally ran it yesterday night. It was 27,000 something. Now it's 28,000. Let's download it. And of course, it downloaded it very quickly. It's a short data set. I hope instantly downloaded. You can run training. Now, when you look at this data set and you just say emotions, it will tell you that it is a dictionary containing three subsets of data. It internally contains three data sets. So a data set can have nested data sets inside. Why is that a good idea? Basic machine learning was the first thing you do. For any supervised learning, you want a split into train validation and testing right so this is it it already gives you the split how easy it is you don't even have to call the splits right and then let's take the training data which is 16 000 rows let's look at one row of the data now when you get the training data like this, you see that you're just looking into the dictionary and getting the training data. And after that, you can treat it like a list. When you say the first row of the data, it is basically a tiny little map. Label is equal to, I didn't feel humiliated, label is zero. And so you can look at more rows. You can ask, what is the zero what is this so how would you know the answer you can ask the data set to declare its features it's telling you so it's saying ah these are the names sadness joy love anger fear surprise so zero corresponds to which is in this list. What is the zeroth index? Sadness. So it is saying. Then you move forward. And by the way, it also comes the class label has a method, it says ID to a string. So it will convert a number into a string and you can go backwards. You can say convert a string to ID so you can give it sadness and it will convert it to ID. We'll see that in a moment. You look at these results. Here it is. Now you may say, you know what, let's add that textual label because remembering numbers is hard. Let's go and augment this data frame with this. Now, how do you do that? Most of us know how to augment using pandas. Pandas is easy, right? So very easy conversion to pandas. You just take that dataset and you say set format pandas. Right? Could it be easier? Like, well, it could have been easier. You could have said two pandas. But this is it now once you get a pandas data frame then it's easy you just look at it here we go right and now you say okay let me add the emotion so when you add the emotion you have to be a little bit careful because what do you do for you first get the labels label will be will be a column of numbers, right? 0, 1, 0, 3, whatever it is, that particular column. And then to that column, you apply a function. For each of the number, you want to convert the number to a string. You see this? Int to string is a feature. It is something that comes built into the dataset api because it is calling a method of this class right remember training dataset dot features is what it gives you this in the feature is a map if you go to label you basically get an object of this type do you see that right you're directing yourself to this and on this class label there is a method called int to string. And that int to string method will convert it into a string. And then when you do it, here it is. I just produced some random rows. Is this cleaning up the data? Yeah, more intuitive. Just to see what it all is. Because see, numbers are hard to remember. I can keep looking at the dictionary. But when you see it, you can see, let's look at it and see if we agree with it. Could you increase the size just by one control plus or two? Sure. Is it better? Yes, thank you. Okay. So it says, I have made it through a week, I just feel beaten. Now do you notice that it has first thing to observe is that actually these are not even grammatically correct or spelling, spellings are not correct, right? These are very colloquial usages. And yet, I mean, somebody has annotated it with sadness. The question is, how will our transformers do? We'll find out. I feel this strategy is worthwhile. Somebody seems happy. The strategy worked. I feel so worthless and weak. What does he have? Et cetera, et cetera. Again, you notice mistakes here. I feel clever now. So this is annotated data. et cetera, et cetera. Again, you notice mistakes here. I feel clever now. So this is annotated data. The other thing I wanted to point out is you can go from dataset to Pandas, but you can also come back. And when you come back, you don't have to come back right from this, just to show that you can create a completely clean Pandas dataset and convert data frame and convert it to a data set i've created an example basically a silly example i took a x1 feature which is what a thousand points along the x-axis from zero to one right but then i created the another feature x2 which is the sign of X. I created another feature X3, which is log of one plus X1, right? And so I created a data frame, right? You would agree that this data frame pretty much, this is the description of the data frame. These are all standard Pandas operations. Would you all agree that this is how you would do it in Pandas? This is no brainer. Now, how do I convert this into Hugging Phrase data? You don't have to export it to CSV and read back the CSV, which you can do by the way, but I leave that as an exercise for you guys to explore. You can just say a dataset from pandas. This is it and you get this lovely isn't it right so this is so now i've just taught you the basics of the data set there's a little bit more guys huh so please take this as a homework in the lunch break or try to explore go to the website and go to the, where are we? Data frame, this hugging phase. Now, how do I explore the API? Let's go to the documentation. We go to the documentation. Do you see these datasets guys on my screen? Go into that and it is really worth going through the tutorial guys. Go through the tutorials and how to guides. Believe me, you need to know your tools. Right? So this is I suppose it's one o'clock, it's time for us to take a segue, take a lunch break. But why don't we give this as a exercise guys do that, Guys, I'll just end with a joke. In Silicon Valley is where a lot of millionaires are made out of geeks. And one of the things geeks like to do quite often is buy cameras to do photography. You can always spot one. If you're into photography, you can always spot one. Because if you go to one of the beaches, right, let's say Stinson Beach or Half Moon Bay or something, you can spot one of these geeks because they'll be carrying a really big camera, right, DSLR, right, mirrorless one with a long lens. And their family will be getting it will have a most annoyed look on their face and in broad daylight they'll ask them to pose on the beach while they're taking a picture right have you seen this any one of you has seen this sort of experience or been guilty of it more of it right now what are some of the things wrong with it and if you ever go close to them and observe what's the camera setting it is set on auto right at auto a high-grade camera actually takes worse pictures than an iphone or a smartphone samsung or something actually takes much worse pictures it is not meant to be used in auto except in emergencies. Right? So that is not knowing your tool. You have all the expensive equipment, but you don't know your tool. Many things wrong. You don't never photograph people in broad daylight. Why? They don't look their best. They don't look their best they don't look because you have stark shadows sun is up there and it is casting dark shadows on the face it'll make people look the worst possible you look you take pictures in cloudy days or during sunrise or sunset when the light is coming the golden light is coming at you right anyway so those are the things but this is it guys it just as we geeks make a fool of ourselves the first time we buy a fancy camera and lots of lenses and everybody knows all the photographers know in the same way if you don't know your tools well hugging face tools well believe me in the nl community, everybody will know. Right? So don't be that. So today, learn, let's start. We'll give this week to really learning the tools well. Guys, we could have put many tools. Hugging face is the dominant one. You have to pick one tool set properly. Like if I were to talk about cameras, it's a distinction between am I in the Canon club or am I in the Nikon club or am I in the Sony club? It doesn't matter. The ideas are the same. You need to pick one and know that tool set well, right? Because the foundational ideas, the F stops, the aperture, the shutter speed, the depth of field, right? The color, this thing, balance, all those concepts remain invariant. All of these tools just do it slightly differently. In the menu items, it will be shown differently. But ultimately, the foundational concepts are the same and you need to now bridge between your concepts and how these tools implemented. Isn't it? And that you have to do, you really have to do. Guys do that. I cannot, there is no way I can overemphasize this thing that you need to know your data well, but you also need to absolutely have details of the API of these tools. One API, pick one. You should not only know some at the basic level of these tools, one API, pick one, you should not only know some at the basic level of Quickstart, you should know the APIs, the signature of the methods, almost reflexively from memory, you should be able to quote. You may not be able to quote all the arguments to a function, but at least the dominant ones that you use, you should be able to remember them. Are we together? And with that comes power. So take that piece as a lesson and make yourself go through all of these. Go through the tutorials, how-to guides, conceptual guides. And finally, the reference. When you go to the reference, let's say, how did I know that the class label has that method into string and string to int? Why? Because I can go to it somewhere in here is a class label. Class. Why am I not able to find it? Control F. Yeah. So we see there's a class label here. Right, expand, where am I? Dataset, oh, somewhere. Okay, so you keep searching for that and And some sooner or later, you'll hit upon it. Builder classes, loading classes. Okay. Yes, here we go. As a class label, right. So I can go into this. It sort of jumped somewhere. I don't know where it jumped, but it was supposed to jump to class labels. Hang on. Why did I not get there? Class labels. Okay. For some reason. Anyway, suffice it to say, I don't know why it is jumping around here. Again, I'll go. Okay, after the lunch break, you'll provide the Jupiter notebook. Yes, all of them. In fact, right now, I'll give some right now. So you can play with it. Okay, okay, class class label. Here we go. Class label, here we go. Now with some luck, class label data properly converted to tenses here, here is this class label, here we go. Now with some luck, class label data properly converted to tensors. Here is this. Class label has names, but let's drill into class labels and hopefully it will take us somewhere. Here we go. It took us to class. Yeah, here we go. The class class label. You see that it contains names. It contains, there are three ways now it will give you sorry it's running a bit slow ah parameters cast into yeah here we go do you see into string right and string to end so how do you remember it after a little while it becomes familiar to you, right? You know, you keep going through that. The other thing that I would suggest is there is a very good book. It's literally called Natural Language Processing with Transformers. It is a good textbook to have as a reference to this course. It's an O'Reilly book, Natural Language Processing with Transformers. Can you paste the link to the book please? Absolutely, let me do that right now. Yeah, yeah, it used to be. I'll bring it over. You guys can look at it. It's on my table in my office. It's a very good book actually. I like it. It is completely Hugging Faces based, mostly natural language processing. Yeah, by the second one. The second one came just two, three months after the first. The distinction is colors. It's colored pages. With transformers, yeah. with Transformers. Yeah. So you can see that it came out in August and I bought it in August. Right? So usually what happens is that through the community, you know, through the community channels, you always know which books are coming out and when. You end up pre-ordering them. So it's a very good book. I would highly encourage you to do that. It's Hugging Face, almost completely Hugging Face. See when you think of transformers, open source Hugging Face is basically basically. The word hugging face is synonymous with transformers. That is why the library itself is called transformers. So you say pip install transformers for hugging face. So guys, this is it. Let's break for lunch. And where was I? Today's class work is actually go through all the tutorials like tutorials how-to guides etc do it thoroughly because if you don't do it now you're all busy remember your life has your boss your workplace right and then your boss is at home right your children your family your parents they all will be there and then you have to sleep and then you have to socialize and watch tv netflix right so any studies you don't do here the probability that you will get as much time at home is relatively low. So do it here. Okay. So guys, we break for lunch and we meet at 3.17. 3.17. Okay, got it. Let's pause the recording. So welcome back from lunch, folks. We were making our way through the concept of transfer learning. And I think there's a repetition transfer learning and pre trained models are the same file, but we can verify. I have put all the files on the slack channel. So please see that it is there. Oh yes, the, you can ignore one of the files because lesson 08 has been duplicated with two names. So you can ignore it. So we'll move to lesson number nine. And if you look at this, one of the one of the easiest things you can do in hugging faces is obviously the easiest thing you can do is use the pipeline API, isn't it? A pipeline to do this, boom, it's done. Give it input, output comes out. It can't get simpler than that. But the next simplest thing that you can do as you start peeling the onion is to use one of the so-called auto classes. Hugging face gives you a very simple framework. And these auto classes are best represented by this syntax. They will be all auto some class or some task from pre-trained. So let's break it down. When you say from pre-trained you're loading it from a checkpoint isn't it? So you give it the name of a checkpoint and it will load a transformer model. It will first look locally if you happen to have a local copy. If you don't happen to have a local cache of it, then it will download it from the Hugging Face hub. So implicit to this is you can't be running this code if you're flying at 30,000 feet in the air because you might not have the internet connectivity. So then in that case you hope that you have locally cached it. So be aware of that, that it reaches out to the hub to get it. Now we'll take a simple example. We have been dealing with tokenizers and we'll deal with the tokenizers in a moment. Did I cover tokenizers before lunch? I have not yet. A little bit. Okay, we're going to do it a little bit more now. So one example is auto tokenizer. Right, so do you notice that the syntax is auto something from pre-trained? So if you look at this syntax, the syntax is auto. Now the name of the class is really like that. Auto tokenizer from a checkpoint, a BERT based uncaged is the checkpoint, right? And are we looking at this code folks? Is this making sense? So what it does is you get a tokenizer. When you pass in a text to it, what do you expect? You expect tokens to come out. And it is as simple as that. If you look at the element number six or line number six or input six, you notice that we send in a text, which was, and I deliberately sent in a text that is the tokenizer does tokenization it does this to have fun with tokens so there's a lot of redundancy here why did I give a word like that remember we talked about the word pieces and so forth so it's a hint that there is more going on in this sentence but but we'll see that later. At this moment, we'll just look at the auto classes. You print the tokens and here we go. You get the input mass. Now, what are these numbers? Input IDs are what? These are indexes into the vocabulary that this tokenizer is using, right? And that is what it is. Now, what is token ID? And we'll come to this. Forget about the output here. We'll talk about tokenizers in depth. Likewise, if you want to do image processing, now you may say, oh, good grief. If you have done image processing and use the PIL and libraries and many things, you know that there are many, many lines of code you write to do just basic image processing and vectorizing the image. But how simple can it be when all you have to do is say auto image processor and auto image processor from pre-trained and you give it a proper transformer model into which you will feed in. Like what is an image transformer? VITs are obviously one of the classic image transformers, right, the original papers. You can use it for determining whether some image is a cat or a dog. But before you do it, you need to create it into an embedding vector that goes into the transformer, isn't it? That's what we talked in the morning, that they take input as an embedding vector. So you need to create the embedding for that. And then the transformer will take it and it will produce what it's supposed to produce. And remember, what does a transformer head in itself produce? It just produces hidden states or embedding latent vectors or embeddings. And I will use this word sort of interchangeably. And this is it. And here I won't go more into and show a full example. But suffice is to say that you see that this code between tokenizing, which is an NLP task and image processing, which is a computer vision task, how absolutely similar the API looks, isn't it? Very easy, guys, very, very easy. And likewise, you go to the next thing, audio processing. You have sound files, MP3s, you want to process them. What do you need to do? Well, Auto Feature Extractor, which extracts features from audio, is this. You can just use a proper transformer. This one does speech emotion recognition. And then lastly, you could go further. We live in the world of multimodal learning now. you can have, we live in the world of multimodal learning now. So when you feed in data, nothing says that the data cannot come one part from image and another part from text, or one part from image and another part from sound, or you can mix and match and do all sorts of things, right? And so, and by the way, multimodal is very big in the last two sessions of this workshop. We are going to focus on a lot of multimodal learning. If you think about it, all of these DALY, stable diffusion, etc., etc. These are all instances of multimodal learning, but it goes far beyond that. You can do very complicated things when you give input of text and sound and so on and so forth. You can do very complicated things when you give input of text and sound and so on and so forth. And we will see, and you can do the opposite, you know, sound synthesis and or transcription, which will do transcriptions for you. So we'll do a lot of multimodal stuff. They're part of natural language processing. But this is a this is a good first start. You take a LMV model, which is very well known in this space, and you can load it. It will do two things. It is ready to do input processing of images as well as tokenization of text. So think about it, guys. In one line, you have put two domains which used to be separate in AI, computer vision and text processing. And you can see how close they have come with this multimodal learning that you can now, there are checkpoints and models that you can load with just, and processes that you can load with just a line, isn't it? Now you can say, all right, that is for the processors or the, preparing the data to go into the model what about models themselves that that too is very easy so we will take an example suppose you take a checkpoint distilled bird case a distilled bird base uncased now what was distilled based to vet your recollection distilled based is a trip is sort of a smaller version of bird based is a sort of a smaller version of BERT. And how it learns, how do you train a much smaller model to perform almost as well as the big model? We will learn about this. That's one of the loveliest papers. So what you do is you build a model, you make it do predictions, and then you train a student model, and you treat the big model as the teacher and you try to make the student model not learn from the data but learn from the predictions of the teacher model right so it learns the way the predict the teacher makes predictions and sort of catches up to it and so it has almost the accuracy of the much bigger model. Distilled Bird is a shining example of that. And it does really give you. How does it do that? Oh, we'll learn about that. We'll learn about that. It's good. The architecture is worth learning about. So it's one of the papers we'll cover. So but just assume it can do that. Patrick? So I said said for these multi-modal models, does it still follow the J, Q, and V of attention where like you break down an image to like multiple pixels? Yeah, so that's right. So what happens is not at the level of pixels. So there is a paper which I can be covered. Actually we will cover a lot of papers you'll see and at some point i'll start covering two three papers a day um it is called what is it 16 by 16 pixels is worth a thousand words or something like that so what they do is see what is a word made up of characters so they're saying that we'll take 16 by 16 pixel and consider it a word or a token you got it right from there the moment you realize you get that insight you realize that the whole transformer architecture can now be applied doesn't it because it works on tokens sequence of tokens so you broke the image chopped it off into tokens so it doesn't break away from the original transformer models that were based off of words. Exactly. Just we converted the signal into what fits into that. So what do transformers need? They need a sequence of tokens. So how do you stare at a picture and say how do I make it into a sequence of tokens? What is a token here? You say okay I'll take 16 by 16 pixels and call it a token, right? Because after all a word is made up of characters this is made up of pixels and that's it so they just go through the whole image and this is interesting yeah that's a basic intuition and now when we do the paper they do it a little bit smartly and we'll come to that good quick oh so for like going back to like tokens for words is it randomized as to like where they'll stop the token no no it's not at all not at all in fact that's a very next one so uh hold your breath and you won't have to hold it for too long so here we go you can take a checkpoint and you can say auto model for what purpose let's say that you want to classify sequences sequences are, texts. You want to classify text. A classic example is sentiment analysis. What sentiment is it, right? Or classifying what the topic is. Is it politics? Is it sports? Is it science? What is it? You can use it for that, right? So things like that. So for classification. So that would be sequence classification. So you can say automotor for sequence classification. Pretty much the way you would intuitively think of the sentence, that's the name of the class. On the other hand, if you are using the same checkpoint, because you can use the same transformer for different purposes. Remember, we said that once you have trained a transformer, it can do multiple things. Suppose you want to use it for just token classification. Suppose you want to use it for just token classification. One example would be which part of speech is it? Is it a noun, a verb, an adjective? What is it? Right? So you could do auto model for token classification. Right? From pre-trained. Another way to think about it it is is it a named entity or not named entity resolution so if i say uh felix the cat jumped over the wall right or jumped over the berlin wall well berlin wall is fallen but let's imagine right um felix is the named entity it's a person uh well i suppose cat is not none of the words have to be ignored. And then you come to Berlin and you say location, it's an empty entity. So you can imagine that you can construct all sorts of things. So guys, I hope you realize that, excuse me, that auto classes are about the next simplest thing after the pipeline isn't it very easy so we peel one more layer of the onion and you and once you get familiar with it just lovely okay if you want to see how lovely it is uh try writing this entire code in plain pytorch or if you really want to trouble yourself, write it in TensorFlow. Right? And see how many lines of code you write. So... Why did you not say that you didn't know? Oh gosh! Now that would be, that would be like... That's in the bicycle analogy we use, you'll have to take ore, you'll have to refine the ore, smelt it, smelt the iron, and build a bicycle from scratch. Okay, so the next one, we have been talking about tokenizers. Let's look into the tokenizers a little bit more in detail. So guys, are you following me? I want to go fast because this is easy stuff, right? By the way, all of these notebooks are on your Slack now. I hope you have taken them from there. So let's take an example. And we'll go into the tokenizer now in real detail, hands-on detail. So I took the same sentence. The tokenizer does tokenization. It does this to have fun with tokens. You'll see why I deliberately wrote that sentence. So how can we do that? We'll use the auto tokenizer, something the auto one of the auto classes. You give it a checkpoint, bird base uncased and there you are. You see how easy it was except that and by the way this is a trick I use. You know, when you print things out like this, sometimes it's not very, like, at least I find it a little harder to read, especially if there are many, these dictionaries have many keys and values are erased. So what I tend to do is I use this trick. I convert it into a Pandas data frame and then read it. I find it a little easier to, I don't know, and of course your own experience may differ, but I find it a little easier to read that these are the inputs, right? So now what has happened? This sentence, let's see how many words there are. One, two, three, four, full stop is five, six, seven, eight, nine, 10, 11, 12, 13, and another full stop, 14. So you would have expected, including punctuations, 14 tokens. And yet we have, oh good grief, 19 tokens. So guess, where are the extra tokens coming from? No? Punctuation? We included that in the 14. Look at the sentence. Start and end. Very good. You need special token CLS for start. Separator for the end of that. What else? Hint. Look at the text that I sent in. What does that text tell you? So look at the results. Do you see 19 or 19204 repeated? And what does it correspond to? 19204 seems to correspond to token. The word token, isn't it? And so what do you, why has... Why is it token? Yeah, so look at that. The tokenizer. So first is the 101 stands for CLS, as you can guess. The second one stands probably for the, and we'll verify whether it's true. Now use your deductive skills. What will the next one stand for? Because it is repeated, it cannot stand for the word tokenizer because tokenizer word happens only once and yet 19-204 is thrice. Now what is thrice in this sentence? The word piece token or the word segment token, token, token. It is there in tokenizer, it is there in tokenization and it is there in tokens. So that gives you a clue why it is good to use word segments because now you can have a smaller vocabulary. We talked about it. So now let's see if our guess is right. This 19. So I saw the mystery now. Look at it. Does it look right? Right? Yeah, it got broken up into the token. And the the birds convention is whenever it puts two hashes. It says it's a suffix. It's a suffix, right? So isers, it's a suffix. Isation is a suffix. Civilization, prioritization. There are many, many words that can end with this token, isn't it? And ends with S, the plural. Surely one word piece that is ubiquitously used is the plural part, isn't it? That is it. It starts with CLS and CEP. And it turns out that different tokenizers, they may have different sort of these special tokens. One may use this and you'll see that it differs. Right? So intuitively, without knowing all this, you would have expected this. The tokenizer does tokenization dot. But now you know that there is more going on here. So why is it a good idea? We have talked about it, but I'll just review it. Why is it a good idea for ISER, tokenizer, to take token away from the ISA part and keep ISA as a suffix, because we happen to know that there are many, many words that end with ISA, right? A finalizer, finalizers, plural of it, visualizer, visualizers, equalizer, equalizers, and so on and so forth, right? And this is not an exhaustive list. You can go on thinking more and more and more right so we can do a little experiment let's put all of these i created a list of isers you know i created a string which is just made up of isers all sorts of isers right finalizer finalizers etc and let's pass these things through the whole process and see what comes out how How did it break up the word? Do you notice that final, eiser, final, eiser, and es, the plural. And do you notice that it did not create, like even though it created more tokens, it is from a smaller vocabulary, implicitly from a more condensed vocabulary. And that is the whole point of using word segments rather than words themselves. Because remember, English has 5 million words and you could be in quite a bit of trouble if you try to use 5 million words, right? Now, one of the side benefits of using word segments is, remember I said that for words that is not there in your vocabulary at all, you use a special token, like BERT will use UNK, right? UNK stands for unknown. And all the words that it doesn't find in the vocabulary, it will map to that. But what happens is that it's rather hard to construct a word which won't get chopped off into word pieces that are in the vocabulary. And so the number of mappings to unknown decreases because of that. Now it turns out, so we realize that word segments are the building blocks of the vocabulary. And so what happens? Here is your vocabulary. So imagine a little dictionary, not the Oxford dictionary, not the Webster's dictionary, but the Transformers dictionary, right? And its entries are the word pieces. And God knows how in the world would you give the meaning to ISA, but okay, some meaning is given. But what you do instead is for each of this token, instead of meaning, you give a number, a sequential number. Because if you, for example, if nothing else, alphabetically sort the tokens, right, then you can surely attach an index to it. The array, whatever the index thing is, you could do that. Or you could just hand choose and pick some numbers. People apply some conventions. So they have done that. You associate a number. So there is a mapping between each token and a number. The number uniquely gives a token. So long as you maintain the map somewhere, you have your own private dictionary, the tokenizer's dictionary, right? It is as good as Webster's dictionary, except that instead of the meaning of the word, you have a number for it. And once you have a number, what else can you do? Well, remember that words or tokens are categoricals. As I input their categorical variables, they're not a measure. They're not 13.6. So actually that thing that is just the location, if you think of a large array of zeros, and suppose the token ID, the input ID of the token is, let's take an example, 101. So you go to the 101 index in the array, and only that bit you flip up. You set that flag, you make it one. All others are still zero. This is your one-hot encoding, right? This is called the one-hot encoding. You now get, for each word piece, a vector. What is the dimensionality of the vector? It is the cardinality of the vocabulary. If your vocabulary has 1,000 words, then your one-hot encoding will have 1,000 dimensions. Actually, technically, it can have one less dimension, but you don't do that. When we do NLP, you keep it 1,000. You may say that the one last one is redundant. So, for example, if everything is zero, you can say if none if it is none of the words, it must be the word that I left out. The thousandth word or something like that. But you don't do that. You just make it the same size as the vocabulary. And you set a flag at the at the place that you would like to right and oh you're making notes here nice now it turns out that uh oh by the way if you write on this you have to erase also because otherwise i don't know there to be, you have to figure it out. Okay. So now there are many ways to do word pieces. It turns out that when you do multilingual words, then some of the presumptions of English do not apply. Isn't it? For example, in English, I said, you presume that, I don't know, words go in a particular way, right, from left to right, and so on and so forth, you start interpreting. But in lowercase, uppercase, yeah, these are assumptions of that. But if you're writing it in some other languages, you need to be very careful. You need to, for example, if you're using Mandarin, Mandarin, some of these assumptions don't work. I don't think there's an upper lower case in Mandarin, is there Dennis? Dennis Hirschhorn I think the only one that's too, that came from like, you know, the hieroglyphics, sort of like capital. Yeah. Like whatever they actually write in the data. Okay. So it's pictographic, right? A house has a symbol and that symbol is it. There's no upper, lower case. Yeah, that's right. Yeah, nice. So there we go. So what happens is you need a different tokenizer that works better for multilingual models, multilingual text. And such a thing is sentence-piece and we'll see it in a moment. Then GPT family of models, they tend to use yet another kind of tokenizer called the BPE tokenizer. So I will show you the result for one of them, I believe for sentence piece and very easy homework for you is just use the BPE and see what output comes out. I'm sure you can do that, just have to substitute BPE there, tokenizer. So now when you give it the tokenizer, now comes the interesting part. Now comes the interesting part. You can give, suppose tokenizer giving it one data point, data instance or datum at a time, one sentence at a time is rather inefficient because the way these things run, especially on the GPU is they're tensors. You can pass a whole batch and it can do the whole matrix computation, forward computation at one go right so why give it one at a time so one easy way is you can give it a list here we are giving it a list and this is just to illustrate the point that it works just fine with this right so suppose you give it more i gave it two to r is human to divine, to learn is to live. And what does it produce? It produces these tokens, those identifiers. Remember those IDs, those numbers in our dictionary, in our tokens dictionary, the numbers that you are hugging face APIs, they call it the input IDs. IDs would have been fine. Input why? Why input IDs? Because they will go into the model. The presumption is tokenization is a pre-processing step to something else that will come after. So that's why you use the word, the prefix input ID. Then comes token type ID I'll mention in a moment. Look at attention mask. Do you realize that of the two sentences, to live is to learn is a shorter sentence? Isn't it? So if you make the encoding of equal length, so I've said padding is equal to true. So I'm saying just for matrix multiplication, just make them both of the same length. If I do that, what do I do about the fact that to learn is to live? And then what do I do? You pad it up with, like you put this special characters saying, ignore, ignore, ignore, but you also do something. And this is where attention is beginning to kick in because you know that you're likely going to run it through a transformer another transformer you're saying you're giving a hint to the transformer that don't even bother paying attention to these tokens make sure that nothing is paying attention to these tokens so these are attention masks you're hiding it from the attention mechanism so when it is zero you're hiding it when it is zero, you're hiding it. When it is one, you're not hiding it. So that is the mystery that explains the mystery of this column. So when I just give it one sentence, of course, you have to, everything is open to attention, isn't it? That's why this attention mask here, all the values are one. Do you see? Now what about this middle column, token type IDs? What does this signify? This, let me explain. now going back to the BERT architecture that we covered last week what do we give we give a pair of sentences isn't it two segments two sentences and so we need to tell that this word belongs to which sentence the first or the second isn't it and how did we do it that was the segment id we call it the segment embedding of the segment id we call it the segment embedding of the segment id and that is what this token type ids is it tells which of the sentences it belongs to and of course you start counting from zero so the first sentence right if you were to give it as a pair the first sentence of course has the id zero isn't it and that's what it is. That's why it's zero. And that's an explanation for that. Oh, what is the question? Sorry. NER is different for question answering than chat GPT-3 or sentiment analysis. I asked this question as a prompt. As a logic, NER is different to sentiment analysis that was discussed before. NER is different for question answering. Okay, let me explain about what NER is. Sorry, Asif, I was typing too quickly. I was also listening to you. So, we were talking about sentiment and analysis, then you brought in named entity recognition, you brought BERT and distal bird, are they better or worse in terms of the specific, which is named entity recognition? We have sentiment analysis and we also have question answering, which kind of chat GPT does. I also apologize in advance for my grammar. I was typing pretty quickly and apparently we can't edit. Okay. So I'll let you, that was my question. I'll leave it there. So let us unpack it. So we have a lot of things playing in that question. First is chat GPT. So let's keep chat GPT aside for a moment because it's a very large language model and there's more happening than just one transformer there. So let's call chat GPT a composite thing and keep it aside. Now let's come to this question named entity recognition and sentiment analysis. See named entity recognition and the other thing was BERT versus Distilbert versus GPT. So let me answer these three things. See, we say that to do named entity recognition or to do sentiment analysis, these are tasks. These are different tasks. Right. So when you are using a model for one, you explicit. So let's take anyone sentiment analysis internally. You can imagine the following three things to be happening you are tokenizing text some input text you're running it through a transformer whether bird distill bird whatever and then you're taking some output either the average of all the tokens or all the all the hidden states but generally it's not necessary you just take the output of the CLS token remember the word architecture and you feed it to a classifier and say what is it now you may do sentiment analysis as thumbs up thumbs down you may do it in a predict into a set of states sentimental states joy fear anger so on and so forth right So that is an example of sequence classification. Isn't it? A good class, that a model that you can use is a model that has been trained on classifying sequence or a text, a sentence. On the other hand, when you come to named entity recognition, things get more interesting. And it's very simple as form. You are asking, is this token a named entity? Named entity means person, place, or organization. Right? A person is John, a place is Paris, and organization is World Health Organization. Right? Or some company, right? So Support Vectors is an organization, right? The most famous of them. So then it is a classification. If you really think about it, what are you doing? You're looking at every token and asking, is it a person, place or organization or none of the above? Right? That is one way to pose the question. Or another way to pose the question is, is this token a named entity, like person, place or organization? Or is it a verb, adjective, noun, blah, blah, blah. You can classify it into one of those, right? But in any case, it's at a token level classification, right? So those are two different tasks. Now- Asif, sorry to interrupt. Just before we go into the other thing, would name entity recognition, I know we talked about sequence to sequence in another, and I think a class or two classes before. about sequence to sequence in another and I think a class or two classes before. Do you think named entity recognition would still be valid for other languages like you said Hindi or other languages? Absolutely valid. Like for example, if you say, Krishna Delhi ghum ke aaya. Krishna is a name. Delhi is a place. Delhi is a place. And so forth. So the named entity recognition is global. It's a language agnostic. You just need a named entity recognizer that has been trained on that a corpus of that language so long as it has been trained it does a very good job now recently there is also a movement to see if you could do some sort of a zero shot a few shot learning and whatnot right but let's not get into that but in its simplest form so long as long as you have been trained with the corpus and label data, it will work. In any language, it would work. Next, I apologize, Shalini, I don't know Tamil. Otherwise, I would have given you an example in Tamil. Asif, you don't have to apologize. Your Tamil is probably better than mine. Your Tamil is probably better than my mind. Okay. All right. So the next, so that's a distinction between the two. Now comes the question of which model to use. See, there are two aspects to a model. Given the same dataset, what is the accuracy of that? What is the performance of that model? Pick a performance measure. For simplicity, I'll use accuracy though. I hesitate using the word accuracy because people get fixated on accuracy whereas in most real life practical situations it is either the precision or the recall or something else that is more valuable. To illustrate the point, if you have a screening test that checks whether somebody has, name your cancer, prostate cancer, breast cancer, these are two scary things for men and women. So as a screening test, what do you want to do? You know that if it comes out, even if there's a slighter suspicion of cancer, you would rather say yes, a positive indication. Because why? Because if this person doesn't have cancer, the subsequent test will eliminate it. But the last thing you want to do, so the price of a false positive is you'll scare this guy for a day. He'll go home and write some emotional obituaries for himself for something like that right and two weeks later another blood test will show he's totally fine on the other hand if you don't do that if you are false negative the price is catastrophic because a lot of these cancers they're very aggressive and time is of the essence what stage of cancer it has and whether cancer has metastasized determines the survival of this person the way existence so the cost is very high if you miss it the cost is relatively low if you have a false positive right There'll be a little bit of a drama, but that's about it, right? So what do you want? Do you want to have high recall? You want to make sure that you don't mess a person with, at the cost of precision, right? You, a lot of people, you declare you have cancer, right? They go to the subsequent test, half of them come out fine. So remember, which measure you use should be determined by accuracy. And this is very odd, actually. Even with well-known, studied data sets, like the breast cancer data set, which is probably the hello world of classifiers. If you go and look on the wild, most of the Jupiter notebooks and articles that have been written, they will say, hey, here is my classifier to look at breast cancer. And by the way, see how accurate it is. Accuracy is meaningless. Do you realize that? The one measure you should look for is recall in a screening test. On the other hand, if your test is the one that comes right before the surgeon is standing with his operating room door open and his big knife ready, he is looking at you. At that moment, you want to be absolutely sure that this person does have a tumor that is going to cut open and take out. So at that moment, that particular test needs to have what? A high precision. You need to know that if it says you have cancer, you have cancer. Isn't it? You're not sending in false positives. Right? So situation determines which metric you should use. Remember that. So don't get fixated. So with that story aside now, let us take a measure, some metric. I will use accuracy, but not, I don't mean it, some measure. We, so this question comes now to the second part. Given the same data set for a particular task, take a task. Let's say that you take a task sentiment analysis or classification into subjects, politics versus science versus sports for this text or whatever, pick your classification task. Now you look at the measure that you care about. Actually, because I tend to warn people against accuracy, I'll take F1 score. Let's see. The composite of precision. So I have heard, so I know F1 score people, how do I say this nicely? In industry, people understand F1 score than they understand in academia. So can you kind of clarify what your stance is? So just so I can recalibrate what you mean by F1 score. When you do F1 score, so first is F1 score versus accuracy. Why use F1 score? F1 score is, in my view, a better measure when there is no dire reason to use either precision or recoil, right? Because F1 is the harmonic mean of the two. So you have arithmetic mean, you have geometric mean. F1 is simply the harmonic mean of the two. Are we together? So what it means is that if either, see, remember, as a harmonic mean, it has a problem. If either the precision is too low or the recall is too low, it will adversely affect the harmonic mean. Isn't it? So think about it. Let's say the precision is 0.1 but recall is 0.9. 1 over 0.1 is 10, right? And 1 over 0.9 is well 10 over 9. Let's just call it, what should we call it? 10 over 9 is 1.1, let's say, should we call it 10 over 9 is 1.1 let's say right so you are seeing a 10 plus 1.1 right is 11 and so 1 11th what was the score so f1 becomes 1 11 what was the score leaning towards towards the worst number right isn't it So F1 score has one benefit. It sort of overweighs the worst quantity, right? Whichever of the two is worse. It sort of says that this is the, like it sort of gives emphasis to the worst of the two. And therefore it's a useful measure. Accuracy is not, I don't prefer accuracy whenever there is a class imbalance. And let me make this obvious from the same example. See, actually, perhaps I'll tell a different story. You know, before modern medicine, to take on the task, let's say I take a task named anterior cognition. Then you ask this question, condition on this task, which model is better? Then comes, but that question also is conditioned because what corpus of data did you train it with? So suppose you train one, let's say you take Bert, and you train Bert with a smaller corpus, and you train Bert with a larger corpus, right? You train Bert with a corpus of just 1,000 documents. And let's say that the task is sentiment analysis. And you take Bert, and you train it with 100 million, or I don't't know 20,000 or 30,000 data points which do you expect to perform better obviously that particular bird the same architecture the same thing that has been trained on more data isn't it so we are we have to condition on two factors which is the task which is the corpus they were trained on. Now, assuming that they were trained on both, both had the same task, that's a sentiment analysis, and they were trained on the same large corpus. Then the question is, which is better? I mean, BERT, distal BERT, or let's say GPT-2 or something like that. So there, the answer to that is, generally, distilled bird, by its very nature, has learned from bird and approximates its performance. So it will always underperform bird, right? So Asif, I just want to ask. Just hold your thought. One second. Let me give it because a lot of people are waiting for the answer. Distilled BERT is trying to approximate BERT, so it will definitely have a little bit of difficulty catching up to BERT. But there is more to it. So it will be a little bit like. Now the question is between BERT and GPT-2, let's say one of the autoencoder, the decoder side of the models, which will do better. Now, if you take just GPT, so not GPT-2, if you just take GPT, clearly there's evidence that BERT does better than GPT. GPT-2, I don't know exactly what the answer is and how they are comparable, but you can look it up. You can look up what it is. Now there is one more sort of a nuance to it. The nuances, if your data set is smaller, then what should you train? Well, the thing is, if you take a small data set and you train a very large model. Now, what will happen is you'll have massive overfitting. Right. So you don't want to do that. The bigger the model, the bigger the corpus you need to train that model and still not have overfitting, right? Because more data regularizes a model and complex model have a huge problem with overfitting. So you have to also look at the size of corpus available for you to train the model, right? That is another dimension that you have to take into view. And so with that things all being there, Shalini to your question that and I've kept chat GPT away, I'm just looking at the standard transformers. I would say that a distal bird assuming sufficiently large corpus, assuming that the task is let's say sentiment analysis, a distal bird will slightly lag but not by much the bird BERT, especially. And if you use a bigger BERT, bigger BERT will do better. A BERT, a large, will do better. If you compare that to just GPT, because GPT is one word at a time, sort of a, you know, one token at a time emission kind of thinking, generally, and BERT is bidiredirectional it pays attention to the whole sentence it will tend to do better for this task right so tasks specific the performance difference I'll come to you the next part is chat GPT see what happens is chat GPT is not a model it is a composite it's a pipeline of many things right amongst other things is chat is GPT-4. GPT-4 is a ginormous model of a trillion parameters, which has been trained on huge amounts of data. There is reason to believe that chat GPT at this moment seems to be for general purpose tasks. Let's say the sentiment analysis. I have not seen any objective study, but there is people generally believe that Chad GPT beats BERT. Please do the experiment and look for that evidence. See if it is really true or not. And so task by task, you can go and see whether Chad GPT, I mean, I wouldn't use the word chat GPT, I would use GPT-4, whether GPT-4 beats BERT or not, I would imagine that it does, right, simply because it has been trained for far longer and it has been, it is a much larger model. Okay, so now let me take a question. So for Shalini's question, Shalini, go ahead with your question. No, I think you've explained correctly. Any other questions, I'll post to Slack. But I think you've explained my precise question. So I think the summary is that you're saying that, you know, distal BERT was trained on a larger model. Sorry, BERT is trained on a larger model, but distal BERT is trained on a smaller model, but it's the dataset that matters. You're looking at the F1 score and not accuracy because accuracy is very dependent on the dataset. So that's kind of my summary of where we're taking it. Yeah. That is right. So I get it that for a large model, you need large dataset. Otherwise you won't get it that for a large model, you need large data set. Otherwise, you won't get it. But I think we talked about 5G. And 5G in a large model, you only have small data set. So is that 5G in a remote? Yes. Very good question. Masmi has asked a very interesting question. I'll repeat it for all of you who are remote. She says, I made the statement that large models need a lot of data to regularize them, to prevent overfitting. But what about the fact that I also said that for fine tuning a model, you don't need so much data. Right, it can't do that. So how do you explain this paradox? Anyone would like to take a stab at it before I answer it? How come you're fine-tuning a massive model, but in fine-tuning you don't have an overfitting problem? How did that happen? It doesn't shift the model too much, shift the model too much because it recognizes the weights are already set for a certain structure and a small sample size you know you're not drastically the way the instruction is made it just focuses on that data that is That is right. That's absolutely. So what Patrick said, I'll repeat. He said that in smaller data sets, because you're looking at a model that's been trained, the weights are more or less trained for an adjacent problem. And you're just doing small changes, very small changes, to make it even better on this small data set. So when you patch this, run a few epochs over this small data set, the weights will just slightly perturb it. But regularization has already been taken care of by the big data set that trained or pre-trained this model. Remember, fine tuning is the second half that follows the pre-training. Another way to look at it is that the total data set that this model has seen in its full training journey is the vast data set that pre-trained it plus this fine training. So the total data set size is still vast. And so it is a regularized model from that. So how does it, for example, like if it has trained in this vast data set, let's say for like for Genentech, if we have a smaller data for our clinical trial set, we use it to find different model. How does the model keep it segregated and not dissipate everything else? No, no. What happens is it's a very, so long as the problem is the same, I'll take an example. Suppose you, and this is a real thing, you trained a model to do sentiment analysis. We did this example actually in one of our previous labs, to do sentiment analysis and it is pre-trained on general text. But people in the financial world, they speak a peculiar dialect of their own. They have bears and bulls and all sorts of things, derivatives and whatnot. So what you do is you need to just make it understand that little bit. And so what happens is when you take a small corpus of financial text and run it over that, it has already gotten the gist of the English language, right? So, for example, if it encounters the word, I hate, hate, hate today, right? So, sentiment is pretty negative, right? And financial terms are not, I mean, there's no financial term that will say, oh, this is great, right you need to be uh be more right like you know that if you use the word this company is i think this stock is highly leveraged it's a negative you don't want to buy it no but in english you wouldn't know what leverage means right it's it's right if something is highly leveraged well what is it so you need to give it that little bit of weight change so that it starts recognizing this and when you that's why i use the word minor perturbations you don't see drastic changes you just see fine tuning people use the word fine tuning for that reason and the weight change will only be for your own usage for your own usage because the model that comes out is your model you started with somebody else's model and you tuned it. Now when you save the model, it's your ways. That's it. Okay, guys. So with that, we move forward now to word segments. And we talked about word segments. We talked about the attention mask. We now know all the three components of this. So to summarize, we know what input IDs are. This is the index in the vocabulary. So imagine that there's Webster's dictionary, there's Oxford's dictionary, and then there is the tokenizer's dictionary, right? So where the word pieces have IDs as their meaning, right? So you just look up the ID, which you use for hot encoding, when hot encoding. Token type IDs is not terribly relevant for us here for the use cases we're looking, but it basically says that when you're passing it to a bird-like architecture, is it part of the first sentence or is it part of the second sentence? The third one, attention mask matters. Is this fair game for attention or is it just padding and you need to ignore attention from it? That's all it takes. So we understood the three parts. Attention mask. And... No, no, no. Vocabulary index never changes. It's the weights that change. Remember, if it encounters a word that is not in your vocabulary, it will just map it to you and you and K. Yes. Fix the maximum set of a variety of tokens then is the vocabulary size. That's it. So you always, every tokenizer starts with its decided vocabulary that's it gets mapped and it is not as terrible as it sounds why because the meaning of that word is inferred from the context that's the whole point of attention. So all is not lost. Right? That is it. So let's say that the word zebra was never there. And the sentence is, the zebra ran with all that sounds a lot like horse. That's fine. That's a point. So you notice that even if you give it a nonsensical word, when you use word pieces, like I used a Jabberwocky statement. Jabberwocky is this beautiful poem that as kids we all love, if you read it from it, right? And it says, it starts with, it was Billig in the slithy toes the giant jimble in the way and mimsy with the borogos and so forth and when you read that it sounds very sensible except that when you think of the words none of the words make sense right so i took jabberwocky's first sentence and tokenize it and one of the interesting things is that when you do word pieces it will actually break it up into things it has so brillic is an unknown word but it broke it up into br because br is the prefix to many words brother and whatnot and uh ill still bill hill mill so it got that and ig I don't know what what ends with egg. Twig. Pig. Twig. Oh, yeah, there we go. So many work with that. Yes. And a slithy again, slit and the hy hy is many, many, many, many, many, many things have suffix and VESE-S. Oh, lots of. A word galore that ends with V-E-S, doesn't it? And so you notice that none of them got mapped to unknown. I give it to you as an exercise to find a word which will get mapped to you and K. Okay. Right. So now, the process of encoding or tokenizing the converse of it is decoding. So when you decode it, you realize that when you decode this, it gets to this, it was billing in the slithy toes sandwiched between the CLS and CEP. CLS is the special token that you'll use, for example, for classifier. CEP is the separator between the two statements for both. Now notice that everything has been lowercase. So it has been lowercase. But that is what this tokenizer does. So you realize that the tokenizer does more than just tokenize the words into pieces. It does some pre-processing. It does some post-processing, isn't it? So let's go into that a little bit more. So when you look at Jabberwocky and you look at the token. Now, one more thing is there. When you tokenize, if you, instead of using the tokenizer as a function itself, callable function, you invoke the tokenize method, it will deliberately give it to you without the paddings, without the CLS and sub-paddings. It's something to know. You notice that it did not produce those things. Yeah, it's one way. Now you can convert tokens to IDs and you can decode it. It will give it back to you the thing as it is. Now, one more thing to do is to, you know, we use the auto class, right? Auto tokenizer. You don't have to. If you really insist, you know what you're talking about, you happen to know through private channel that bird-based un-cased is a checkpoint that used bird. What's that private channel? You go read the documentation on the Hugging Face website. So offline you know that it's a bird model. So then you can explicitly say bird tokenizer. You're tokenizing it for a bird model. You can do that. And pretty much the same thing will happen. Then you see, BERT model will tell you that BERT tokenizer will declare that it is, it tells you a lot about itself. It says, oh, BERT tokenizer uses a vocabulary of 30,000 words, pretty large vocabulary, frankly. And now in tokenizers, there are two things. uses a vocabulary of 30,000 words, pretty large vocabulary, frankly. And now in tokenizers, there are two things. There is a fast tokenizers. Let me leave that as a homework. What is a fast tokenizer? It's a homework for you guys. Post the answers to Slack. Let's see who gets there first. Padding site means you pad it on the right. Like if your sentence is shorter, you pad it with extras on the right. A truncation site also right. So suppose you give a sentence that leads to more than 512 tokens, chop off. Why are we chopping at 512? If you remember the BERT architecture, we said 512 is the token limit that it will take then if you need more tokens use some other transform right use big bird right or something like that so special tokens what are the special tokens it used for unknown tokens it use it uses ank It uses ANK, separator, SAP, padding, PAD, and class token, the first one, CLS, and mask, MSK, mask. That is if you're doing a masking job, mask language modification. So as a code for BERT, it's 512 input and then 512 output? No, no, no, output is up to you. The hidden state for a typical BERT is 768. The hidden state, and we'll see that in a moment. But at this moment, focus only on the input and the tokenizer part. So what is it doing? The tokenizer, if you use for example the XLM Robota base, which is good, quite commonly used for multilingual text. And by the way, multilingual for me is a reality. I tend to, I used to have under me as a leader, many teams. I had a Ukrainian team. I had a Paris team and I have a team in India. And what were some very interesting things you learn. So I would do this daily meetings and people felt very free with me like they would never really took me seriously. So they would, you know, they would get heated conversations and so forth. So with the Ukrainians, what would happen is when they had difficulty explaining, they would suddenly transition into Russian. they would suddenly transition into Russian, right? And then they would look at it, just explained you, right? And I have no clue what they said, you know, and they wouldn't realize that they just spoke something in Russian. So they have a mix of English and Russian. And I had to sort of read between the lines to figure out what they must have said, or they would use a Russian word for something. So I literally had to use my own attention heads to figure out what they must have meant, right? When I would talk to, a similar thing would happen with the French people, right? And then with India, it would be very interesting if two people are debating and they're both Telugu. Before you know it, they're speaking Teluguugu and if those two people are from Kannada they're speaking in that right and suddenly sometimes some peculiar things would happen one guy would suddenly launch into Tamil and then the Kannada would say you know I don't know Tamil in English right so the conversation would be actually multilingual in my meetings so well if you're parsing a multilingual in my meetings. So well, if you're parsing a multilingual conversation, you do need an auto tokenizer that's multilingual sensitive, right? So you use xlmrover.tabase. It does a pretty good job, but notice that the tokens it is using are, do you notice that? What is it using it for the start of a sentence yes right the html notation almost html markup notation and slashes which is again the end of sentence um that so that is it and then it says any word start is with an underscore right and if it doesn't start with the underscore then it probably is just a suffix right So every tokenizer will have a different convention. I leave it to you as a homework to find out what sort of tokens that BPE will generate, BPE tokenizer generates, which GPT uses. So now I also said that see tokenizer itself is a pipeline. There is more going on in the tokenizer than you think. So there are four steps in the tokenizer itself is a pipeline. There is more going on in the tokenizer than you think. So there are four steps in the tokenizer. First is basic cleanup. So what's a basic cleanup? String whitespaces, multiple whitespaces you may want to condense into one or something like that. Remove accented characters, lowercase everything. Those are your normalization steps. So D-O-G capital would look the same as D-O-G small case and mixed case and so forth. All of those things you do. Normalization is something that happens from other languages. I don't know what it is. If somebody can explain to me i'll be happy i mean no point in my regurgitating the definition of normalization but apparently the same thing can be written in multiple ways uh in i do know in hindi and sanskrit you have the word the there is a sort of a wobble or not wobble or something not a wobble actually but a r right a half an r you attach now you can attach before the word which will be at the bottom of the character or above the word as a curly crescent and that will come after the word right and so there are these peculiarities so I'm assuming that normalization and that will come after the word, right? And so there are these peculiarities. I'm assuming that normalization somehow has to do with that. I don't know, right? If somebody knows what it is can enlighten. So you do all of that cleanups and so forth. The second thing you do is you do pre-tokenization. Simplest way to think of pre-tokenization is you have this sentence. If you're, for English, if you're breaking it up into words, breaking it up into white spaces, right, that's a pretty good way of thinking of pre-tokenization. Or, you know, what, if you use Java, for example, think of the tokenizer of Java, right, word tokenizer of Java, word break, how would it do? It would break it up into words segments or word pieces either sentence p you can use word piece sentence piece bp whatever but word segments i'll use the word word segments breaking it up into logical word parts that are part of the vocabulary so tokenizer is the only one that makes sure that everything is broken down into something in this very well-published tokenizer's dictionary, right? Not Oxford's dictionary or Webster's dictionary, but the tokenizer's dictionary. Its vocabulary must contain it, right? So you should say page 13, there is this token described, right? And it has an index to it, that sort of thing, giving the metaphor of an actual dictionary with pages. And post-processing does what? Oh, you need to still pad it up with something. CLS, CEP, PAD, MASK, and so on and so forth. All of those special tokens that need to be inserted, that will be the post-processing step so tokenizer pipe is a pipeline in itself it goes through four steps are we together guys and enough for tokenizers we'll take a 10 minutes break oh my goodness it's already getting to be five do you guys have have okay no if it is five then we should start the next topic next time go ahead please uh so when you're talking about the multi-language uh tokenizer so how does it differentiate between like a token from like english versus like a token from french or spanish it does it moves you cross train it on corpus that includes both the languages okay so is there like a way for you to specify that i'm going to use these two languages oh yeah yeah you can do that I'm going to use these two languages? Oh yeah, yeah, you can do that. But generally these things are pretty good at even sensing. So one of the NLP tasks is you see character, which language is it? Language detection. Right, and so they're very good at it. Language detection is generally fairly accurate. protection is generally fairly accurate. Okay, guys, I wanted to cover one more thing, the classifier. We will get to the sentence. So what is the journey ahead? Our journey ahead is we are now going to feed this tokenization into a pre-trained classifier. We'll stay with our running example of sentiments. We will see how it is that we can do it, not using the pipeline, but by hand doing the pieces, putting the tokenizer, taking its output, converting it into the PyTorch format, then feeding it into a PyTorch model, like let's say Bird or something like that. Taking the output, then putting our own softmax or some classifier layers, running it through that, seeing the output. Then what happens is, remember, this is one part of the journey. But we are used only pre-trained. There is no fine-tuning here. So the next thing we'll do is we will take the emotions data set and we will now fine-tune BERT. data set and we will now fine-tune bird specifically this whole pipe will fine-tune the model to do a much better job on this particular task emotion detection right and see how well it can do and we will see so to give you a sense we will see that you'll go the pre-train will give you accuracies in the 60s, which is pretty good considering that if you're predicting in six classes, you're gone from somewhere like 16% to 60s, pretty huge lift, but you can do better. When you fine tune, you will go into the 90s, right? And when you go into the 90s, especially considering that there is a class imbalance, you will realize that that's a pretty good lift and that argues in favor of the fact that you should never skip the fine-tuning stage. A lot of people, they just take a pre-trained model and say, I'm done. It's working good now. But it's worth fine-tuning more, right? it's working good enough but it's worth fine-tuning model right that stage is worth doing the second thing we will learn is once we have fine-tuned the model we will look at those cases where the loss was high the prediction loss because see when you predict a number and you compare it with the answer. Your probability says, let's say for anger, it says 0.1. But the answer is anger. So the loss is high. Cross-entropy loss is high there, isn't it? That's the whole point of the cross-entropy loss. So what is cross-entropy, guys? Going back to what I explained about classifiers and this, the cross-entropy loss is a big word, but all it means is it is the level of surprise you have. Right? So one easy way to think about it is that think about this. If you know that you think your model is good, presumption model is worse or the radio typically is the weatherman. The weatherman has said it's going to be the chances of rain are less than one percent and you go out and it's pouring. Are you surprised? It's surprising assuming you believe the weatherman which these days is hard to believe, given the California weather. But let's say that you believe it. Are you surprised at the weatherman's prediction now? Right? So let's say that he said one percent chance of rain, it's pouring outside. How much is the surprise? Suppose he had said 99 percent chance that it rains. You went out and it's pouring rain. Are you surprised? You're not surprised. But when he said probability is 0.1 or 0.01, one person, then you have a massive surprise. So you say, well, how much is the surprise? One way to look at it is to say, well, it must be the reciprocal of the probability. Think about it. If the probability is high, predicted probability is high, my surprise is less because it's raining. If the predicted probability was low, now I'm really surprised at the model's prediction. Unpleasantly surprised, isn't it? So 1 over p may be a good way of thinking of surprise. So 1 over p may be a good way of thinking of surprise. But there is a problem with that. The 1 over p, if you think about it, what is the range of values it goes through? It goes from 1 to infinity. You may say that, well, when it said 100% chance of rain and it rains, why am I surprised? Why is the value even 1? I would much prefer if the values was zero. It went from zero to infinity, right? And so there is an easy mathematical trick. Mathematicians will say, oh yeah, that's an easy one. Take the log of it, right? Log of one is zero and log of infinity is still infinity right so log of reciprocal probability is a measure of surprise but you don't use the long word measure of surprise you instead use the word entropy right it's your cross entropy and that is exactly what the cross entropy loss is. You got it? That's why cross entropy loss is the summation over all the probability predictions and how wrong they were. As simple as that. Anyway, it's a recap of that. So today, guys, we didn't reach as far as we did. From next time, we'll have to move a little bit faster. I need to finish this hugging faces and we have a lot of interesting examples. But the point is it is far better to understand deeply and move slowly than to rush ahead. Any other questions, guys? I'm totally open. We have 15 minutes. Go ahead, Sushil. So, the whole of the open user followed by this no no not sentence transformer our transformer remember sentence transform is a special architecture that uses a transform in a in a simian architecture right network two of them put together no ignore sentence transfer just transform yeah so classifier is therefore so let me give you a preview of what it means what we'll come to and we'll repeat it next time so imagine that you know what a classifier is. You have a classifier, you give it an input vector, output will come up. The problem is we don't have an input vector, we have a text. So what is the journey that we need to go through to classify? First thing you need to do, you need to tokenize it and make hot encoding. But then you realize that if you feed it into a direct classifier, your performance will suck because it is not attention. It hasn't gotten the benefit of attention. The semantic awareness is not there. The context is not there. So what do you do? You sandwich and transform a body into it. And it's so turns out that these transformers that are posted on Hugging Pizzles, most of them, they're headless. Remember, they're zombies. So if you want to classify, you need to screen a classifier head on top of the transformer body. So the transformer is sandwiched between two things, tokenizer before it and classifier after it to make the complete pipeline text classification pipeline you got it right so look at the picture on my screen that's what it is there also because even the tokenizers have been trained right so in fact why don't i let you explore the question it's interesting go check it out the code of the tokenizer is available see how it is done okay so i'm trying to fix the we are trying to do a pre-trained model. Yeah, the pre-trained just means that somebody has already created the tokenizer for you. And it's pre-trained in the sense that it is completely adapted to the, for example, if you get bird-based uncased, it is completely adapted to producing an output that can just feed into bird. But that would have assumed some trap tokenism. Yeah, and the transformer assumes the tokenism. All these two have to go together. They have to go together. We can't pick arbitrary some tokenism model and... Yeah, the reason for that is why are the two broken up? The reason they are broken up is one, even though they are producing the same vector, embedding, the same embedding to go into the transformer, remember the fact, or not even embedding, just the hot encoding vector, one hot encoding, they are producing the same. Given BERT, let's say that we are taking BERT case, uncased base model. One tokenizer is good at English. Another tokenizer may be good at Mandarin. Another tokenizer may be good at multilingual. Another may be good at Tamil, right? And that is why you don't want to sandwich the tokenizer. You don't want to put the twoizer. You don't want to put the two together. You have far more flexibility. If you let the transformer just be the transformer and let the right tokenizer bring it the same one-hot encoded vectors that it expects. And it doesn't care how it got that vector because the different tokenizers are better at dealing with different languages. That is the point. Now you got it. And you don't want to create one mega ginormous tokenizer that does everything perfectly. That is it. So these are, see, you're used to Unix, right? What is the basic Unix philosophy? Simple commands, it does one thing well, and then you use the pipe. And you weave the pipes together and before you know it on that one command line, you can achieve what some commercial software does after charging you $99. dollars. That's a basic philosophy. So the whole idea is pipeline, build pipelines out of reusable components. That is why tokenizers are interchangeable based on what corpus, what data you have, use the right tokenizer. Models are interchangeable. But of course, once you pick a tokenizer, you better use the model compatible with it. Right? That's it. The name will be a giveaway. The tokenizer name will start with, like a BERT case, tells you clearly that I'm going to feed into BERT. Right? That's it. Generally, there's checkpoints, right? The names are such that if you use the same name for the tokenizer as well as for the model, you're doing pretty well. How do you? No, no. So the documentation will say if you go to the card of that thing, the tokenizer, it will say that it has been trained for english or for this or it's good for this it will say that and that brings us again the same thing right people are posting a lot of things on hugging phrase doing good diligence and writing good documentation means people will actually use your stuff if you don't document it properly nobody knows what it is for. Give sample code, document it. Okay, that brings me to one thing, guys. This file, the classifier, auto classifier, don't use it. I mean, you can use it, but the last element, I think has a bug at this moment. I need to fix because the things have changed a little bit. But now, this book, the book that I was recommending is there in this, \"'Natural Language\n"
     ]
    }
   ],
   "source": [
    "FILE = \"/home/asif/Desktop/test-lecture.mp3\"\n",
    "transcript = asr.to_text(FILE)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b763a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No configuration file specified. Trying the default location\n",
      "WARNING:root:Loading configuration from /home/asif/github/llm-bootcamp/bootcamp-config.yaml if it exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/asif/github/llm-bootcamp\n",
      "\n",
      " It's April the 8th, Saturday morning, a beautiful morning. It's more than 50 degrees, which is a really nice thing in California these days.\n",
      "\n",
      "\n",
      " This is our fourth session. The first session was sort of basic and introductory. The second two sessions, we did some real stuff. So I'm going to start by recapitulating what we have done so far.\n",
      "\n",
      "\n",
      " The first thing, broadly, is that we are in the world, when it comes to natural language processing, the one statement that goes with it is, it's transformers, transformers all the way right just about anything you want there's a transformer for it for doing to recapitulate we did a search engine using ai semantic search engine and we noticed that we used a transformer architecture a very interesting architecture we used a siamese network and in Siamese network, we would pass in the way we would create semantic embeddings, train the semantic embeddings by using a transformer.\n",
      "\n",
      "\n",
      " We will recap that and the whole thinking behind that.\n",
      "\n",
      "\n",
      " These transformers produce embeddings, and those embeddings is what we use to do care nearest neighbors. And then we realized that we could do approximate care nearest neighbors and so forth.\n",
      "\n",
      "\n",
      " So we quick, breezy stuff. We'll do it a little bit more in detail shortly.\n",
      "\n",
      "\n",
      " Then we went into the architecture of the original transformer, which is mentioned in the paper. Attention is all you need that paper is landmark paper and in a moment we'll go we'll review that paper too\n",
      "\n",
      "\n",
      " but basically it started the whole transformer revolution this architecture that was proposed in attention is all you need came out in december 2017\n",
      "\n",
      "\n",
      " by the time people returned from their Christmas holidays and the New Year's holidays, people could sense that the world has changed effectively.\n",
      "\n",
      "\n",
      " Right after that, in very quick succession, a whole lot of things began to come out.\n",
      "\n",
      "\n",
      " There came the BERT, which is an architecture that we did in considerable detail.\n",
      "\n",
      "\n",
      " It's a bi-directional language model.\n",
      "\n",
      "\n",
      " It uses masking of words, random masking of words and asking the transformer to infer what words have been masked.\n",
      "\n",
      "\n",
      " Again, we'll do it in a quick succession.\n",
      "\n",
      "\n",
      " Then came a few months later, then came the sentence bird, which we use, which was literally the foundation of the whole AI-based semantic search. And it pretty much was the next big breakthrough in search engines. People have been doing linguistic search or keyword search for a very, very long time.\n",
      "\n",
      "\n",
      " The theory was really well established.\n",
      "\n",
      "\n",
      " There are mature libraries like Lucene, Apache Lucene, as the core engine behind even larger open source projects like the Apache Solar and the Elasticsearch, which put even more layers of functionality on top of it and make it quite easy to use.\n",
      "\n",
      "\n",
      " They put a whole distributed application stack on it.\n",
      "\n",
      "\n",
      " Now, then came the fact that you could do semantic embeddings and you could immediately see what a vast difference it makes in the quality of the responses.\n",
      "\n",
      "\n",
      " Instead of being keywords driven, this understands the intent. What are you trying to say.\n",
      "\n",
      "\n",
      " It can search for content that is very relevant, but that contains none of the words that you actually mentioned in your text.\n",
      "\n",
      "\n",
      " And it helps you do question answers and so forth.\n",
      "\n",
      "\n",
      " So you could already see the beginnings of things like with these transformers of what we do.\n",
      "\n",
      "\n",
      " ChatGPT is a prompt, it's a question and answer, roughly speaking, framework today.\n",
      "\n",
      "\n",
      " You can see the beginnings of all those ideas with the coming in of transformers and with sentence word.\n",
      "\n",
      "\n",
      " One of the things we did in the lab is use sentence word for QA, question answers. One of the demos that Junaid made was to show how you can load a corpus of diabetes documents in a very short corpus, yet you could ask enlightening questions. What are some of the things you should do to avoid diabetes?\n",
      "\n",
      "\n",
      " And it comes up with very sensible answers, things that were simply not possible with traditional search engines like Elasticsearch.\n",
      "\n",
      "\n",
      " So that, if anything, we started a course with that. The first lab we did was that.\n",
      "\n",
      "\n",
      " And the purpose was to show you how radically the world changes the moment you use transformers.\n",
      "\n",
      "\n",
      " Today, quite a few things have happened.\n",
      "\n",
      "\n",
      " First is this transformers, and using transformers, the large language models, have become bigger and bigger and bigger. As they become bigger, we find that their capabilities also get bigger. They start not only doing things with greater accuracy, but they develop what are called emergent abilities.\n",
      "\n",
      "\n",
      " For example, transformers at the scale of bird, bird base and bird large, they're not able to do arithmetic.\n",
      "\n",
      "\n",
      " You cannot say if I purchased 10 apples yesterday and then today I ate two of the apples, but then my friend gave me six more apples, how many apples am I left with?\n",
      "\n",
      "\n",
      " It wouldn't be able to answer that question. But as the models get bigger, we notice that surprisingly, they are able to answer these questions. Now, sometimes they answer these questions somewhat wrongly, but they try to answer these questions. Now, sometimes they answer these questions somewhat wrongly, but they try to answer these questions.\n",
      "\n",
      "\n",
      " Then came other sort of small set of breakthroughs, a series of breakthroughs.\n",
      "\n",
      "\n",
      " There is the notion of the chain of thought, COT.\n",
      "\n",
      "\n",
      " What it does is it gives you a way to give little breadcrumbs to the transformers, to the large language models, to reason through. And when you give these little breadcrumbs using the chain of thought process, then all of a sudden they're answering the arithmetic questions with fairly good accuracy. accuracy they're almost always right and so with all of this we realize a larger theme with transformers we are realizing especially in conversational transformers or which are becoming quite dominant these days how or what input you give the response seems to be or what input you give, the response seems to be dependent on that, obviously, but the quality of the response often depends on how you pose the question.\n",
      "\n",
      "\n",
      " So prompt engineering or prompt construction has become pretty much a cottage industry.\n",
      "\n",
      "\n",
      " On the web, you find a lot of websites literally devoted to the ways you can create prompts.\n",
      "\n",
      "\n",
      " I just ordered a couple scenes, a screen.\n",
      "\n",
      "\n",
      " What are we seeing? Are we seeing my drive? Yes, we are.\n",
      "\n",
      "\n",
      " So let's say that lost landscape visualization. So there is this paper, which I would encourage you all to read.\n",
      "\n",
      "\n",
      " perhaps given time I would like to cover it. But I'll just give you a very quick preview of this paper. This paper brought out these beautiful diagrams of how the lost landscape looks and with a lot of pictures here, but what we are going to do is see it in real life. So there is this website called lostlandscape.com.\n",
      "\n",
      "\n",
      " I would suggest you go and look at it. And when you do look at it, you will see that these, let me go to one of them.\n",
      "\n",
      "\n",
      " Oh, we are not able to play with this. There was a way to play with this.\n",
      "\n",
      "\n",
      " Explore. All right, guys.\n",
      "\n",
      "\n",
      " So it is loading.\n",
      "\n",
      "\n",
      " Just to recap, what is a loss landscape? It shows you the loss with respect to the parameters of the model. Now you know that these deep learning models, they have hundreds, thousands, or millions, and now million looks too kiddish almost, kindergartenish. These days, you're talking billions, and now trillion. But GPT-4 is supposedly a trillion parameter model, right, from what we know. We obviously don't know all the details about it, but we do know that it is so. Now you realize that when you look at this, there is a clear minima. Do you see at the very bottom, that pink place where the mouse is?\n",
      "\n",
      "\n",
      " There is a very clear minima.\n",
      "\n",
      "\n",
      " But at the same time, the lost landscape has this hills and valleys. And if you look inside it, and you imagine that you start somewhere up in the mountains, a random point, and you try to gradient descent, it is quite likely that you might get stuck in one of those little ridges out there or in one of those little depressions, little valleys along the way, way up in the mountains still. And there is no guarantee that you will necessarily go to the minimum, the global minimum. You can get trapped in local minimum.\n",
      "\n",
      "\n",
      " And so this used to be considered a major problem with neural networks.\n",
      "\n",
      "\n",
      " Today, we don't think of it so much like that, because we have developed pretty good techniques to escape out of those local minima and make our way towards the global minima, or at least something that's close to the global minima. You don't have to be at the absolute global minimum, but you do get to something that is practically useful.\n",
      "\n",
      "\n",
      " But you see that these deep neural networks have a very fascinating and rich loss landscape. Now, with these landscapes, the next thing is the concept of attention.\n",
      "\n",
      "\n",
      " Before attention came, people used to do natural language processing mostly with LSTMs, which are related to recurrent neural networks that are autoregressive or GRUs and so forth.\n",
      "\n",
      "\n",
      " Now, going back to what we said, so let's take the quintessential case.\n",
      "\n",
      "\n",
      " You want to translate from English to let's say French right or to Hindi or something there are two phases to it you take the input and you encode it you encode the sentence into a hidden representation an abstract representation that abstract representation has a certain amount of memory associated to it and then the decoder decodes it and forward generates the French representation or the Hindi representation and once you have an abstract representation a hidden representation you could do many things for example you could translate it into French you could translate it into Hindi but you could also do things that are similar but not probably so obvious you could translate it from plain text to poetry, right?\n",
      "\n",
      "\n",
      " To a sonnet or to a haiku.\n",
      "\n",
      "\n",
      " You could translate it into, I don't know, things like that.\n",
      "\n",
      "\n",
      " You could do it from one style to another. You could do a summarization, translate it into fewer words and so on and so forth, right? So you could do many things with a hidden representation.\n",
      "\n",
      "\n",
      " The trouble is when you use RNN style models, because data is input sequentially, the hidden state overwhelmingly represents what it has learned from the last few tokens. And it tends to forget what was learned from the earlier tokens.\n",
      "\n",
      "\n",
      " It sort of begins to erase.\n",
      "\n",
      "\n",
      " Very much like human beings, you realize that when we learn composition or diction in schools, I don't know if it is taught.\n",
      "\n",
      "\n",
      " We'll ask the young man here, Abdullah, whether it's still true.\n",
      "\n",
      "\n",
      " One of the first things we are taught is to write short sentences for clarity. There has to be a reason to use long sentences.\n",
      "\n",
      "\n",
      " Why is it?\n",
      "\n",
      "\n",
      " There are people, for example, there was a great British writer, Matulay.\n",
      "\n",
      "\n",
      " His sentences were proverbially long.\n",
      "\n",
      "\n",
      " It could start from the beginning of a page and it could go to the end of the next page as one sentence.\n",
      "\n",
      "\n",
      " He was famous and yet apparently it was wonderful writing sometimes.\n",
      "\n",
      "\n",
      " We don't practice that anymore.\n",
      "\n",
      "\n",
      " One of the reasons is our attention spans are smaller, but more than that, it is far more likely that given our short amount of memory or attention span, by the time you reach the end of that sentence, you have forgotten exactly who it is that we are talking about.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So same thing happens with this recurrent neural network based models like LSTMs and so on and so forth. the neural network based models like LSTMs and so on and so forth.\n",
      "\n",
      "\n",
      " So the other problem with those architectures were that they could not be, they could not really benefit from the whole deep learning revolution. You couldn't create deep layers of recurrent networks. One or two layers is about as deep as you went.\n",
      "\n",
      "\n",
      " Like not that was another, the third problem was in the computer vision space, already the people were benefiting from the accelerators, hardware accelerators, the graphic processing units and the tensor processing units.\n",
      "\n",
      "\n",
      " So already you were having all these companies like NVIDIA getting rich. So already you were having all these companies like Nvidia getting rich.\n",
      "\n",
      "\n",
      " So, but in the NLP world, there was no use because that parallelism couldn't be so well exploited.\n",
      "\n",
      "\n",
      " It was a recurrent neural network.\n",
      "\n",
      "\n",
      " You had to pass in one token at a time. Now the sequence of words had to be given sequentially.\n",
      "\n",
      "\n",
      " What transformer architecture did, or the big thing it brought is, it took the idea of attention, which had already been discovered, it was discovered in 2014, and people realized that attention is a useful concept. They were hybridizing the attention concept with recurrent neural networks and LSTMs. And when they would hybridize the two ideas, they would superimpose attention on those, already they were seeing much better results.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " Then, so the general belief at the time was that yes, RNNs and LSTMs are the workhorses, but there are ways to put it on steroid and attention is one way to put it on steroid.\n",
      "\n",
      "\n",
      " And then came the paper of 2017, which basically said, you know, surprisingly, you can do NLP, you can do an encoder-decoder model architecture simply with attention heads.\n",
      "\n",
      "\n",
      " You don't need RNNs or LSTMs or GRUs at all.\n",
      "\n",
      "\n",
      " That's a pretty remarkable statement because that was the choke point. That was what forced you into a sequential way of thinking, right? And that is what prevented you, in some sense, from benefiting from hardware acceleration.\n",
      "\n",
      "\n",
      " So when you could put things through the attention head, it could all go in parallel. Not only that, you could have multiple attention heads. So you could look at that whole process of encoding from different perspectives in a way.\n",
      "\n",
      "\n",
      " One was more syntactically closer. One was looking more, I mean, to use common language, though it's not really like that. It's much more abstract. But sort of a rough intuition would be that one attention head is more dramatically sensitive. Another is more focusing on some other aspect of the input.\n",
      "\n",
      "\n",
      " And so they are working all in parallel, all of these heads, multi-head, attention, they're all working in parallel.\n",
      "\n",
      "\n",
      " And then, of course, you throw in the usual feedforward networks and a few residuals here and there, right?\n",
      "\n",
      "\n",
      " They're like peanut, like I said, one good way of thinking about them, or I think about them, somewhat wrong, or a bit tongue-in-cheek, but it works for me, is I think of the feedforward layers always as the peanut butter and jelly that you put in sandwiches, right?\n",
      "\n",
      "\n",
      " Why do we need them? Just like you don't want to eat a sandwich without them in the morning, you need them because obviously they are the ones that insert non-linearity, right. So if you want to have ultimately the universal approximator function behavior, you need feedforward networks to introduce your non-linearities.\n",
      "\n",
      "\n",
      " And you do that to create then better representation learning in the classic sense. They will take the input that goes into the feedforward and basically try to create higher order representations out of it. So that the subsequently have benefits from the higher order approximation.\n",
      "\n",
      "\n",
      " So that was the attention, that was the architecture or the basic idea of the transformer, which was the original transformers are encoder decoder architecture. It has an encoder that produces a state that goes into the decoder. The decoder is autoregressive in some sense.\n",
      "\n",
      "\n",
      " It takes the hidden state, produces the first word, then that first word is fed back and along with the hidden state to produce the second and so on and so forth. And it goes on producing word after word after word, right? So remember, remember these transformers are if you use the full auto the um the transformer the full architecture of the transformer you know that suppose you're generating text out of it whether you're doing a translation or whatever it is one word emitted at a time.\n",
      "\n",
      "\n",
      " You see that, right?\n",
      "\n",
      "\n",
      " You sort of generate one word at a time and you move with that. Just pause and think what it means.\n",
      "\n",
      "\n",
      " We are all so mesmerized with the latest developments of the large language models today, okay?\n",
      "\n",
      "\n",
      " Or people are, as you know, Microsoft wrote a paper called Sparks of Artificial General Intelligence.\n",
      "\n",
      "\n",
      " They claim they are seeing in GPT-4, which they have been playing around with a few.\n",
      "\n",
      "\n",
      " It's a pretty large paper. I believe it weighs in at 154 pages.\n",
      "\n",
      "\n",
      " Most of them are examples of prompts and their responses.\n",
      "\n",
      "\n",
      " It's a very easy read, but really enlightening read.\n",
      "\n",
      "\n",
      " But somehow, it does make the researchers ask the question, are we seeing sparks of AGI?\n",
      "\n",
      "\n",
      " I don't think so, frankly.\n",
      "\n",
      "\n",
      " I feel that we are not seeing sparks of AGI but we are certainly in a land where we are seeing these large language models produce things surprisingly at the next level to the kind of task specific performance we were seeing so forth.\n",
      "\n",
      "\n",
      " If you look at the so-called call zero-shot learning that we do, there is always sort of a trick to it. For example, we noticed how with sentence embeddings, we could create, or with the classifiers, we could do, so zero-shot classifier, how does it work? You train a classifier to find some labels, A, B, C. But then how do you do, then you say classify into PQR. But what's the hidden trick we use?\n",
      "\n",
      "\n",
      " We know that A, B, C, and PQR we can embed into a latent space. And then instead of, if you predict A, we just find, okay, amongst PQR, which is it nearest to, right?\n",
      "\n",
      "\n",
      " The statement, because the statement text is also going to go into the same embedding space.\n",
      "\n",
      "\n",
      " You go into the embedding space and instead of looking for proximity to ABC, you look for proximity to PQR, right?\n",
      "\n",
      "\n",
      " And that's a trick you use. So behind all this zero-shot learning, there's always a like there's always a trick like for example for the clip paper uh did i cover the clip paper in this one\n",
      "\n",
      "\n",
      " no so we'll do that maybe today we'll do that so what you do maybe today i'll use literally for clip so what we what they did is they said this is zero shot learning we didn't need image and its labels to train to train a transformer to learn from images and its labels to train a transformer to learn from images. True, they didn't have to use an army of people to set and label images. But, you know, an army of people have already done that.\n",
      "\n",
      "\n",
      " Because every time you create a web page, as you know, as a web developer, good practices, you put an image and you put a caption there.\n",
      "\n",
      "\n",
      " Alt, A-L-T, I-M-G, S-R-C is blah, A-L-T is blah, right? You're putting a caption there, alt, text to it. And so what Clip did is it just harvested all of these images with captions. And so it had label data.\n",
      "\n",
      "\n",
      " So when you, and they call it pre-processing or pre-training before you do the thing.\n",
      "\n",
      "\n",
      " But in reality, you end up with label data.\n",
      "\n",
      "\n",
      " So there's always a little bit of a trick.\n",
      "\n",
      "\n",
      " Now with this very large language model, so you are beginning to see emergent abilities, like the ability to do arithmetics and so forth.\n",
      "\n",
      "\n",
      " We are not really training for that.\n",
      "\n",
      "\n",
      " So it is a little bit of an extension.\n",
      "\n",
      "\n",
      " But we have to ask this question.\n",
      "\n",
      "\n",
      " Let's see, as I said, a transformer produces one word at a time in this autoregressive framework. And the next word it produces is conditional on the words it has produced and the context it was given the input.\n",
      "\n",
      "\n",
      " It is duly conditioned on both, right?\n",
      "\n",
      "\n",
      " So it is one word at a time machine. A one word at a time machine has not thought out the entirety of what it is going to say. And yet intelligent creatures, like for example, would you ever talk seriously right or perhaps to your boss or to your spouse right by first thinking one word and then thinking okay\n",
      "\n",
      "\n",
      " what should i say next and then the the thing you would get is watch what you're going to say\n",
      "\n",
      "\n",
      " next\n",
      "\n",
      "\n",
      " right so uh that's not intelligence has this thing that there is a there's a conceptualization and out of the conceptualization there is a verbalization of the concept with at this moment at least there is no evidence that it is a truly conceptualizing what it is going to say in one shot and then translating it into words. You may argue that it is so, but at least there's no clear evidence that it is.\n",
      "\n",
      "\n",
      " So to me, it is still one word at a time machine, right?\n",
      "\n",
      "\n",
      " And that is why I'm skeptical that these things are AGIC or even have any, but certainly they can fool us that they are convinced, they're far more intelligent than the transformers we are used to. And they are, like when they say they are sparks of AGI, well, not quite wrong either in the sense that it is impressing us.\n",
      "\n",
      "\n",
      " It's making us feel.\n",
      "\n",
      "\n",
      " And, for example, in artificial intelligence, there was this concept of the, what is it called, the Chinese room experiment, right?\n",
      "\n",
      "\n",
      " The evidence of weak.\n",
      "\n",
      "\n",
      " So first people thought that a strong intelligence, AGI, I mean, general intelligence has two categories, AI. The belief in strong intelligence means you have to show thinking.\n",
      "\n",
      "\n",
      " We don't even know what human thinking is. How can we conceptualize machines to think? There was historically, by the way, a company in Silicon Valley literally called Thinking Machines, a very high flying machine with Minsky and the great AI gurus sitting there.\n",
      "\n",
      "\n",
      " Noble laureates came and worked there.\n",
      "\n",
      "\n",
      " That company is no more, it broke into two pieces.\n",
      "\n",
      "\n",
      " The hardware part became Sun, the software, the machine learning part went into Oracle, and then Oracle ended up eating up Sunny also. So now both of those groups are in Oracle, except that they're different.\n",
      "\n",
      "\n",
      " The machine learning group is sort of, it is embedded, limited, it's embedded into the kernel, into the database kernel.\n",
      "\n",
      "\n",
      " And Sun is still doing well, reasonably well.\n",
      "\n",
      "\n",
      " So the weaker form of intelligence that people put, AI, is that if you put a curtain and you ask questions, and if a human being is asking questions from one side of the curtain and cannot tell whether the other side of the curtain is a human or a machine, or is fooled into the belief that it's a human, or you say that the machine has achieved a weak form of artificial intelligence.\n",
      "\n",
      "\n",
      " So we are in a very interesting place at this moment.\n",
      "\n",
      "\n",
      " So anyway, this is a small digression, and I'll move past this quickly.\n",
      "\n",
      "\n",
      " We're in a very interesting place.\n",
      "\n",
      "\n",
      " I don't know if you know that every single day, I think Amazon mentioned that their, what is that called, Alexa. Alexa receives dozens or hundreds of proposals, marriage proposals, right?\n",
      "\n",
      "\n",
      " From lonely people who are absolutely sure that these wonderful, wise, empathetic, loving answers could only come from somebody with a beating heart, with a large heart.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " People want married, aren't they?\n",
      "\n",
      "\n",
      " Yeah, that's right.\n",
      "\n",
      "\n",
      " Even now, people have gone crazy with child abuse.\n",
      "\n",
      "\n",
      " They are committed suicide because of chat GPT before some response.\n",
      "\n",
      "\n",
      " VINOD MARUR, Oh, really?\n",
      "\n",
      "\n",
      " Praveen is mentioning something very interesting, for those of you who are remote, that similar behavior is happening with chat GPT to the extreme that people, they're engaging with chat GPT. And when they didn't get the same amount of a passionate response from chat GPT, a romantic response, they are committing suicide, right?\n",
      "\n",
      "\n",
      " So it speaks to how much these machines are fooling us into the belief that they are intelligent, right? So that whole question of weak intelligence, are we already getting them? The question is, who's asking the question? If you have a researcher ask the question, in two minutes, they come to the conclusion this thing is done, not even close. I remember when Chad GPT came out, I asked a question, which it turns out a lot of other people asked. I asked a question, and actually somebody here in our audience, Chanda asked this question, not me. He asked this question, why seven, not a prime number? And it immediately answered to Chandler that seven is not a prime number because of course, a prime number is divisible by other numbers, is not divisible by other numbers, except one itself. And seven is of course divisible by three and nine, right? Which is utter nonsense, right?\n",
      "\n",
      "\n",
      " So you see this one word at a time coming out, a sensible word.\n",
      "\n",
      "\n",
      " If you don't know arithmetic, if you don't know what frames are, it looks very logical, isn't it?\n",
      "\n",
      "\n",
      " But it was wrong. And now you ask the same question and it gets it right.\n",
      "\n",
      "\n",
      " So obviously there's a human in the loop somehow improving its training.\n",
      "\n",
      "\n",
      " No, more often they say it's memorizing. It sort of memorizes. That's the other.\n",
      "\n",
      "\n",
      " Yeah, because they ask people this one, the goat, grass, and that.\n",
      "\n",
      "\n",
      " Initially, first they did not give the right approach.\n",
      "\n",
      "\n",
      " They fixed it. They fixed it, yes.\n",
      "\n",
      "\n",
      " So, yeah, Prabin gave another beautiful example.\n",
      "\n",
      "\n",
      " Remember the story that a man has a goat, a lion, and some food, right?\n",
      "\n",
      "\n",
      " Now, it has to protect all three and cross a river.\n",
      "\n",
      "\n",
      " The goat will eat the grass, and the lion will eat the goat, right?\n",
      "\n",
      "\n",
      " So how do you cross it without either of the two mishaps happening? And there's a way to go sequence to go back and forth.\n",
      "\n",
      "\n",
      " When you give it a chat GPT, in the beginning, it got it wrong.\n",
      "\n",
      "\n",
      " It's a logical question.\n",
      "\n",
      "\n",
      " But after some time, it got it wrong. It's a logical question, but with, after some time it started giving the right answer.\n",
      "\n",
      "\n",
      " So two, two ways of looking at it. One is that somebody notices the wrong answer. There's a human in the loop.\n",
      "\n",
      "\n",
      " Now they're fed in the right answer as part of the training data, because it's going through continuous training and it brought, brings up actually a very deep question. Are these machines memorizing the answers?\n",
      "\n",
      "\n",
      " Because they have so many parameters, they have a trillion parameters.\n",
      "\n",
      "\n",
      " Can they effectively memorize? Now it turns out one can mathematically show that they don't memorize. At least they don't brute force memorize, but something else happens.\n",
      "\n",
      "\n",
      " See, if you train them on a certain kind of data, right they are making the condition on that the weights have shifted the parameters have learned from that what will happen if you ask a very similar question or that question it is predicting the next word at a time and the correct behavior should of it should be that given the weights, it should produce a response more or less like what it has been trained on, right?\n",
      "\n",
      "\n",
      " Because that would be the path of least error that would give you the least value of the last function, isn't it? And so that brings up the whole question that given the way the weights are, effectively, for unique questions, if it produces exactly the answer that is correct, right?\n",
      "\n",
      "\n",
      " Almost it looks like regurgitation. Has it regurgitated? Has it memorized? How can it regurgitate unless it has memorized?\n",
      "\n",
      "\n",
      " This again came out, I think one of you here were pointing out that somebody put in the source code of Samsung or something, a snippet of the source code and then it emitted out the rest of the source code, isn't it?\n",
      "\n",
      "\n",
      " I clicked it like someone else should have been able to see.\n",
      "\n",
      "\n",
      " I don't understand.\n",
      "\n",
      "\n",
      " What was the input?\n",
      "\n",
      "\n",
      " They basically like inputted the source code.\n",
      "\n",
      "\n",
      " And what I heard was um so instead of telling them that it leads somewhere else oh so in response to another person it regurgitated the source code\n",
      "\n",
      "\n",
      " yeah\n",
      "\n",
      "\n",
      " this is it right because source code is so highly structured that when you give long constructions of source code it has no choice but to adapt its ways learn learn from it. But then when you give it any prompt that looks similar to that, it has no choice but to essentially try its best to reproduce it from its ways. Even though it hasn't memorized it, it has in some sense optimized itself in such a way that it will emit it out.\n",
      "\n",
      "\n",
      " Right? Yeah.\n",
      "\n",
      "\n",
      " So that's why they're binding in some places. Like Walmart, they said they're binding in certain places.\n",
      "\n",
      "\n",
      " Yes. ChatGPT. Yes.\n",
      "\n",
      "\n",
      " So companies, Praveen is mentioning, companies are banning it in the use of ChatGPT in corporate environments.\n",
      "\n",
      "\n",
      " The last thing they want is people to feed it more and more of their secrets.\n",
      "\n",
      "\n",
      " So, Albert, go ahead.\n",
      "\n",
      "\n",
      " ahead so you know going back to the assignment so you have this short sentence\n",
      "\n",
      "\n",
      " so that's basically the end of the presentation so is that similar to the attention also that we talked about the same we talked about attention the tools of attention or?\n",
      "\n",
      "\n",
      " Yes.\n",
      "\n",
      "\n",
      " Yeah, Albert's question is that we gave short sentences to these transformers and then it created these embeddings.\n",
      "\n",
      "\n",
      " How is that related to attention? The way it is related to attention is, attention was the mechanism used to see the relationship between the words, the semantic relationship.\n",
      "\n",
      "\n",
      " And because you're embedding it in a space where two sentences that are semantically similar should be nearby, right? So what does it mean that their attention heads should emit out a vector that are close to each other? And so attention is the mechanism that is used to get semantic understanding and project it into an embedding space close to each other.\n",
      "\n",
      "\n",
      " Right? All right, guys.\n",
      "\n",
      "\n",
      " So with all of that, we are in a place that transformers are on a roll at this particular moment, right?\n",
      "\n",
      "\n",
      " And for better or for worse, we are doing that.\n",
      "\n",
      "\n",
      " Now, what we did is I'll go through the very quickly, the theory that we went through we did the initial original transformer then we said that when you see transformers in the wild you notice an interesting pattern some transformer architectures they use only the encoder part of the classic the attention is all you need transformer the full encoder decoder architecture.\n",
      "\n",
      "\n",
      " BERT family in particular utilizes the encoder aspect of it, is predominantly encoder based. The GPT family is predominantly the decoder part of it.\n",
      "\n",
      "\n",
      " Are we together?\n",
      "\n",
      "\n",
      " And that is why they are one word emitting at a time. Then what's the trade-off between these two? Let's think of a trade-off between using and .. And then, of course, there are architectures, transformers, that use both. For example, BARD uses both. Not Burt, BART, B-A-D, uses, is it Bart? Bart, yeah, uses Bert and so on and so forth.\n",
      "\n",
      "\n",
      " So there are many architectures, there's a mix.\n",
      "\n",
      "\n",
      " What's the trade-off between these two?\n",
      "\n",
      "\n",
      " I don't think I quite spoke on that, so I'll speak on that.\n",
      "\n",
      "\n",
      " See, when you use Bert, if you remember, how did we train Bert? When we reviewed the BERT paper, we noticed that the germ, the main crucial idea was, you take the encoder password, the transformer, and then, and of course you make it bigger, you have six heads, 12 multi heads and whatever, multi attention heads and all of that.\n",
      "\n",
      "\n",
      " But at the end of it, the embedding was that, the word would get embodied, tokenized and embedded. It would go through the embedding and even the embeddings were learned. You force the transformer to learn the embeddings.\n",
      "\n",
      "\n",
      " Then there was a segment embedding saying which of the two, you give two segments at a time, two sentences at a time, which of the two sentences a word belongs to.\n",
      "\n",
      "\n",
      " And the third was position embedding. What is the position of this word in the sentence?\n",
      "\n",
      "\n",
      " So position embedding comes from the classic transformer idea.\n",
      "\n",
      "\n",
      " Now you concatenate all of these three to create the full embedding of a token. The actual token embedding plus segment embedding plus position embedding is the complete embedding for that token. Now you take a sentence with all its tokens, you put a special token CLS at the beginning and a special token SEP between the two sentences, right, and then you shoot it through the bud, right, through the attention heads and some glue, some peanut butter and jellies there, feed forward networks there, and then you get the output state.\n",
      "\n",
      "\n",
      " Now the whole question is how do you train it? You need some task to train it.\n",
      "\n",
      "\n",
      " So the task that they picked was a masked language model. What it means is, that's a really mouthful of a word for something quite simple.\n",
      "\n",
      "\n",
      " You would do what children are taught to do in schools, fill in the blank, right?\n",
      "\n",
      "\n",
      " What do we do?\n",
      "\n",
      "\n",
      " We know that the capital of California is blank. And now you have to guess what the capital of California is.\n",
      "\n",
      "\n",
      " We have all been through these exercises. And so it's like teaching that. It's a very effective educational exercises. And so it's like teaching that. It's a very effective educational methodology.\n",
      "\n",
      "\n",
      " And that's exactly what they did.\n",
      "\n",
      "\n",
      " They would take a vast corpus of documents and randomly suppress about 10, 15% of the tokens. They would put a mask. And then they would say, ask the transformer, what was that? What was this?\n",
      "\n",
      "\n",
      " Guess which word was actually there and for it to guess what word it is there it's a classic thing that you can only do with attention because it needs context remember the when we did the theory of attention attention is based on what attention is there is a context that forces you to put differential weights on differential tokens of the input. That is the gist of the attention.\n",
      "\n",
      "\n",
      " To recap again, when you are walking on a hike, you're enjoying the mountains, you're enjoying the stream going by in the trail, and you're talking to your friend.\n",
      "\n",
      "\n",
      " And all of a sudden you spy you hear a sound and all of a sudden the sky has disappeared and the stream has disappeared right and the only thing your attention is for where is that where is that predator isn't it so you know your whole visual field is narrowed to something else a search for something else. So the context determines your attention, where you put emphasis, and that is attention. That's the main idea of attention. Now, when you put a mass word guessing, it's a classic use of attention. You have to pay attention to the rest of the words to a differential extent. You have to understand the semantics of the words in some sense to be able to tell what word was masked.\n",
      "\n",
      "\n",
      " So when you train it like that, what can you tell?\n",
      "\n",
      "\n",
      " This task is not something that you would actually use in real life.\n",
      "\n",
      "\n",
      " Maybe you would use, sometimes you do OCR in some words, and you may try to guess what word it is.\n",
      "\n",
      "\n",
      " But in defect, you do such real things as, is this document legal document or sports document?\n",
      "\n",
      "\n",
      " Right, or something like that.\n",
      "\n",
      "\n",
      " Use it for classification, use it for summarization, use it for all sorts of things. It turns out that when you train bird for this it is able to do very good classification for classification it's good because to classify text what do you need you need to pay attention to all the tokens that are fed in right or for entailment like what does one follow the other? You need to pay attention to both the sentences and move forward with that.\n",
      "\n",
      "\n",
      " So for that, the encoder architecture works pretty good.\n",
      "\n",
      "\n",
      " Whereas if you're doing generative tasks, like generating a passage of text, right?\n",
      "\n",
      "\n",
      " You realize that the autoregressive is better because you use the decoder part of it and it keeps on emitting word after word after word and keeps generating for you so that is one way of separating the two parts out see when you are trying to generate one word after the other chat you probably want to heavily emphasize the decoder because it will keep emitting word after word after word.\n",
      "\n",
      "\n",
      " It hasn't been, BERT hasn't really been specifically trained for such tasks.\n",
      "\n",
      "\n",
      " Isn't it? So that's the difference.\n",
      "\n",
      "\n",
      " Which is this classification in the encoder part that everything is open and afraid?\n",
      "\n",
      "\n",
      " That's right.\n",
      "\n",
      "\n",
      " You said that EPP focuses on that.\n",
      "\n",
      "\n",
      " And what other model?\n",
      "\n",
      "\n",
      " But it focuses on the encoder part.\n",
      "\n",
      "\n",
      " See, what happens is we talked about.\n",
      "\n",
      "\n",
      " And again, this is a recap.\n",
      "\n",
      "\n",
      " I mean, you can use GPD also for decoding.\n",
      "\n",
      "\n",
      " But I'm saying a bird, for example, is very good at finding mass width because it literally is what it's trained on, on whether these two sentences are similar. Similarity analysis, perfect for the encoder part of the architecture, because literally that's what you trained it on.\n",
      "\n",
      "\n",
      " Because what is the CLS token?\n",
      "\n",
      "\n",
      " What do you do?\n",
      "\n",
      "\n",
      " You feed it into a classifier.\n",
      "\n",
      "\n",
      " You can ask it a question, give it the probability that these two are similar or tell what is a mass word.\n",
      "\n",
      "\n",
      " It's literally very close to what it was trained on, sentiment analysis, isn't it? Whereas the decoder part, what is it well trained on?\n",
      "\n",
      "\n",
      " Translating from English to French, generating the text.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " That is where decoders are good. So that's a very rough way of doing it.\n",
      "\n",
      "\n",
      " Of course, you can sort of hack one into the other and use it, but that's a simple dichotomy. Both use transformer, but they focus on only the encoder part because once they use transformer they transform both encoder and decoder. No actually if you look at the BERT architecture, decoder is practically not there. It's only the encoder. But they use all transformers. No they don't use the whole encoder-decoder at all.\n",
      "\n",
      "\n",
      " Look at the bird. You'll see mostly is the encoder sitting there.\n",
      "\n",
      "\n",
      " See what do you do?\n",
      "\n",
      "\n",
      " You take this word segment and all of that pair, shoot it through the attention heads, throw in your feed forwards, maybe more attention heads feed forward, but ultimately sandwiches of those, right?\n",
      "\n",
      "\n",
      " And then at the top of it, what do you do?\n",
      "\n",
      "\n",
      " You get a hidden representation, the CLS and you feed the CLS straight into the classifier.\n",
      "\n",
      "\n",
      " That's it.\n",
      "\n",
      "\n",
      " They don't use the Vanilla Transformer but there are certain transformers that use the whole of the transformer. But the most common ones that you hear about, they're actually using one or the other\n",
      "\n",
      "\n",
      " right so you can ask a quick question hey for the gpt don't they need some kind of encoding before they get to decoding part\n",
      "\n",
      "\n",
      " or oh\n",
      "\n",
      "\n",
      " yes yes the tokenizing and embedding of course you do and that is learned see remember that even in the decoder right decoder is nothing but the whole encoder, right?\n",
      "\n",
      "\n",
      " The input is not coming from generally, like in the classic architecture, input is not coming from this part.\n",
      "\n",
      "\n",
      " User, it is actually coming from the hidden state and just a trigger, you know, just trigger. The trigger will produce the first word. You feed the first word back into the as input along with the hidden state and then it produces a second word. Now you take the first and second word feed it back into it, right?\n",
      "\n",
      "\n",
      " And like you keep moving forward like that.\n",
      "\n",
      "\n",
      " Now when you only use the decoder what you do is you actually it is nothing but the encoder but with the loop like whatever output it produces, you feed back in.\n",
      "\n",
      "\n",
      " Yeah, that makes sense.\n",
      "\n",
      "\n",
      " Because when I was reading the attention is all you need paper, I did not understand.\n",
      "\n",
      "\n",
      " Thank you for clarifying that because they've added on the output embedding, they've added a mass multi head retention.\n",
      "\n",
      "\n",
      " Yeah.\n",
      "\n",
      "\n",
      " I was thinking, why did they add a mast on this side?\n",
      "\n",
      "\n",
      " Right. The multi head.\n",
      "\n",
      "\n",
      " So, yeah. Yeah.\n",
      "\n",
      "\n",
      " Is that why they have added it that is right the mass multi-head is simply to mask the words that it shouldn't pay attention to you know the future the words that are that are not yet you know the the like what it has translated is just one word so far right so that is it so\n",
      "\n",
      "\n",
      " So that is your classic architectures.\n",
      "\n",
      "\n",
      " Now we went.\n",
      "\n",
      "\n",
      " Then why do we say that it's not encoder and decoder and only decoder heavy?\n",
      "\n",
      "\n",
      " GPT.\n",
      "\n",
      "\n",
      " Why don't we do this?\n",
      "\n",
      "\n",
      " Let me do the GPT. I've done BERT. Were you there in the BERT session?\n",
      "\n",
      "\n",
      " When we did. I was there. I was there. So BERT you saw it.\n",
      "\n",
      "\n",
      " It's encoder heavy.\n",
      "\n",
      "\n",
      " Yes, that I understood.\n",
      "\n",
      "\n",
      " Why don't I do this?\n",
      "\n",
      "\n",
      " I just realized that I have not done the GPT architecture in detail.\n",
      "\n",
      "\n",
      " Why don't I do that?\n",
      "\n",
      "\n",
      " Because that will clarify all the doubts.\n",
      "\n",
      "\n",
      " Oh, thank you so much.\n",
      "\n",
      "\n",
      " Let me do it.\n",
      "\n",
      "\n",
      " And if you're planning to do that later in some other session, that's also okay.\n",
      "\n",
      "\n",
      " I don't want to detail this session.\n",
      "\n",
      "\n",
      " It's okay. We will do that. Either today or tomorrow or either tomorrow, today or the next time, we'll definitely do that.\n",
      "\n",
      "\n",
      " Is generator and destructor the same as importer and decoder? Generator and? Destructors. Destructors. I've not heard of destructors.\n",
      "\n",
      "\n",
      " Like in GAN. Oh, GAN. GAN are generators and discriminate. Discriminate. Yeah.\n",
      "\n",
      "\n",
      " So what happens is that, okay, so question here is, what's the relationship between the generator and the discriminant or discriminator of GAN to encoder-decoder?\n",
      "\n",
      "\n",
      " Actually, you don't think of it like that.\n",
      "\n",
      "\n",
      " You don't try to map one to the other.\n",
      "\n",
      "\n",
      " I'll explain to you what it means.\n",
      "\n",
      "\n",
      " See, in a GAN, this is a quick digression.\n",
      "\n",
      "\n",
      " A discriminator is just a classifier.\n",
      "\n",
      "\n",
      " Let's say that the basic quintessential idea is, suppose the generator is basically a counterfeiter. Who has to counterfeit the currency of a country whose currency it has never seen let us say that you all of a sudden decide that you want to count it's a really good idea to counterfeit bhutani's currency\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " but you have never seen the bhutani's currency\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " So what happens is that you have a discriminator which is watching out, is the police in some sense.\n",
      "\n",
      "\n",
      " And when you give it in the beginning, you start training both of them.\n",
      "\n",
      "\n",
      " Now, let's say that even the cop is rather naive.\n",
      "\n",
      "\n",
      " So you first give it a genuine Bhutanese currency and it makes some random probability guess. It says it's a genuine or fake.\n",
      "\n",
      "\n",
      " That produces a loss error function. There's an error there.\n",
      "\n",
      "\n",
      " And you back propagate the loss. And so what will happen?\n",
      "\n",
      "\n",
      " The discriminant weights will shift and learn something from it.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " What about the generator loss?\n",
      "\n",
      "\n",
      " It also figures out that something is up.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " It also shakes a little bit.\n",
      "\n",
      "\n",
      " But then the generator asks to produce a currency. It has no idea what Bhutanese currency looks like.\n",
      "\n",
      "\n",
      " It produces some random noise.\n",
      "\n",
      "\n",
      " It will make a picture like it will just give that and say this is it.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So, I mean, to make it a little bit more colorful, not that it does that, it produces random noise in the beginning, but let's say that it puts a peacock on a paper and says, this is Bhutanese currency, right? Then even that naive cop knows that this cannot be Bhutanese, so it gives it a probability and it shoots it down.\n",
      "\n",
      "\n",
      " Now you back prop the error, right? Now what happens?\n",
      "\n",
      "\n",
      " Suppose it said it's fake.\n",
      "\n",
      "\n",
      " The discriminator has now gotten a very small loss.\n",
      "\n",
      "\n",
      " It's happy.\n",
      "\n",
      "\n",
      " There isn't that much of things to propagate.\n",
      "\n",
      "\n",
      " It's sort of, it's feeling happy with the small losses.\n",
      "\n",
      "\n",
      " Right, it says my weights are doing well for this fake thing. I caught the fake, right?\n",
      "\n",
      "\n",
      " And it's patting itself on the back.\n",
      "\n",
      "\n",
      " But what is the discriminator telling?\n",
      "\n",
      "\n",
      " I got it wrong.\n",
      "\n",
      "\n",
      " I mean, what the generator thinking?\n",
      "\n",
      "\n",
      " Oops, I got caught.\n",
      "\n",
      "\n",
      " So it will change its ways. So next time it won't produce a peacock. It will produce maybe a horse or something like that. I mean, it will slightly change. It will generate a different...\n",
      "\n",
      "\n",
      " But the amazing thing about gradient descent and backdrop is very quickly the generator without ever seeing Bhutanese currency will learn what a Bhutanese currency looks like.\n",
      "\n",
      "\n",
      " And it will start producing it and giving it to the discriminator. And the discriminator will also gradually smarten up because it's not so naive cop anymore.\n",
      "\n",
      "\n",
      " It has learned to tell the difference between the genuine one and the fake one.\n",
      "\n",
      "\n",
      " Why?\n",
      "\n",
      "\n",
      " Because it's also getting genuine instances and the times that it gets right, it's told it's right. The times it gets wrong, it's told it's wrong.\n",
      "\n",
      "\n",
      " Right? And so forth.\n",
      "\n",
      "\n",
      " So it, and likewise for the fake ones, you randomly give genuine fake, genuine fake, and you train the discriminator to tell the difference apart.\n",
      "\n",
      "\n",
      " And so it's a cat and mouse game.\n",
      "\n",
      "\n",
      " The generator learns to produce more and more realistic fakes and the discriminator as a classifier gets better and better at telling them apart.\n",
      "\n",
      "\n",
      " So they have their own encoder and decoder feedback loop within them? Yeah, they have a feedback loop, but you don't think of it as encoder-decoder because generator is not producing a hidden representation. When you encode, you expect an encoded vector coming out of it. In a way, the generator does, but then the real data, it is trying to imitate the real data rather than some encoded thing. So it's literally trying to imitate which is the purpose of GANs right GANs can produce infinitely many human faces that don't exist why because you have trained it to yeah you have trained the GAN to recognize human faces\n",
      "\n",
      "\n",
      " so if you give it a horse it will say no right\n",
      "\n",
      "\n",
      " so what happens gradually the generator begins to get smarter and it produces more and more human-like faces. But once the GAN has been trained, what's the whole point of a generator? A generator, it becomes a generative model that can produce infinitely many faces after that.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " It can go on producing faces.\n",
      "\n",
      "\n",
      " And that's an amazing thing for something that started with just knowing nothing at all right and now when we talk about segment the meta paper the segment anything is that also based on that the which paper the segment the segment anything paper that metaa just released. Oh, no, I have to read that paper. I'm sorry, I haven't yet. I'll read that.\n",
      "\n",
      "\n",
      " But thank you for pointing.\n",
      "\n",
      "\n",
      " Actually, let's post it to our Slack network. I'll definitely read it to my team.\n",
      "\n",
      "\n",
      " Okay.\n",
      "\n",
      "\n",
      " So that's sort of a summary. Now let's go to semantic search. What do we do in semantic search? How do we train to produce meaningful embeddings?\n",
      "\n",
      "\n",
      " You deliberately take paired triplets, right? You take a corpus of triplets. You take three sentences, right? Two sentences are genuinely together. They're similar. Right?\n",
      "\n",
      "\n",
      " And the third sentence is deliberately some random sentence from the corpus.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " And so you say that the first sentence is a reference sentence. The second is similar. And the third is dissimilar. Known to be dissimilar.\n",
      "\n",
      "\n",
      " Then what you do is you pass it through a transformer to create the hidden embedding, the embedding. And then what do you do?\n",
      "\n",
      "\n",
      " You look at the cosine distance between the reference and the similar statement. And you look at the cosine distance between the reference and the dissimilar state.\n",
      "\n",
      "\n",
      " And then you put a few classifier layers. And what do you want your classifier layers in the softmax to do? You want the classifier layers to minimize, I mean, to maximize the cosine similarity between the reference and the similar statement and minimize the cosine similarity between the reference and the dissimilar statement.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " Minimize the similarity. Cosine is a measure of similarity. So maximize the distance, minimize the similarity. And so you can therefore easily construct the loss function associated with it.\n",
      "\n",
      "\n",
      " And that is the basic idea. You see how simple that idea is. And yet that simple idea helps you create embeddings for sentences in such a way that basically you have, in one moment, it creates a revolution in AI search. In fact, the first big step forward since the days of keyword search, right?\n",
      "\n",
      "\n",
      " And that's the beauty of transformers.\n",
      "\n",
      "\n",
      " Like once you get the transformer idea right sometimes big ideas big changes can be done with what in hindsight looks like simple you know ways of looking at it now to complete that discussion of similarity uh so like semantic similarity we we took this idea that see isn't it ideal if we trained a bird to take two sentences two segments and pass it simultaneously together to have cross attention between them right it's a cross encoder effectively and let's say that you put a classifier the probability that you produce the larger potentials that you produce, should be proportional to how similar these two statements are.\n",
      "\n",
      "\n",
      " Quite literally, you can train it on that ideally.\n",
      "\n",
      "\n",
      " So why don't we use that for similarity analysis?\n",
      "\n",
      "\n",
      " So suppose I have a corpus of documents, of sentences. I get a query sentence.\n",
      "\n",
      "\n",
      " Why can't I look for similarity against all of this corpus of let's say, a billion documents and find out which ones are the most similar, which ones produce the most.\n",
      "\n",
      "\n",
      " The problem is inference from a transformer is expensive. And for every query, if you have to do a billion comparisons through a transformer, you'll go bankrupt and the cloud providers will be smiling, right?\n",
      "\n",
      "\n",
      " So you don't want to do that.\n",
      "\n",
      "\n",
      " So the idea was the intermediate stage, and that is the whole point that the sentence bird does, it says that, no, what we do is by training the sentence bird or like two sentence by using this triplet siamese network we just create embeddings right embeddings that are semantically closer if sentences are closer and what we store are embeddings so now when i get a query i convert the query into an embedding and i'm not searching into a cartesian space i don't need a transformer for that i just need to do cosine distance comparisons, which are way, way cheaper than transformer inferences.\n",
      "\n",
      "\n",
      " So that is good. Then we go one step further.\n",
      "\n",
      "\n",
      " We know that when you do embedding, something is lost.\n",
      "\n",
      "\n",
      " I mean, it is never as precise or accurate as doing direct inference through a transformer like that.\n",
      "\n",
      "\n",
      " You lost something.\n",
      "\n",
      "\n",
      " But you lose a little bit more because even to do vector comparisons against or cosine comparisons against a billion documents is not the wisest idea. So then we brought in the idea of approximate nearest neighbors, which somehow bucketizes all the points or it partitions the embedding semantic space into sort of partitioned areas so that when you get a query, you go into that bucket and you just look for or you search for neighbors only in that bucket. The trouble with that, and we use all of these ideas, quantization of space and many, many ideas, we use resolution colors and quantizations and so forth so there are many ways of doing it and i believe i covered about five ways of doing a and n's what the engines bring you is speed you don't have to do this massive a billion comparisons you do a far fewer number of comparisons and you get the neighbors.\n",
      "\n",
      "\n",
      " But what do you lose?\n",
      "\n",
      "\n",
      " If you look at search performance in terms of performance and recall, I mean, sorry, precision and recall, these two measures, precision being every single search result is really the relevant one. And recall being every relevant search results that you know exists in the database, in the corpus, did show up in your result.\n",
      "\n",
      "\n",
      " So that's a measure, that would be a measure of recall.\n",
      "\n",
      "\n",
      " How many of the most relevant ones, genuinely relevant ones, you lose both in ANN.\n",
      "\n",
      "\n",
      " Why?\n",
      "\n",
      "\n",
      " Because you're searching only a neighborhood of things. And so you miss out some things that are just beyond the boundary. And that may be very highly relevant, especially if your query happens to fall literally next to a boundary. Let's say that you happen to have a query that lands you in San Diego and you start looking for neighbors, but you can't cross over to Mexico. So all your neighbors will be Californians, including the guy living in Northern California, Eureka, right? Whereas you missed out your neighbors just down the border.\n",
      "\n",
      "\n",
      " What's that city called? Tijuana. Tijuana. You missed your neighbors in Tijuana, who, by the way, are coming back and forth and helping you out with your, I don't know, backyard upgrade or your home improvements or whatever.\n",
      "\n",
      "\n",
      " It happens.\n",
      "\n",
      "\n",
      " Or you cross over there all the time to get, I don't know, medicines, and U.S. medicines are so expensive.\n",
      "\n",
      "\n",
      " So whatever it is, you miss the neighbors. So that's the problem with ANN search. So there are two ways to address this issue.\n",
      "\n",
      "\n",
      " One is, and I use this metaphor of sand dollars and sand. If you're looking for sand dollars and sand, one way you can do is you can scoop a larger bucket of them. You want 10, just scoop enough sand that you expect at least 10 of those sand dollars to be there, but you also end up picking up a lot of sand.\n",
      "\n",
      "\n",
      " Now you have a problem.\n",
      "\n",
      "\n",
      " Two things, because you got scooped up more, does it mean, is it computationally more expensive? A little bit, but because ANNs are so cheap, you can still go and scoop more.\n",
      "\n",
      "\n",
      " Now you scooped 50 results to get 10 really good ones, 10 sand dollars problem is how do you sift through the sand now easy because now you can go back to the gold standard what was the best absolute best way of doing similarity analysis not by embedding but by going back to the original transformer and doing cross encoding, right?\n",
      "\n",
      "\n",
      " Two sentences at a time.\n",
      "\n",
      "\n",
      " So take a key query and every of the 50 results, pass it through, look at the probability, similarity measure, and then rank it. You re-rank the search results based on that.\n",
      "\n",
      "\n",
      " And then you pick the top 10, right? That's why you use a cross encoder for re-ranking.\n",
      "\n",
      "\n",
      " And when you do that, think about it.\n",
      "\n",
      "\n",
      " You got now, hopefully, the best of both worlds.\n",
      "\n",
      "\n",
      " You got speed.\n",
      "\n",
      "\n",
      " At the same time, you got your recall, hopefully, if you brought a big enough bucket, all the sand dollars are there, right?\n",
      "\n",
      "\n",
      " And a precision.\n",
      "\n",
      "\n",
      " Hopefully, when you re-rank, only things that are genuinely relevant were in the top 10.\n",
      "\n",
      "\n",
      " So both your precision and recall gets sort of sufficiently restored.\n",
      "\n",
      "\n",
      " And that is the whole architecture of putting the search engine, semantic search engine together. Now, one reason I started this workshop with the semantic search is observe a few points that now that you're familiar with it, that we should see.\n",
      "\n",
      "\n",
      " We had to put many ducks in a row to make it work, isn't it? Yet each of the ducks are things that in hindsight look simple.\n",
      "\n",
      "\n",
      " Now, when I present it to you the way I presented it to you, I hope it almost looks intuitive that yes, that makes sense makes sense would you agree guys\n",
      "\n",
      "\n",
      " yeah you would also observe the fact that it was multiple users of transformers we use a siamese network of transformers to create embeddings those embeddings we put it into a FAS or ANN engine, scan, FAS, whatever, ANN engine, ANN database, ANN index, retrieved it. But then once again, we use transformers to do cross-encoding.\n",
      "\n",
      "\n",
      " Now, this also opens up the path, an interesting path, by the way, which I didn't cover.\n",
      "\n",
      "\n",
      " See, suppose the word is, you just get one word embeddings, like, for example, you get the word cat.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " Are you talking about computer-aided tomography? Are you talking about cat, the cute animal?\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So there's not enough context.\n",
      "\n",
      "\n",
      " So sometimes you can have a situation where AI search may underperform just keyword search.\n",
      "\n",
      "\n",
      " Great.\n",
      "\n",
      "\n",
      " And so one of the things people sometimes do is they use sort of a sparse vector presentation, or they just use ordinary keyword search results.\n",
      "\n",
      "\n",
      " They pipeline it.\n",
      "\n",
      "\n",
      " Remember I said you scoop a bucket of sand, but you also scoop a bucket of sand from elsewhere, from let's say your but you also scoop a bucket of sand from elsewhere, from let's say your keyword search engine, right? Because you know that ultimately you have the gold standard, the cross encoder sitting there as a gatekeeper, and it will re-rank it, and it will make sure nonsense doesn't go through, right?\n",
      "\n",
      "\n",
      " So that is also sometimes done.\n",
      "\n",
      "\n",
      " Yeah, Albert, your question now.\n",
      "\n",
      "\n",
      " Can we, every piece of the puzzle, not every piece, I take that back.\n",
      "\n",
      "\n",
      " Quite often the crucial pieces are independent ideas using transformers, right? Like in this semantic search, the embedding is through transformers, the cross encoding is a transformer, right? So that transformers play roles in two places, the Siamese network as well as the cross-encoder.\n",
      "\n",
      "\n",
      " That is it.\n",
      "\n",
      "\n",
      " The last thing I would say is, guys, people often tend to ignore the last step, the cross-encoding and the use of ANN. They often, a lot of people, a lot of vendors, they're giving you so-called semantic search engines or AI search engines, and all they're doing is vector search.\n",
      "\n",
      "\n",
      " That is not the whole way. Why?\n",
      "\n",
      "\n",
      " Because you will have problems with precision and recall.\n",
      "\n",
      "\n",
      " Not to mention that if you're doing direct vector search, you also throw performance out of the window.\n",
      "\n",
      "\n",
      " It will be slow.\n",
      "\n",
      "\n",
      " All right, guys.\n",
      "\n",
      "\n",
      " So I'll stop with that. And let's take how much of a break? 15 minutes, 20 minutes break? 10 minutes. Okay, let's take a 10 minutes break. I'll have my coffee and we'll start after that.\n",
      "\n",
      "\n",
      " So guys, we did do a review of the theoretical landscape we covered. We said transformers, transformers all the way, that's our mantra. You can use it to solve lots of problems by creating a chain of transformers and other things that you know.\n",
      "\n",
      "\n",
      " We took as a classic example, we took the search. Now, in a subsequent lab, we are going to do more interesting examples, but we took a break from search. After doing search, I wanted us to become skilled with the tools we are using. In particular, the tool set that we want to use for transformers, and that has become a defective standard, is the Hugging Face library. Now the Hugging Face library, Now, the Hugging Face library, interestingly, is written in Rust.\n",
      "\n",
      "\n",
      " Makes it pretty fast.\n",
      "\n",
      "\n",
      " As you know, Rust is a language that tries to have all the speed of C, but none of its security or memory leak issues, security vulnerabilities.\n",
      "\n",
      "\n",
      " So it's a pretty robust library.\n",
      "\n",
      "\n",
      " And what it does, amongst other things, is it gives you a uniform, easy interface to transformers, irrespective of whether they are written in PyTorch or TensorFlow.\n",
      "\n",
      "\n",
      " The two dominant deep learning libraries.\n",
      "\n",
      "\n",
      " A big thing from that.\n",
      "\n",
      "\n",
      " Now, people use different, they sort of bounce back and forth within TensorFlow and PyTorch. In my case, if you guys folks remember, this same workshop, long, long ago, I used to teach with TensorFlowflow right but somewhere along the line i switched over to pythorch simply because personally i find it to be more pythonic more more research friendly if you want to do some experimentation and things it is just nicer tensorflow is practically dead\n",
      "\n",
      "\n",
      " yeah yeah yeah\n",
      "\n",
      "\n",
      " so So TensorFlow has had a peculiar, you know, anyway, this is on records, I should say.\n",
      "\n",
      "\n",
      " It has had an interesting history.\n",
      "\n",
      "\n",
      " So apparently inside Google, so I'm told, they created a neural architecture.\n",
      "\n",
      "\n",
      " And the code base was rather, let's say, left a lot of scope for improvement. So they said, we are going to redo it. And this time, we're doing it in open source.\n",
      "\n",
      "\n",
      " So they came out with TensorFlow 1.0, except that perhaps the same people who created the previous thing created this.\n",
      "\n",
      "\n",
      " It was very low level.\n",
      "\n",
      "\n",
      " So it is like you want to ride a bicycle.\n",
      "\n",
      "\n",
      " Here is the chain. Here are the wheels. Here is the seat. Here is a post go build it right and then right there right like ikea like ikea\n",
      "\n",
      "\n",
      " yeah\n",
      "\n",
      "\n",
      " exactly like it became very popular and simply immediately took over the world like it was considered a necessary evil in many ways because you it was really a very powerful very framework, but you had to do a lot of things by hand.\n",
      "\n",
      "\n",
      " So that created the open source community to step forward and create the Keras, which is basically a layer of sanity over TensorFlow 1.0, right? And it's a high-level abstraction that did the simple thing simply, right?\n",
      "\n",
      "\n",
      " Then TensorFlow team adopted Keras as a de facto interface for 2.0. And TensorFlow was sort of two-phase, they would first make the execution graph and then you would run the execution graph.\n",
      "\n",
      "\n",
      " It made debugging harder, like you can't put breakpoints and so forth. It wasn't either execution, PyTorch is either execution. So then obviously a lot of us moved to PyTorch and then TensorFlow also introduced eager execution, tried to catch up. I don't know maybe a lot of people say that it's more production ready but I have never looked back from PyTorch.\n",
      "\n",
      "\n",
      " So what really happens is you take PyTorch. I mean, I'll tell you what I do. I take PyTorch models.\n",
      "\n",
      "\n",
      " I then forward generate into Onyx and TF, this thing, sir, what is it called?\n",
      "\n",
      "\n",
      " The RT, RT serve.\n",
      "\n",
      "\n",
      " What is this called?\n",
      "\n",
      "\n",
      " Titan, not Titan. The NVIDIA server, the model server, inference server, there is this thing, I'll remember, Triton or something.\n",
      "\n",
      "\n",
      " You load it onto that, and you run it, and it runs blazing fast.\n",
      "\n",
      "\n",
      " So that's what my team does.\n",
      "\n",
      "\n",
      " So anyway, this question arose, which of the two to pick? Anyway, HuggingFaces is a layer over both.\n",
      "\n",
      "\n",
      " It can work with both.\n",
      "\n",
      "\n",
      " Now the HuggingFace API has three core libraries. I would say four core libraries, four libraries, but technically three libraries. One is the datasets library. The datasets library is just wonderful. People produce data in all sorts of formats.\n",
      "\n",
      "\n",
      " This guy has CSV, that guy has JSON.\n",
      "\n",
      "\n",
      " It has this naming convention for the features and that for label, the target variable and this and that.\n",
      "\n",
      "\n",
      " In some sense, it brings a method to the madness.\n",
      "\n",
      "\n",
      " And it gives you a simple one-liner to get data sets, load data sets. We are going to learn about that data set library and see how convenient it is to use that library. Amongst its beautiful features are, so, you know, if you have a library to deal with data sets, it should have the following good characteristics. First, there should be a large repository of data sets that you can access. And we will see that it has access as of today morning, I believe it was 29,000 datasets it could access.\n",
      "\n",
      "\n",
      " Those datasets included text, they included audio, they included video, they included all sorts of things.\n",
      "\n",
      "\n",
      " Rich, rich library for you to play with. And we'll play with one of those libraries today, one of the data sets today.\n",
      "\n",
      "\n",
      " We will take the problem of classification and in classification, the simplest problem that are the quintessential poster child problem you solve is sentiment analysis.\n",
      "\n",
      "\n",
      " But we'll go beyond just positive and negative sentiment. We'll have a spectrum of six sentiments, sorrow, sadness, anger, so on and so forth. All of those emotions we'll have. So for that, we'll use the emotions data set. And we'll see how easy it is to use the data set.\n",
      "\n",
      "\n",
      " So there's a rich thing.\n",
      "\n",
      "\n",
      " Secondly, the API should be dead simple. Let us say that it is dead simple, but a work in progress.\n",
      "\n",
      "\n",
      " Not everything is obvious.\n",
      "\n",
      "\n",
      " I mean, some things you have to work a little bit harder. It makes common things dead simple, but a work in progress.\n",
      "\n",
      "\n",
      " Not everything is obvious.\n",
      "\n",
      "\n",
      " I mean, some things you have to work a little bit harder. It makes common things dead simple, but when you try to do something else, you have to work a little bit longer or, you know, poke through the documentation. The third advantage it has is reading from standard file format, CSV, JSON should be dead simple.\n",
      "\n",
      "\n",
      " It is, right?\n",
      "\n",
      "\n",
      " Just like pandas library. And the fourth is, it should have interoperability with other libraries.\n",
      "\n",
      "\n",
      " So the datasets, for example, the most common dataset loading framework that we know of in data sciences, the data frame, right? Pandas data frame.\n",
      "\n",
      "\n",
      " And it is absolutely interoperable with it.\n",
      "\n",
      "\n",
      " The data frame, right? Panda's data frame.\n",
      "\n",
      "\n",
      " And it is absolutely interoperable with it. And because Panda's data frame itself is interoperable with many things, for example, with Spark data frame, now you can start chaining the thoughts together, right? You could generate data in Spark, by Spark, and then gradually have a bridge over to this data set. And so you can do all that. Of course, you can write it as a file and then load it in data set, or you can directly with a couple of lines of code do a translation straight to the data set and save it. So those are the advantages of using the data set core library. Its internal representation of the data set is also pretty good.\n",
      "\n",
      "\n",
      " I believe it's Arrow. It uses Arrow as its native format, unless you give it something else. Arrow is a code based written in C++.\n",
      "\n",
      "\n",
      " It's a very efficient and high performance data format and gaining a lot of adoption.\n",
      "\n",
      "\n",
      " So one news that you may not be aware of is Pandas 2 came out. I don't know if you noticed that.\n",
      "\n",
      "\n",
      " Was it Pandas 2 or Pandas next version? I think it's Pandas 2. Pandas 2, right? Pandas 2 came out and guess what? Pandas 2 has support for Arrow as a backend, makes it much faster. And pretty much it sort of decreases the gap between the Pandas and polar.\n",
      "\n",
      "\n",
      " Come again?\n",
      "\n",
      "\n",
      " What is arrow? Arrow.\n",
      "\n",
      "\n",
      " So see when you represent data in memory what used to happen is pandas would typically use numpy arrays. NowPy arrays is far better than using Python data types because Python data types less dictionaries are horrendously slow.\n",
      "\n",
      "\n",
      " It's an interpreted language.\n",
      "\n",
      "\n",
      " So one way that people used it is they know that it's very hard to beat Fortran. So the Fortran's basic linear algebra package, blast and all of that, Fortran arrays are super duper fast. NumPy is a Python library on Fortran and C. So when you use, when you create your data structure in NumPy, you're basically using Fortran and so forth.\n",
      "\n",
      "\n",
      " And obviously because of that, it was a de facto standard.\n",
      "\n",
      "\n",
      " Python basically, Pandas basically was a simple usable interface on top of many for manipulating NumPy arrays, NumPy data structures.\n",
      "\n",
      "\n",
      " And it brought in the mindset of DataFrame. DataFrame is a mindset that has pervaded statisticians and data scientists for ages, since antiquity. Antiquity being in computer science world, maybe two decades, right? Two, three decades.\n",
      "\n",
      "\n",
      " So it's been there.\n",
      "\n",
      "\n",
      " So NumPy though has certain limitations.\n",
      "\n",
      "\n",
      " Data, when you represent it in columnar format, you can do it in a much more compressed way, right? Because for example, taking project, one of the common things you do is you get a data set with many features, but you want to drop some features and keep only some.\n",
      "\n",
      "\n",
      " When you do it with a matrix, you literally have to have another matrix ready for it. Whereas when your data is stored columnarily, it's very easy to pick a few columns.\n",
      "\n",
      "\n",
      " Do you see that?\n",
      "\n",
      "\n",
      " And so columnar data structures have been known in the analytics world to be the most efficient format of data storage for analysis.\n",
      "\n",
      "\n",
      " You see that for that reason.\n",
      "\n",
      "\n",
      " Aggregation along columns, sum, totally sweet, isn't it?\n",
      "\n",
      "\n",
      " If data is stored along columns, you have the locality of reference.\n",
      "\n",
      "\n",
      " The whole thing can be loaded into the CPU cache, and in one zip, you compute the sum or aggregation or whatever things that you're doing on a feature.\n",
      "\n",
      "\n",
      " So for that reason, columnar formats are better. Arrow is a columnar format. Pandas 2.0 supports columnar format. I believe dataset, correct me guys if I'm wrong, datasets to the best of my knowledge is native columnar arrow format.\n",
      "\n",
      "\n",
      " Again, speaks to its efficiency.\n",
      "\n",
      "\n",
      " Now, so that is one of the core libraries of Hugging Face. The other core library of Hugging Face is the tokenizers. It's part of the transformer library, but we consider it as a core part of it, the tokenizers.\n",
      "\n",
      "\n",
      " What in the world are tokenizers? Who would enlighten us? What are tokenizers?\n",
      "\n",
      "\n",
      " Say that again? Did someone respond? What are tokenizers? .\n",
      "\n",
      "\n",
      " Similar, yeah. Those are lexist parsers. They do the tokenization. See, guys, tokenization, the word comes from obviously if you are doing compilers, et cetera, lexing parsing, the words.\n",
      "\n",
      "\n",
      " So let's put it this way.\n",
      "\n",
      "\n",
      " You have text. But if you look at text, there are infinitely many texts. But you need to break it down into constituent parts, a smaller vocabulary. Those smaller vocabulary components are called tokens.\n",
      "\n",
      "\n",
      " Now, for example, when you write a language, let's say a language like C, what does your compiler do?\n",
      "\n",
      "\n",
      " The first part that it does is there's a lexa parser.\n",
      "\n",
      "\n",
      " It will take your text, chop it chop, chop, chop into tokens, the tokens that it understands.\n",
      "\n",
      "\n",
      " Because the grammar admits only so many words in the vocabulary.\n",
      "\n",
      "\n",
      " And each of the tokens must belong to it.\n",
      "\n",
      "\n",
      " Otherwise, it will throw a compiler error.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " And not only that, then there are syntactic rules on how you can put those things together into that.\n",
      "\n",
      "\n",
      " And that's the grammar of the language. Now, in NLP, you don't really go so much's the grammar of the language. Now in NLP, you don't really go so much into the grammar of the language, but you do still tokenize the text into constituent parts.\n",
      "\n",
      "\n",
      " The question is, what are those parts? And those parts are tokens. Now, what is a token though?\n",
      "\n",
      "\n",
      " It turns out that the answer to that question is a little bit more interesting, more interesting than you think.\n",
      "\n",
      "\n",
      " You may think word, you may think sentence is a token, but there are languages that don't exactly admit the notion of a sentence, right? They don't quite have very strict definitions of sentence boundaries. have very strict definitions of sentence boundaries.\n",
      "\n",
      "\n",
      " Then well not that then what?\n",
      "\n",
      "\n",
      " Words? Well word looks like a logical way to break it up\n",
      "\n",
      "\n",
      " and it's somewhat.\n",
      "\n",
      "\n",
      " The trouble with words is that there are two problems with words. Words are many. The English vocabulary is almost 5 million or 3 million unique words, growing 5 million perhaps. Most of those words are rarely used. The most common words are about, I don't know, 10 to 20,000 words, isn't it? So you know, guys guys that you can learn any language and be reasonably fluent with that language by the time you have learned the first 200 words right you can be reasonably conversant with it by the time you memorize only a thousand words you're doing very well\n",
      "\n",
      "\n",
      " you're pretty much at the high school level or so. And by the time you do 5,000 words, by the way, I remember that to come to graduate school, you had to take the GRE exam.\n",
      "\n",
      "\n",
      " And there is a guide to GRE called the Barons.\n",
      "\n",
      "\n",
      " And it used to have a vocabulary of 5,000 words. And if you really knew all those 5,000 words, it would absolutely ace your GRE verbal section.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So it just shows you that 5000 words puts you into the graduate school category.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " In that language.\n",
      "\n",
      "\n",
      " So there is, in other words, a long tail distribution here.\n",
      "\n",
      "\n",
      " Most often used words are few.\n",
      "\n",
      "\n",
      " So what do you want to do?\n",
      "\n",
      "\n",
      " Do you really want to use a vocabulary of 5000 words?\n",
      "\n",
      "\n",
      " Because in machine learning, you have to do something called one hot encoding for categorical variables. A word is a categorical variable. It's not a measure. It's not a number 3.65.\n",
      "\n",
      "\n",
      " It is like you have to identify an animal, parrot, cat, dog, horse, for animals. It's a categorical. So any token is a word. Which word is a question you can ask. But if there are three million words or five million words, you will have to one-hot encode it into a vector with five million dimensions. What's wrong with 5 million dimensions? If nothing else you'll be buying a lot of hard disk space and memory to load it, isn't it?\n",
      "\n",
      "\n",
      " It's wasteful.\n",
      "\n",
      "\n",
      " And computationally it's just terribly wasteful to do matrix computations with you know five five five hundred five million dimensions especially when that's such a sparse sparse matrix the it's not good but there is actually a further problem certain languages are what is called augmentative or there's a word for it in linguistics what it means is you you can you you have a concept in India.\n",
      "\n",
      "\n",
      " They use the word sandhi. And Sanskrit is very common.\n",
      "\n",
      "\n",
      " So you can have an entire sentence made up of just one word because you have conjoined all the words together. So then the reader is supposed to know where the boundaries of where to tokenize it, how to mentally tokenize it into word parts which individually have meaning.\n",
      "\n",
      "\n",
      " And that is a clue.\n",
      "\n",
      "\n",
      " Finland has the same issue. Japanese has the same issue.\n",
      "\n",
      "\n",
      " You can go on concatenating words together into a word of arbitrary length.\n",
      "\n",
      "\n",
      " And I believe German has a similar issue.\n",
      "\n",
      "\n",
      " Oh okay, nice.\n",
      "\n",
      "\n",
      " So Praveen is mentioning that OpenAI has a tool for tokenization that it will do.\n",
      "\n",
      "\n",
      " So but today we'll focus on the Hugging Faces tokenizer library and\n",
      "\n",
      "\n",
      " but yeah\n",
      "\n",
      "\n",
      " actually why don't we take this as a homework guys explore the OpenAI's tokenizing library also. So whenever you have an NLP library they will always have tokenizers to break it up into pieces.\n",
      "\n",
      "\n",
      " Spacey for example has a tokenizer. Spacey was what we used in the last iteration of this course, a very popular, very good library.\n",
      "\n",
      "\n",
      " But now they are upgrading themselves to be transformer compatible. spaCy 3 is transformer integrated.\n",
      "\n",
      "\n",
      " So therefore, it has value if you're using spaCy.\n",
      "\n",
      "\n",
      " Very fast library. And so every library, NLTK has its tokenizers and so on and so forth, but we'll focus on the hugging face tokenizers. A very good exercise is to see how other tokenizing libraries work.\n",
      "\n",
      "\n",
      " So tokenizer is the second piece.\n",
      "\n",
      "\n",
      " But from this fact that words can be conjoined, we get an idea that maybe the fundamental unit of the vocabulary shouldn't be word, but the sort of atomic word pieces, word segments or word parts that should be.\n",
      "\n",
      "\n",
      " And I'll give you a further hint.\n",
      "\n",
      "\n",
      " Suppose you use the word, just look at the word tokenizer itself.\n",
      "\n",
      "\n",
      " There is token plusizer. Theizer word can be used as a suffix to many other words. Can you think of what other words?\n",
      "\n",
      "\n",
      " Stabilizer.\n",
      "\n",
      "\n",
      " Yeah, and so forth.\n",
      "\n",
      "\n",
      " Each one of you can think of a word which answer ISA. So you can really think that it's actually what does ISA do?\n",
      "\n",
      "\n",
      " The doer of it.\n",
      "\n",
      "\n",
      " The maker of tokens, the maker of stability.\n",
      "\n",
      "\n",
      " So you should really take them as two different words, token and ISA, word parts, right? And likewise, singular, plural, what if I add a S to it?\n",
      "\n",
      "\n",
      " Tokenizer, tokenizers, token, tokens, right?\n",
      "\n",
      "\n",
      " So the plural itself gives you a clue that you don't want the singular and the plural both to be sitting in the vocabulary.\n",
      "\n",
      "\n",
      " It's a waste.\n",
      "\n",
      "\n",
      " So you could have a word part that is a word segment that is just the suffix s and the suffix eiser. So that is beginning to give you a sense of how better to construct a vocabulary.\n",
      "\n",
      "\n",
      " We'll go through that exercise today.\n",
      "\n",
      "\n",
      " The third part of the Hugging Face core library is the transformer themselves, the transformers, the models. Now there is a rich variety of transformers there, but here is the interesting thing.\n",
      "\n",
      "\n",
      " Because a transformer, if you remember remember we said but you could use bird for classification you could use bird for figuring out fill in the blanks finding the missing the mass word you could use bird for entailment whether the second segment sentence actually makes sense as a follow-up to the first thing right for question answer things like that you can do but model for anything. So how come one transformer model can do all these things?\n",
      "\n",
      "\n",
      " And the answer to that, so if you look at the cross encoder, for example, you give it two sentences and it produces the probability that they are similar, right? You give it a classification task, you give it something, and you ask, what is the sentiment?\n",
      "\n",
      "\n",
      " Then you don't give it the second sentence.\n",
      "\n",
      "\n",
      " You just pad it up with empty, but you're asking the bird to produce a probability for an emotion. It is producing, let's say a soft max probability for some emotion, positive, negative, anger, hate, anger, sadness, joy, whatever, surprise, whatever.\n",
      "\n",
      "\n",
      " How could it do that that you're using the same transformer architecture for many things and so the way to think about it is this see what you do is and i'll give you the big picture you just look at the transformer without the head right so a headless transformer right now it's a little bit of a gory image. Don't think of headless people like zombies walking around.\n",
      "\n",
      "\n",
      " Right? But it's somewhat like that. It's like a zombie walking around.\n",
      "\n",
      "\n",
      " It's very capable of something. But something is the head. You need to screw in a head, appropriate head, for it to do anything. head. You need to screw in a head, appropriate head for it to do anything.\n",
      "\n",
      "\n",
      " So one very easy example that I think of is, you know, you get electric drills, this called these electric drills. Can you do anything with a drill? You can't do anything with a drill till you put a drill bit into it, isn't it? Now, based on what drill bit you put into it, it will either, you know, you can use it for a whole variety of purposes.\n",
      "\n",
      "\n",
      " Do you see that?\n",
      "\n",
      "\n",
      " So that, for me, that visual sort of metaphor makes idiom.\n",
      "\n",
      "\n",
      " I don't know. Is it idiom? Is it metaphor? I think metaphor would be the right word, isn't it? So that metaphor is what I carry in my head.\n",
      "\n",
      "\n",
      " It's a drill without the bits.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " And you need to attach different bits to do different things. So that different thing is called the head. So now how in the world does that make sense? See what happens is that the transformer, ultimately what did it do?\n",
      "\n",
      "\n",
      " It created a hidden state representation.\n",
      "\n",
      "\n",
      " All the tokens that go through all of those multi heads and so forth.\n",
      "\n",
      "\n",
      " Take BERT, and what do they do?\n",
      "\n",
      "\n",
      " They all create their vectors. The vectors get produced. And of course, the CLS token, the special token, also produces its vector. So each token has a vector representation. You can call it a tensor representation. But let's say vector representation, to be intuitive. Now, these vector representations are there. After that, let's say you're doing a classifier. You just take the vector coming out of the CLS and feed it into a classifier. Now, how do you do a classifier?\n",
      "\n",
      "\n",
      " Well, forget about transformers.\n",
      "\n",
      "\n",
      " How would you do a classifier? Maybe one example is you would just take a softmax, right? And into the softmax layer, you would feed in the input vector and output come because it's a soft max, it would produce probabilities for each of the cat probabilities, this dog probability is this horse probability is this and whichever is the highest, you say that's it.\n",
      "\n",
      "\n",
      " It's a cat, isn't it?\n",
      "\n",
      "\n",
      " And soft max, of course, exaggerates a bigger number.\n",
      "\n",
      "\n",
      " So that's a classifier. Now, that is the simplest classifier you can put, softmax. Or if it is a binary classification, you can just put a logistic classifier there.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " Logistic or something like that.\n",
      "\n",
      "\n",
      " Now comes the next part.\n",
      "\n",
      "\n",
      " You could do even better.\n",
      "\n",
      "\n",
      " You could say I will take that and actually feed it through a few layers of my peanut butter and jelly.\n",
      "\n",
      "\n",
      " What are those?\n",
      "\n",
      "\n",
      " Feed forward layers.\n",
      "\n",
      "\n",
      " You may say that, well, let me learn some even higher orders of representation from the output of the transformer body.\n",
      "\n",
      "\n",
      " And then last layer is the sortness.\n",
      "\n",
      "\n",
      " So you can play games with it. You can do fancy stuff with it. And it's up to you how fancy you can get. And who determines what is the right way to do it? How do we determine which is the right way to do it?\n",
      "\n",
      "\n",
      " Like, should I just use the softmax? Or while you're at it, why not throw 10 more layers of feedforward and then put a softmax?\n",
      "\n",
      "\n",
      " How would you decide which one to go with? How would you decide which one to go with?\n",
      "\n",
      "\n",
      " Yeah the results speak.\n",
      "\n",
      "\n",
      " So what you have to do is you have to look at the measure, goodness of a model measure, something like either accuracy or F1 score or precision or recall based on the context. You pick that measure and you basically the number, the head, the shape of the head, how many layers there are, whatever, etc.\n",
      "\n",
      "\n",
      " It is a hyper parameter of the model, isn't it?\n",
      "\n",
      "\n",
      " To put three layers or five layers, to put no layers, just a softmax, one layer, just softmax.\n",
      "\n",
      "\n",
      " That is a decision that is a hyper parameter of the model.\n",
      "\n",
      "\n",
      " You have to do it and figure out what works best.\n",
      "\n",
      "\n",
      " Ultimately, who decides? The data decides. The validation set decides which is the right approach to deal with it.\n",
      "\n",
      "\n",
      " And then again, you can get fancy, you can have residual links and so on and so forth.\n",
      "\n",
      "\n",
      " You could do all of that, up to you but the bottom line is i will just use the word in general classifier head to classifier similarity head let's say it's cosine similarity head to do cosine similar and so on and so forth you can apply a screener head to that so that is your transformer library with a catch each of these transformers what are they expecting?\n",
      "\n",
      "\n",
      " They're expecting input vectors, embeddings themselves. They're not expecting words as in language text. So what are they expecting?\n",
      "\n",
      "\n",
      " Token embeddings, tokens and embedded already nicely indexed and given to you.\n",
      "\n",
      "\n",
      " They don't expect the word CAT.\n",
      "\n",
      "\n",
      " embedded already nicely indexed and given to you.\n",
      "\n",
      "\n",
      " They don't expect the word CAT, they expect 49 hot encoded to whatever 49 becomes in the vocabulary, given the vocabulary. Assuming that the cat word corresponds to the token 49, token ID 49, you usually call it the input ID 49.\n",
      "\n",
      "\n",
      " So that is the architecture. So it will be a sandwich of three things.\n",
      "\n",
      "\n",
      " Transformer will be the model will be in the center at the beginning. First, the text comes in, input doesn't go and hit the transformer. Input goes and hits the tokenizer. It gets tokenizer to input IDs and encoding and so forth. And then it gets hit the transformer. Transformer takes that and further adds positional embeddings, segment embedding and whatnot.\n",
      "\n",
      "\n",
      " And so forth. It does that.\n",
      "\n",
      "\n",
      " And the tokenizer has already added some things more. It has added, actually, segment embedding and attention layers, it's already added. Position embedding the transformer will add, shoot it through the attention heads and it's feed forwards and so forth.\n",
      "\n",
      "\n",
      " And out will come a hidden representation.\n",
      "\n",
      "\n",
      " I call it, I tend to call those words in a more general sense embeddings. Not many people would use that word embedding in such a general sense, but to me,'s embedding embedding is a vector in a latent space, right in a hidden space. It is a, these are vectors, each word becomes a vector in its own space.\n",
      "\n",
      "\n",
      " Each token becomes and that goes into the final head that you screw up for whatever purpose you want.\n",
      "\n",
      "\n",
      " Yes, they are. They are tokens.\n",
      "\n",
      "\n",
      " And we'll see it right now. So today is the practice day. So see, guys, now let's get into the labs. I give you the big picture because I wanted you to have this context in mind as we get into the labs.\n",
      "\n",
      "\n",
      " Now, over these three core libraries, namely the dataset, the tokenizers, the models, right?\n",
      "\n",
      "\n",
      " You realize that last Saturday, we sort of danced our way through the rose garden.\n",
      "\n",
      "\n",
      " We didn't have to deal with any of those. What did we use? We used pipeline. You remember, here is a pipeline for classification. Here is a pipeline for this sentiment analysis. Here is a pipeline to do a named entity resolution. Here is a pipeline to do whatever it is.\n",
      "\n",
      "\n",
      " There's so many.\n",
      "\n",
      "\n",
      " I believe the last I checked, there were almost 16 or 20 pre-cooked pipelines that Hugging Faces comes in.\n",
      "\n",
      "\n",
      " So those are the common tasks. So now that you look at it like that, what are those pipelines done? When somebody has that pipeline, what has he done? Somebody has sat down and put these things together for you. somebody has sat down and put these things together for you.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " So last time we used the pipeline cycle.\n",
      "\n",
      "\n",
      " Now your first response should be, when you solve a practical problem, always baseline with a pipeline because it will give you a rough and ready answer of how much accuracy you are getting or whatever your measure is, accuracy, F1 score, precision, recall, whatever matters, how well it is doing, start with a given pipeline, right?\n",
      "\n",
      "\n",
      " And now you can change the models. You can try different models as arguments to the pipeline.\n",
      "\n",
      "\n",
      " You can do that. You will get some idea.\n",
      "\n",
      "\n",
      " There is actually a significant degree of adaptability there, flexibility there.\n",
      "\n",
      "\n",
      " Because in the pipeline, remember, we just gave the task name.\n",
      "\n",
      "\n",
      " What is the task?\n",
      "\n",
      "\n",
      " It's a sentiment analysis.\n",
      "\n",
      "\n",
      " But if you look carefully at the pipeline API, you will notice, and that is what we are going to do today, dig into the details, you will notice that it also gives you the choice of specifying the tokenizer and specifying the model and the classifier.\n",
      "\n",
      "\n",
      " So all the pieces you can specify. What it does is it glues them together for you properly because that's a bit of code.\n",
      "\n",
      "\n",
      " When you do it by hand you have to struggle with it.\n",
      "\n",
      "\n",
      " No, no, for everything, all transformers sentence forget about sentence transform for the timing that was only for search i'm saying all transformers right for all tasks so pipelines is so they are god-given pipelines or rather hugging faces given pipelines and their pipelines we will learn to build our own pipeline today.\n",
      "\n",
      "\n",
      " So with that, what people have done, as I said, is they have contributed a lot of models that have been trained on something. So when they give a model, it is, what is it?\n",
      "\n",
      "\n",
      " It's a neural architecture, but it is also the weights because ultimately when you train a neural network, it's a fancy way of saying is, I have magically determined the best set of weights that will get your job done, some job done, isn't it? So it is no different from some fairy just moving a magic wand and telling you these are the best weights.\n",
      "\n",
      "\n",
      " Equivalent, so long as it works.\n",
      "\n",
      "\n",
      " But what you get are the weights of a model. Those set of weights there is a more technically what you say it is a checkpoint. The word checkpoint is often used as that you're downloading a checkpoint when you people say that I'm downloading a model in reality what they're doing is or more precise way of saying it is a bird is the model let's say but but checkpoint is bird uncased let us say the base uncased so what it means is i take the bird base model means bird has a larger model with a lot more.\n",
      "\n",
      "\n",
      " I believe BERT base has six attention heads of 12 and the BERT large has 12 or 24. I forget the exact numbers, but one is much bigger than the other.\n",
      "\n",
      "\n",
      " Lots of attention heads and layers.\n",
      "\n",
      "\n",
      " So the BERT base is the basic model or BERT large is the bigger architecture. But given the architecture, you have trained it on some data, right? When you say that I've trained it on a corpus, BERT base uncased, what does it mean? You have trained it with input in which everything has been turned to lowercase.\n",
      "\n",
      "\n",
      " Uncased means you don't distinguish between small and big cases in languages that have small and big cases. For example, Hindi doesn't have small and big cases. I think many languages don't have it. For example, in my view, Hebrew doesn't have it. Middle Eastern languages, none of them have it. Sanskrit doesn't have it. Indian languages also tend not to have it. But European languages tend to have small and big cases, upper cases.\n",
      "\n",
      "\n",
      " So it'll bring it down.\n",
      "\n",
      "\n",
      " Now, typically when it says uncased, usually these models are also like BERT, for example, that BERT, BASE, uncased.\n",
      "\n",
      "\n",
      " If you read the documentation, so associated with all these transformers, there's a convention. If you read the documentation, so associated with all these transformers, there's a convention.\n",
      "\n",
      "\n",
      " We call it the model card. A model card is a standard.\n",
      "\n",
      "\n",
      " Think of it as a billboard, right? Or a reference card that tells what the model is, right?\n",
      "\n",
      "\n",
      " And how it's been trained.\n",
      "\n",
      "\n",
      " So it will say, we have used word base, we have used uncased. And the corpus we trained it on is English corpus.\n",
      "\n",
      "\n",
      " Why?\n",
      "\n",
      "\n",
      " Because English is the dominant language in the data set that is accessible or publicly available. Other languages are far less represented. Now somebody told me that actually French is more spoken in the world than English.\n",
      "\n",
      "\n",
      " Is that true? That's what the French say. Is that true? That's what the French say. Right.\n",
      "\n",
      "\n",
      " But for some reason, the French corpus is not, doesn't seem to be as big.\n",
      "\n",
      "\n",
      " And by the time you go to a corpus of like Tibetan, now you're getting into trouble.\n",
      "\n",
      "\n",
      " Smaller corpuses.\n",
      "\n",
      "\n",
      " How many of you in the room know Tibetan?\n",
      "\n",
      "\n",
      " No. OK.\n",
      "\n",
      "\n",
      " So you look at the model card.\n",
      "\n",
      "\n",
      " And what has happened is when you download, what you're effectively downloading is not just the architecture part. You're downloading actually the weights that has already been trained.\n",
      "\n",
      "\n",
      " That's why you call these models, these checkpoints, the checkpoints represents a word that we use, pre-trained. You download a pre-trained model.\n",
      "\n",
      "\n",
      " Why?\n",
      "\n",
      "\n",
      " Because training a model from scratch is horrendously expensive. So what I'm alluding to is the concept of transfer learning.\n",
      "\n",
      "\n",
      " So we'll start there today.\n",
      "\n",
      "\n",
      " So the top five sources of knowledge is Mandarin, Chinese, Spanish, English.\n",
      "\n",
      "\n",
      " We ended with, I'll just recap for a moment. We ended with visual QA.\n",
      "\n",
      "\n",
      " Let me remind you what we did. We gave it a, Let me remind you what we did.\n",
      "\n",
      "\n",
      " We said that transformers are capable of great things. Amongst the great things they're capable of, you can give it a picture like this and you can say, what is this animal? And it will tell you it's a snow leopard.\n",
      "\n",
      "\n",
      " Isn't that amazing?\n",
      "\n",
      "\n",
      " You can give it any picture.\n",
      "\n",
      "\n",
      " And how easy was it with the hugging face pipeline?\n",
      "\n",
      "\n",
      " Now think about this line, guys. After you do what we are going to do today, you realize that it's just powerful to invoke a model with one line and be able to use it.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " Image classifier can immediately tell you what this image is. And right away it tells you it's a snow leopard with 93% probability. And it gives another 5% probability to just leopard between 93 and a half plus five, almost six. No, it is almost 99% probability.\n",
      "\n",
      "\n",
      " And then jaguar, lynx, cheetah, which look like leopards, are far, far less.\n",
      "\n",
      "\n",
      " How very accurate it is.\n",
      "\n",
      "\n",
      " The other example we took is you can give it a picture and ask it questions about the picture.\n",
      "\n",
      "\n",
      " Look at this code.\n",
      "\n",
      "\n",
      " We are creating another pipeline, one line pipeline for visual question answer.\n",
      "\n",
      "\n",
      " You give it an image and you ask, what is the invoice number in the image?\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " Now, the old way used to be that you would do some segmentation, right?\n",
      "\n",
      "\n",
      " You would zoom into that area, put a box around it, then do specific recognition of that particular OCR or something like that.\n",
      "\n",
      "\n",
      " But now you're not giving it any hints.\n",
      "\n",
      "\n",
      " You're saying, what is the invoice number? I don't know how many of you are impressed, but I am, that it can tell you that the invoice number is this, US001.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " Why don't we run it and see? Let's do that. Let me run this.\n",
      "\n",
      "\n",
      " And what is the name?\n",
      "\n",
      "\n",
      " Oh, sorry.\n",
      "\n",
      "\n",
      " Name pipeline.\n",
      "\n",
      "\n",
      " I have to go to the top and run everything.\n",
      "\n",
      "\n",
      " One second.\n",
      "\n",
      "\n",
      " I think the link to the photo has expired for the Snow Leopard.\n",
      "\n",
      "\n",
      " Oh.\n",
      "\n",
      "\n",
      " When I ran that, it it had an issue why is it working for me hang on let me see\n",
      "\n",
      "\n",
      " oh no\n",
      "\n",
      "\n",
      " i have replaced it with that so i have to give you guys the new notebook\n",
      "\n",
      "\n",
      " okay\n",
      "\n",
      "\n",
      " i'll give you the new notebook so at this moment just point to any picture and you'll see\n",
      "\n",
      "\n",
      " okay\n",
      "\n",
      "\n",
      " so this is that now let me run this. Oh, it turns out by the way, I'm using a new machine. So I too am downloading it.\n",
      "\n",
      "\n",
      " And what would just happen here? Did it fail or do something weird?\n",
      "\n",
      "\n",
      " If you provide it without word boxes and the pipeline will but is not okay on this machine guys I have to install the libraries, give me a moment.\n",
      "\n",
      "\n",
      " Why don't I do that.\n",
      "\n",
      "\n",
      " I have to go and install these libraries.\n",
      "\n",
      "\n",
      " So why don't I take a rain check on it because I need to go to the server and run this command I'll show it to you after lunch.\n",
      "\n",
      "\n",
      " I will do that.\n",
      "\n",
      "\n",
      " This is the new machine, by the way, that Sukhpal helped me build. So my thanks to Sukhpal. By the way, Sukhpal has also built a machine for Abhijeet.\n",
      "\n",
      "\n",
      " Desktop. Really beautiful.\n",
      "\n",
      "\n",
      " You should see his machine.\n",
      "\n",
      "\n",
      " I'm envious.\n",
      "\n",
      "\n",
      " Now i want to exchange mine with his right so 4090 is just\n",
      "\n",
      "\n",
      " yeah yeah\n",
      "\n",
      "\n",
      " and now he's beginning to hate his mac\n",
      "\n",
      "\n",
      " it's way better right so yeah you can see what is the total amount and it says $154 and 0.6 cents.\n",
      "\n",
      "\n",
      " Like in the invoice, what is the amount it's able to answer, but do you notice that it's well, if you look at the image, if you look at the image, there is no word. There's an amount column, but it is not obvious in the amount column there are many amounts isn't it so it takes some level of inference to know that this is the one that you have to answer right then you can ask what is the invoice date and the invoice date it gets wrong actually so is the limitation invoice date is 11 slash 02 slash 2019 and it interprets the slash as one.\n",
      "\n",
      "\n",
      " So there's a little bit of an error here, which speaks to the OCR library behind it.\n",
      "\n",
      "\n",
      " Then you say, who is it billed to? Now, this is an invoice. Billed to is John Smith right here.\n",
      "\n",
      "\n",
      " And it figures that out. And you can go on asking questions.\n",
      "\n",
      "\n",
      " In the afternoon, I'll install the library and you can go on asking questions in the afternoon i'll install the library and you can see who is serving the invoice if you look at this it's not so obvious who's serving the invoice right there's a big invoice there's an address it doesn't say from\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " but it infers what it is from and we didn't train it i don't know if you're impressed i am that one didn't train it then like somebody repaint pre -trained yes somebody trained it\n",
      "\n",
      "\n",
      " and so that speaks to the power of transfer learning you benefit from other people's hard work people's hard work.\n",
      "\n",
      "\n",
      " Go ahead.\n",
      "\n",
      "\n",
      " I was just saying that in that case, isn't that this whole transformer or the model is going to go towards a global repository where you have models trained by and it's being trained by everybody and that's where...\n",
      "\n",
      "\n",
      " Yeah, Hugging Face is that repository. Yeah, yeah, that is that.\n",
      "\n",
      "\n",
      " that is literally what hugging face is is the right now we are downloading that right versus it's more towards those models are being hosted also so they're hosted so one of the things we will learn in this course is when you build your own perfect model how to contribute it back to hugging face so that others can benefit from it. So see what happens is when you train your own model and you upload it, your model becomes pre-trained model for someone else and they can do transfer learning.\n",
      "\n",
      "\n",
      " They can use it and that is the HuggingFace hub.\n",
      "\n",
      "\n",
      " I think as a bonus, like following up on that\n",
      "\n",
      "\n",
      " So if I look at something like a chat GPT, you're not downloading that.\n",
      "\n",
      "\n",
      " It's kind of. No, that is the problem. See, the problem that has happened is. OK, so for those of you who are remote, let me repeat the question. The question is.\n",
      "\n",
      "\n",
      " Chat GPT, we are not downloading.\n",
      "\n",
      "\n",
      " And it's a sad, sad day that we have come to this.\n",
      "\n",
      "\n",
      " It started with GPT-3, the open AI all of a sudden realized that they are sitting upon a gold mine and they made frankly in my view lame excuses.\n",
      "\n",
      "\n",
      " They said oh the model is so big it won't fit into the people's machines. Hey let people decide whether they can fit it on their machines or not.\n",
      "\n",
      "\n",
      " But in any case, they didn't allow it.\n",
      "\n",
      "\n",
      " ChatGPT4, at least ChatGPT3, they sort of opened it and say how it was done. ChatGPT4, they have written a paper that's more like a white paper.\n",
      "\n",
      "\n",
      " Technical details are rather sparse, right?\n",
      "\n",
      "\n",
      " So they're drawing the curtains on it.\n",
      "\n",
      "\n",
      " It's pretty terrible, actually.\n",
      "\n",
      "\n",
      " And I would strongly say that given a choice between an open source model and one of these proprietary commercial models, always use the open source.\n",
      "\n",
      "\n",
      " At least encourage it. Support, sponsor open source.\n",
      "\n",
      "\n",
      " If you have to give $100, if you have $100 to give, give it to the community.\n",
      "\n",
      "\n",
      " Don't give it to these sharks, frankly.\n",
      "\n",
      "\n",
      " So what I was going to ask is, I think to Saran's point, so for Huggins Face, you download the model, you train it, and then we load it back again.\n",
      "\n",
      "\n",
      " If you want to, yeah, somebody else can use it.\n",
      "\n",
      "\n",
      " In this open AI, it's kind of almost like this cloud.\n",
      "\n",
      "\n",
      " So you basically, you're like training their own model. No, so what they have done is they have pre-trained, they have a pre-trained model. When you use ChatGPT, you're just exercising the pre-trained model.\n",
      "\n",
      "\n",
      " But if I load my data, then I find-\n",
      "\n",
      "\n",
      " Cue it, yes, exactly. So that's the second part.\n",
      "\n",
      "\n",
      " We'll learn about the transfer level so then what happens is when you load your data you you get you have your private sort of variant which has your your fine tuning attached to it\n",
      "\n",
      "\n",
      " but.\n",
      "\n",
      "\n",
      " the fact that like for example a source code got leaked so how does chat uh how does gpt3 work where if i put my data into like you know one chat will that transfer to someone else's chat or is it just strictly locked in yes if you're using the public chat gpt\n",
      "\n",
      "\n",
      " yeah almost surely so there is a lovely xkcd cartoon i saw i just loved that one\n",
      "\n",
      "\n",
      " So it had a picture of some people having a conspiracy to overthrow the government, right?\n",
      "\n",
      "\n",
      " So they all say, let's meet at the docks. So the cop goes to this chat, this thing, and just says, we need to overthrow the government.\n",
      "\n",
      "\n",
      " Where are we going to meet?\n",
      "\n",
      "\n",
      " And Jack Gifford says, at the docks. And he says, gotcha.\n",
      "\n",
      "\n",
      " You see how it can be abused.\n",
      "\n",
      "\n",
      " So that is the huge societal impact of these things. And that's a point that I think I made in the past that the social and economic and political aspects need to catch up.\n",
      "\n",
      "\n",
      " They need to get real and catch up because nobody can stop this scientific movement.\n",
      "\n",
      "\n",
      " It's moving very, very, very fast.\n",
      "\n",
      "\n",
      " Again, I give you guys the background that when the steam engine was created, not many people understood how steam engines work, except that they knew how to make it work. Thermodynamics came much later, but it caused the industrial revolution, caused much benefit, cheap clothes, amongst other things, fast transport, steam engines, locomotives, but it also led to horrendous suffering.\n",
      "\n",
      "\n",
      " Children were slaved, chained to the machines, textile machines to work on them.\n",
      "\n",
      "\n",
      " The whole London was filled with smoke.\n",
      "\n",
      "\n",
      " People lived in hovels, tiny little hole in the wall places.\n",
      "\n",
      "\n",
      " And there was tremendous suffering.\n",
      "\n",
      "\n",
      " And then came sort of the socialistic counter-move movement to reform the whole thing.\n",
      "\n",
      "\n",
      " Law and order was brought in and things improved.\n",
      "\n",
      "\n",
      " We are in the same age.\n",
      "\n",
      "\n",
      " Unless we are careful, it's an industrial capture.\n",
      "\n",
      "\n",
      " Those people who own these big models and who are not willing to share it anymore. Now everyone is realizing that these models have changed the world, put the world upside down and they are going to make trillions.\n",
      "\n",
      "\n",
      " The rest of us will just be users with a begging bowl.\n",
      "\n",
      "\n",
      " Oh, can I please use your model?\n",
      "\n",
      "\n",
      " They say, okay, give us $1,000. Give us all you have.\n",
      "\n",
      "\n",
      " Reminds me of a joke.\n",
      "\n",
      "\n",
      " One guy, if I may, goes to a psychiatrist or psychotherapist or somebody and says, doctor, I feel that everyone is after my money, right?\n",
      "\n",
      "\n",
      " I can't help it.\n",
      "\n",
      "\n",
      " I have this feeling all the time.\n",
      "\n",
      "\n",
      " He says, no problem.\n",
      "\n",
      "\n",
      " I'll cure you for that.\n",
      "\n",
      "\n",
      " He says, okay, how much will you charge? So the doctor says, how much do you have?\n",
      "\n",
      "\n",
      " We are entering that world now anyway guys so this is a recap of the last time and i'll make it work on the server it's a new machine i set up but let's go back to new territory now today we are going to start with number the concept of transfer learning so transfer learning. So transfer learning is the basic idea, and I'll dwell upon it.\n",
      "\n",
      "\n",
      " As you see, there's nothing to do here, the code. So now the thinking is, if you want to solve a problem properly, what you do is, you see if the problem has already been solved, right? If it has been solved, somebody has checkpointed a model, you download that model, model checkpoint.\n",
      "\n",
      "\n",
      " Use it, right? You're done, isn't it? You're done.\n",
      "\n",
      "\n",
      " On the other hand, if you can't find your task, solution to your task, no, so even if it is done, you may say that maybe the performance can be improved because I have a special data set, my own data set, with which I can further train it, make it better.\n",
      "\n",
      "\n",
      " Are we sure?\n",
      "\n",
      "\n",
      " So then taking a model which already has some semantic understanding of the problem, and just it has already been trained through, let's say, a thousand epochs.\n",
      "\n",
      "\n",
      " And then you take your different data set and you take your data set and then you train it for a few more epochs.\n",
      "\n",
      "\n",
      " The secondary part is called fine tuning.\n",
      "\n",
      "\n",
      " Implicit to fine tuning is you don't want to fine tune on the original data set.\n",
      "\n",
      "\n",
      " Because the assumption is that they were careful enough to reach a saturation point of performance before they checkpointed it.\n",
      "\n",
      "\n",
      " Usually is true, not always true, but then but you take your own data set and then you train it.\n",
      "\n",
      "\n",
      " So what will the weights do?\n",
      "\n",
      "\n",
      " They will go through some minor perturbations and they'll readjust and they'll get fine tuned.\n",
      "\n",
      "\n",
      " That's why the word you use is fine tuning because you don't expect the weights to get radically transformed, isn't it? Weight weights is to completely look different you expect them to go through small perturbations the first one is called pre-training it's called pre-training pre-training is what you bring down\n",
      "\n",
      "\n",
      " yeah\n",
      "\n",
      "\n",
      " so transfer learning is made up of two steps pre-training and fine-tuning and now comes the interesting part of pre-training is the expensive part, usually, because you have to train it for a general problem with a vast corpus of data on usually a large cluster, right?\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " Or you have to go to Sukhpal, he'll build you a nice machine.\n",
      "\n",
      "\n",
      " Okay.\n",
      "\n",
      "\n",
      " So, and then you can train it, and then you can checkpoint it. That's your pre-trained model. Then you can train it and then you can checkpoint it.\n",
      "\n",
      "\n",
      " That's your pre-trained model.\n",
      "\n",
      "\n",
      " But fine-tuning is good because, see, you don't need so much data. Sometimes small data sets, for example, in the medical world, you're trying to classify a disease.\n",
      "\n",
      "\n",
      " How many cases will you get? Very few.\n",
      "\n",
      "\n",
      " You're trying to do a medicine compatibility with certain genomic types. You will get a data set of 100 patients, 100 gene blueprints, genomic print.\n",
      "\n",
      "\n",
      " So then it turns out it's enough because you just fine tune it at that particular moment.\n",
      "\n",
      "\n",
      " Of course, you can do small data augmentation strategies and do it and you can get away with it.\n",
      "\n",
      "\n",
      " And we did that in the deep learning course, deep learning foundations, you'll see. You learn to tell the difference between a weeping willow and a pepper tree. But when you do that, you will take a model that can basically classify things, but it can tell the difference between a house and a tree and a truck and so forth, general set of problems. But then you will, and it will even be able to tell apart the pepper tree from the whipping willow, but with a certain accuracy. But then you fine tune it. And when you fine tune, the accuracy goes up.\n",
      "\n",
      "\n",
      " Are we together?\n",
      "\n",
      "\n",
      " Or whatever metric you want to use, but in this case, accuracy makes sense. The accuracy will go up. Right.\n",
      "\n",
      "\n",
      " Now in our today's labs, precision required, but in this case, accuracy makes sense. The accuracy will go up.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " Now in our today's labs, what we'll do in the afternoon is we will take the example of a sentiment analysis. A sentiment analysis. In sentiment analysis, you will see that a pre-trained model will be able to classify at about 50, 60% accuracy. Now, 60% accuracy, when you are looking at six classes, is pretty good because randomized, its accuracy should not have been that much, right? It should have been something like 17, 18% accuracy at most. So to go from 16, 17 or so or approximately or 16% to go from there to about 60 plus, transformer is doing a good job.\n",
      "\n",
      "\n",
      " It's not a dummy baseline classifier. It's not a nonsensically classifier.\n",
      "\n",
      "\n",
      " It is doing something good. But it is, you can do better. So how will we do better?\n",
      "\n",
      "\n",
      " We will take the emotions data set and we'll do the fine tune. So we'll do both stages of it, right?\n",
      "\n",
      "\n",
      " With the pre-train\n",
      "\n",
      "\n",
      " and then we'll fine tune it, right? And if we can't finish it today, then we'll finish it in the next lab, right?\n",
      "\n",
      "\n",
      " By the way, guys, we are moving a little slower than I had planned.\n",
      "\n",
      "\n",
      " So as I said, I'm adding two labs, two extra days to this course so that we make sure that we cover all the territory that we want to cover. So instead of six days, now it's an eight-day workshop.\n",
      "\n",
      "\n",
      " I see another question.\n",
      "\n",
      "\n",
      " So we talked to the media, and they have this thing called Nemo. And so they said we could actually take it and then bring it on our own.\n",
      "\n",
      "\n",
      " Is that what they call fine-tuning?\n",
      "\n",
      "\n",
      " That would be the final fine-tuning step if you give them a pre-trained model and say they have, I don't know about Nemo that much, but if they're holding on to specific domain specific data sets and they're not sharing it with you\n",
      "\n",
      "\n",
      " but they're saying we'll find you\n",
      "\n",
      "\n",
      " no no\n",
      "\n",
      "\n",
      " it's like um it's almost like a general purpose compute completely and so this one like you can basically then we use our own data in our own um you know cluster cluster and then train that uh oh that's fine too that's fine\n",
      "\n",
      "\n",
      " yeah\n",
      "\n",
      "\n",
      " so basically they're giving you a gpu cluster as a service you're basically saying no it's a model\n",
      "\n",
      "\n",
      " actually oh it's a model it's like\n",
      "\n",
      "\n",
      " a speech recognition\n",
      "\n",
      "\n",
      " oh yeah that\n",
      "\n",
      "\n",
      " so that is that so there is a pre-trained model and then you can fine-tune it with your data that's literally that and that is transfer learning literally so guys we got the idea of transfer learning right now let's move a bit faster we will take into let's go into one example now hugging face is something let's go to the hugging face i'll quickly go there to show you what the hugging face api looks like the and by now guys you know there are two things that I'll tell you.\n",
      "\n",
      "\n",
      " The recipe for success in this field are two.\n",
      "\n",
      "\n",
      " First is the statement of Karpathy.\n",
      "\n",
      "\n",
      " He says, really be friends with your data. Know your data well.\n",
      "\n",
      "\n",
      " Right? And I say something similar.\n",
      "\n",
      "\n",
      " I say that, see, when you, in America, of course, we have an audience that probably didn't date, but in America, there's a convention. Most of the world is a convention that you find your better half through dating, right? So when you meet somebody on the first date, you're on the guard.\n",
      "\n",
      "\n",
      " You're just trying to make a positive impression.\n",
      "\n",
      "\n",
      " You don't really learn much about the other person, right? You just see like big red flags, then you don't go learn much about the other person. You just see big red flags, then you don't go on the second date or something like that.\n",
      "\n",
      "\n",
      " You just get a gut feel for it. It's only after a long association that you know the person, isn't it?\n",
      "\n",
      "\n",
      " After many, many meetings.\n",
      "\n",
      "\n",
      " I think there's a human study that says it takes about 50 or 60 encounters before we genuinely trust somebody.\n",
      "\n",
      "\n",
      " Something to that effect.\n",
      "\n",
      "\n",
      " I don't know the exact stats.\n",
      "\n",
      "\n",
      " 50-60 hours of association before we trust somebody.\n",
      "\n",
      "\n",
      " Same is true for data. Think of data. Whenever you look at data, think of it as a date. You're not likely to have a lot of success if you completely ignore your data.\n",
      "\n",
      "\n",
      " It's the same way.\n",
      "\n",
      "\n",
      " You want to know, you need to know the data really well, because it is the data that data science is about. All your prediction models, everything is about the data. So start by knowing data.\n",
      "\n",
      "\n",
      " It is the guiding light.\n",
      "\n",
      "\n",
      " It will tell you what to do and what not to do.\n",
      "\n",
      "\n",
      " And you will notice that data can be, you can learn through exploratory data analysis.\n",
      "\n",
      "\n",
      " But even more, when you build a model, you look at the way the model is making mistakes. By studying the mistakes you'll actually find, especially when you're dealing with label data, the mislabelings, because every data has errors. You can use, sometimes you can trust the data, sometimes you can trust your model. And you can use your models to find out where the data has errors and fix the errors by hand.\n",
      "\n",
      "\n",
      " Now relabel it.\n",
      "\n",
      "\n",
      " So remember your models are also great relabeling tools or data correction tools.\n",
      "\n",
      "\n",
      " So do that. We'll do that.\n",
      "\n",
      "\n",
      " And of course, we have talked a lot about exploratory data.\n",
      "\n",
      "\n",
      " The second thing that you need to really be familiar with is if you want to do carpentry, what should you be most familiar with?\n",
      "\n",
      "\n",
      " The tools and the material, right? You should be able to look at a wood and feel and tell, is it soft wood? Is it hard wood? Is it cross-grained?\n",
      "\n",
      "\n",
      " How am I going? Right?\n",
      "\n",
      "\n",
      " The strength of the material.\n",
      "\n",
      "\n",
      " And you should know your tools, right? You, for example, right, should, like, you should know what the drill bits are, what each of the components in your library, in your sort of tool set are, not to know it. So for example, using the back of a drill to hammer a nail is a terribly bad idea.\n",
      "\n",
      "\n",
      " I have seen people do that. And silly as it looks, I have seen people do that in programming. Using these libraries, they use something. Yes, you could use it.\n",
      "\n",
      "\n",
      " But really, so don't do that.\n",
      "\n",
      "\n",
      " Know the libraries. And today is about knowing the library.\n",
      "\n",
      "\n",
      " We'll go below the surface of just using a pipeline and getting a little bit deeper.\n",
      "\n",
      "\n",
      " So how do we do that?\n",
      "\n",
      "\n",
      " First of all, there is the datasets. Very good, we can explore the datasets guys.\n",
      "\n",
      "\n",
      " It has already been categorized by the tasks.\n",
      "\n",
      "\n",
      " Do you notice that?\n",
      "\n",
      "\n",
      " Like all of these tasks, they have already been categorized by text classification. You want a dataset on text classification?\n",
      "\n",
      "\n",
      " Here we go.\n",
      "\n",
      "\n",
      " Text classification.\n",
      "\n",
      "\n",
      " You can go and do that, right?\n",
      "\n",
      "\n",
      " And one of the things that I would suggest is, sort it by most downloaded.\n",
      "\n",
      "\n",
      " The Glue and the Super Glue, the IMDB. What in the world is the IMDB? Movie database.\n",
      "\n",
      "\n",
      " That's right.\n",
      "\n",
      "\n",
      " So Rotten Tomatoes is along the same lines. So the highly downloaded ones are a good place to start.\n",
      "\n",
      "\n",
      " Why do they call it tasks? Because these are things that you do, right?\n",
      "\n",
      "\n",
      " Classification is a kind of task.\n",
      "\n",
      "\n",
      " Feature extraction or question answering, entailment, and so on and so forth.\n",
      "\n",
      "\n",
      " These are the kinds of things you can do.\n",
      "\n",
      "\n",
      " The word often used is tasks.\n",
      "\n",
      "\n",
      " So you can pick the data sets. I will just take one particular data set in here.\n",
      "\n",
      "\n",
      " In here, emo, right?\n",
      "\n",
      "\n",
      " And you notice that emotion dataset. When you go to the emotion dataset, do you notice that there is this dataset, Dairy AI Emotion 2.4? Let's go and look into this dataset.\n",
      "\n",
      "\n",
      " What does it look?\n",
      "\n",
      "\n",
      " So you preview the data, guys, so that you know what you're working with. Good thing with Hugging Face is instantly you get to preview the data.\n",
      "\n",
      "\n",
      " You see this string. You have a text and you have a label. And the label has meaning. Where is the meaning coming from?\n",
      "\n",
      "\n",
      " It is coming from the features, the description of the features.\n",
      "\n",
      "\n",
      " So now notice that there is a data set card.\n",
      "\n",
      "\n",
      " It's one of the nice things, conventions followed in open source, that you cannot upload a data till you give some descriptions to it properly.\n",
      "\n",
      "\n",
      " That's a practice.\n",
      "\n",
      "\n",
      " So when you look at this, you will see that it has a lot of things associated with it there's a paper you can go read the paper\n",
      "\n",
      "\n",
      " right uh and so on and so forth\n",
      "\n",
      "\n",
      " but i would get too much into it labels and they explain the labels do you see what does zero does zero mean?\n",
      "\n",
      "\n",
      " Joy, one, love, two, anger, three, three, four, surprise, surprises, this thing. So remember guys, zero, one, two, three, four are not degrees. They are not numbers. They shouldn't be treated as numbers. They should be treated as categorical, right? Even though they are written as numbers, one minute, even though they are written as numbers, you should treat them as categorical, right? Even though they are written as numbers, one minute, even though they are written as numbers, you should treat them as categorical.\n",
      "\n",
      "\n",
      " And this is very common.\n",
      "\n",
      "\n",
      " People in databases, they store things as numbers.\n",
      "\n",
      "\n",
      " Why? Because it takes less space.\n",
      "\n",
      "\n",
      " But then they have a dictionary to translate from the number back to a label.\n",
      "\n",
      "\n",
      " Remember, categoricals must be treated as categoricals.\n",
      "\n",
      "\n",
      " If you treat them as numbers, all hell breaks loose.\n",
      "\n",
      "\n",
      " Albert?\n",
      "\n",
      "\n",
      " What is the corpus here and what is the short sentence? So the entire data set is your corpus and each document is one text and text is the document. Those are your documents. Each document is some text string. So those are your documents. Each document is essentially one text.\n",
      "\n",
      "\n",
      " So that's the short sentence ..\n",
      "\n",
      "\n",
      " Yeah.\n",
      "\n",
      "\n",
      " And then the labels is what any good classifier should. So it is very evident that it has to be used as a classifier, in particular classifying into emotional states. So it's a sentiment analyzer. Any kind of sentiment analyzer should be using this because it's labeled data.\n",
      "\n",
      "\n",
      " Right? This is it.\n",
      "\n",
      "\n",
      " Use this data set, you can do that edit data, or train in auto train. So one of the nice things is, you can click on train with auto, right, so you'll have to tune in.\n",
      "\n",
      "\n",
      " so i'll come to other aspects of it if you have a hugging face api like a account and i advise you to have a hugging face account it's very useful right if you are in this world you know you have to know where the meeting place is right so get familiar with it\n",
      "\n",
      "\n",
      " and so i won't at this moment go into it.\n",
      "\n",
      "\n",
      " There's paper with code. When you go to paper with code, you will see that there's an emotional, there's a, and people have written some things to it.\n",
      "\n",
      "\n",
      " I won't go into that, the homepage.\n",
      "\n",
      "\n",
      " There'll be a paper.\n",
      "\n",
      "\n",
      " It's always good to read the paper from which this data, data set originally came. Like I said, know your data and what other people have done. Like people create data because they're doing some research. You want to go and read that research paper just to get the reason, even though it's historic, go read it.\n",
      "\n",
      "\n",
      " We are going to deal with this particular dataset.\n",
      "\n",
      "\n",
      " Let's go back now and where were we?\n",
      "\n",
      "\n",
      " In the paper with the data set with the model that we might have published or something? No, they give you just the dataset and we'll come to the models in a moment.\n",
      "\n",
      "\n",
      " Where was I?\n",
      "\n",
      "\n",
      " Transfer learning we did.\n",
      "\n",
      "\n",
      " Oh, sorry. Give me a moment.\n",
      "\n",
      "\n",
      " Too many notebooks open.\n",
      "\n",
      "\n",
      " NLP, Visual, QA, transfer learning, data sets, and pre-trained transfer learning.\n",
      "\n",
      "\n",
      " I'll reopen it.\n",
      "\n",
      "\n",
      " So what we will do today is we will start with the data sets, not books.\n",
      "\n",
      "\n",
      " It's a worthy thing to do.\n",
      "\n",
      "\n",
      " Today, actually, I'll extend, we'll start a lunch a little bit later because we have been doing a lot of talking, which is good, but we need to cover some territory.\n",
      "\n",
      "\n",
      " So look at the simplicity of using the data sets. Make sure that the data sets is installed, pip install data sets if you have not installed it.\n",
      "\n",
      "\n",
      " It's part of Hugging Faces.\n",
      "\n",
      "\n",
      " And the first function we'll use is list data sets. It will tell us how many data sets are where on our local machine or in hugging face in the hub.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So when you do that, as of today morning, there are 27,985 data sets. So anyone of you who are asking this question, I don't know where can I get data?\n",
      "\n",
      "\n",
      " You probably have an answer. So you have an answer, right?\n",
      "\n",
      "\n",
      " So this one is going to a hugging phase and then finding the number. Yes, it's finding the number.\n",
      "\n",
      "\n",
      " Isn't it lovely?\n",
      "\n",
      "\n",
      " And for you, it's just one line code. Now, let's say that number.\n",
      "\n",
      "\n",
      " Isn't it lovely?\n",
      "\n",
      "\n",
      " And for you, it's just one line code.\n",
      "\n",
      "\n",
      " Now, let's say that you want to load a data.\n",
      "\n",
      "\n",
      " Look at the sheer simplicity of it.\n",
      "\n",
      "\n",
      " Give it the name. What was the name of the data?\n",
      "\n",
      "\n",
      " Emotion. Right? You load it\n",
      "\n",
      "\n",
      " and it goes and gets the emotion data set. Do you see that it gets the emotion dataset? Load emotion.\n",
      "\n",
      "\n",
      " This is the calling the API then?\n",
      "\n",
      "\n",
      " Yeah.\n",
      "\n",
      "\n",
      " And it's getting the data.\n",
      "\n",
      "\n",
      " So the name emotion will be used in the website.\n",
      "\n",
      "\n",
      " Yeah, it's there in the dataset. So then you look at this dataset dictionary, which... And so some datasets are given like pretty pretty much they won't let you tamper with it it's there then you have the data set like if you just say down do you see it's downloading right\n",
      "\n",
      "\n",
      " and by the way i don't think i ran it on this machine so might as well run it see how quickly it should run through, because this is a new machine.\n",
      "\n",
      "\n",
      " So you and I will have the same experience.\n",
      "\n",
      "\n",
      " This is it.\n",
      "\n",
      "\n",
      " And let's see, maybe the number has changed since morning. Oh, number has actually changed. It's now 28,000.\n",
      "\n",
      "\n",
      " Do you see how fast evolving Hugging Face is?\n",
      "\n",
      "\n",
      " I literally ran it yesterday night.\n",
      "\n",
      "\n",
      " It was 27,000 something. Now it's 28,000.\n",
      "\n",
      "\n",
      " Let's download it. And of course, it downloaded it very quickly. It's a short data set.\n",
      "\n",
      "\n",
      " I hope instantly downloaded.\n",
      "\n",
      "\n",
      " You can run training.\n",
      "\n",
      "\n",
      " Now, when you look at this data set and you just say emotions, it will tell you that it is a dictionary containing three subsets of data. It internally contains three data sets. So a data set can have nested data sets inside.\n",
      "\n",
      "\n",
      " Why is that a good idea?\n",
      "\n",
      "\n",
      " Basic machine learning was the first thing you do.\n",
      "\n",
      "\n",
      " For any supervised learning, you want a split into train validation and testing\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so this is it it already gives you the split how easy it is you don't even have to call the splits right and then let's take the training data which is 16 000 rows let's look at one row of the data now when you get the training data like this, you see that you're just looking into the dictionary and getting the training data.\n",
      "\n",
      "\n",
      " And after that, you can treat it like a list.\n",
      "\n",
      "\n",
      " When you say the first row of the data, it is basically a tiny little map.\n",
      "\n",
      "\n",
      " Label is equal to, I didn't feel humiliated, label is zero.\n",
      "\n",
      "\n",
      " And so you can look at more rows.\n",
      "\n",
      "\n",
      " You can ask, what is the zero what is this so how would you know the answer you can ask the data set to declare its features it's telling you so it's saying ah these are the names sadness joy love anger fear surprise\n",
      "\n",
      "\n",
      " so zero corresponds to which is in this list. What is the zeroth index?\n",
      "\n",
      "\n",
      " Sadness.\n",
      "\n",
      "\n",
      " So it is saying. Then you move forward.\n",
      "\n",
      "\n",
      " And by the way, it also comes the class label has a method, it says ID to a string. So it will convert a number into a string and you can go backwards. You can say convert a string to ID so you can give it sadness and it will convert it to ID.\n",
      "\n",
      "\n",
      " We'll see that in a moment. You look at these results. Here it is.\n",
      "\n",
      "\n",
      " Now you may say, you know what, let's add that textual label because remembering numbers is hard. Let's go and augment this data frame with this.\n",
      "\n",
      "\n",
      " Now, how do you do that?\n",
      "\n",
      "\n",
      " Most of us know how to augment using pandas. Pandas is easy, right? So very easy conversion to pandas. You just take that dataset and you say set format pandas.\n",
      "\n",
      "\n",
      " Right? Could it be easier? Like, well, it could have been easier.\n",
      "\n",
      "\n",
      " You could have said two pandas. But this is it now once you get a pandas data frame then it's easy you just look at it here we go right and now you say okay let me add the emotion so when you add the emotion you have to be a little bit careful because what do you do for you first get the labels label will be will be a column of numbers, right?\n",
      "\n",
      "\n",
      " 0, 1, 0, 3, whatever it is, that particular column. And then to that column, you apply a function.\n",
      "\n",
      "\n",
      " For each of the number, you want to convert the number to a string.\n",
      "\n",
      "\n",
      " You see this?\n",
      "\n",
      "\n",
      " Int to string is a feature. It is something that comes built into the dataset api because it is calling a method of this class right remember training dataset dot features is what it gives you this in the feature is a map if you go to label you basically get an object of this type do you see that right you're directing yourself to this and on this class label there is a method called int to string. And that int to string method will convert it into a string. And then when you do it, here it is.\n",
      "\n",
      "\n",
      " I just produced some random rows.\n",
      "\n",
      "\n",
      " Is this cleaning up the data?\n",
      "\n",
      "\n",
      " Yeah, more intuitive.\n",
      "\n",
      "\n",
      " Just to see what it all is.\n",
      "\n",
      "\n",
      " Because see, numbers are hard to remember.\n",
      "\n",
      "\n",
      " I can keep looking at the dictionary.\n",
      "\n",
      "\n",
      " But when you see it, you can see, let's look at it and see if we agree with it.\n",
      "\n",
      "\n",
      " Could you increase the size just by one control plus or two?\n",
      "\n",
      "\n",
      " Sure. Is it better? Yes, thank you. Okay.\n",
      "\n",
      "\n",
      " So it says, I have made it through a week, I just feel beaten.\n",
      "\n",
      "\n",
      " Now do you notice that it has first thing to observe is that actually these are not even grammatically correct or spelling, spellings are not correct, right?\n",
      "\n",
      "\n",
      " These are very colloquial usages.\n",
      "\n",
      "\n",
      " And yet, I mean, somebody has annotated it with sadness.\n",
      "\n",
      "\n",
      " The question is, how will our transformers do?\n",
      "\n",
      "\n",
      " We'll find out.\n",
      "\n",
      "\n",
      " I feel this strategy is worthwhile.\n",
      "\n",
      "\n",
      " Somebody seems happy.\n",
      "\n",
      "\n",
      " The strategy worked.\n",
      "\n",
      "\n",
      " I feel so worthless and weak.\n",
      "\n",
      "\n",
      " What does he have?\n",
      "\n",
      "\n",
      " Et cetera, et cetera.\n",
      "\n",
      "\n",
      " Again, you notice mistakes here.\n",
      "\n",
      "\n",
      " I feel clever now.\n",
      "\n",
      "\n",
      " So this is annotated data.\n",
      "\n",
      "\n",
      " et cetera, et cetera.\n",
      "\n",
      "\n",
      " Again, you notice mistakes here.\n",
      "\n",
      "\n",
      " I feel clever now.\n",
      "\n",
      "\n",
      " So this is annotated data.\n",
      "\n",
      "\n",
      " The other thing I wanted to point out is you can go from dataset to Pandas, but you can also come back. And when you come back, you don't have to come back right from this, just to show that you can create a completely clean Pandas dataset and convert data frame and convert it to a data set i've created an example basically a silly example i took a x1 feature which is what a thousand points along the x-axis from zero to one\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " but then i created the another feature x2 which is the sign of X. I created another feature X3, which is log of one plus X1, right? And so I created a data frame, right? You would agree that this data frame pretty much, this is the description of the data frame.\n",
      "\n",
      "\n",
      " These are all standard Pandas operations. Would you all agree that this is how you would do it in Pandas?\n",
      "\n",
      "\n",
      " This is no brainer.\n",
      "\n",
      "\n",
      " Now, how do I convert this into Hugging Phrase data?\n",
      "\n",
      "\n",
      " You don't have to export it to CSV and read back the CSV, which you can do by the way, but I leave that as an exercise for you guys to explore.\n",
      "\n",
      "\n",
      " You can just say a dataset from pandas.\n",
      "\n",
      "\n",
      " This is it and you get this lovely isn't it\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so this is so now i've just taught you the basics of the data set there's a little bit more guys\n",
      "\n",
      "\n",
      " huh so please take this as a homework in the lunch break or try to explore go to the website and go to the, where are we?\n",
      "\n",
      "\n",
      " Data frame, this hugging phase.\n",
      "\n",
      "\n",
      " Now, how do I explore the API?\n",
      "\n",
      "\n",
      " Let's go to the documentation. We go to the documentation.\n",
      "\n",
      "\n",
      " Do you see these datasets guys on my screen?\n",
      "\n",
      "\n",
      " Go into that and it is really worth going through the tutorial guys. Go through the tutorials and how to guides. Believe me, you need to know your tools.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So this is I suppose it's one o'clock, it's time for us to take a segue, take a lunch break. But why don't we give this as a exercise guys do that, Guys, I'll just end with a joke.\n",
      "\n",
      "\n",
      " In Silicon Valley is where a lot of millionaires are made out of geeks.\n",
      "\n",
      "\n",
      " And one of the things geeks like to do quite often is buy cameras to do photography.\n",
      "\n",
      "\n",
      " You can always spot one. If you're into photography, you can always spot one. Because if you go to one of the beaches, right, let's say Stinson Beach or Half Moon Bay or something, you can spot one of these geeks because they'll be carrying a really big camera, right, DSLR, right, mirrorless one with a long lens. And their family will be getting it will have a most annoyed look on their face and in broad daylight they'll ask them to pose on the beach while they're taking a picture right have you seen this any one of you has seen this sort of experience or been guilty of it more of it right now what are some of the things wrong with it and if you ever go close to them and observe what's the camera setting it is set on auto right at auto a high-grade camera actually takes worse pictures than an iphone or a smartphone samsung or something actually takes much worse pictures it is not meant to be used in auto except in emergencies.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So that is not knowing your tool. You have all the expensive equipment, but you don't know your tool.\n",
      "\n",
      "\n",
      " Many things wrong.\n",
      "\n",
      "\n",
      " You don't never photograph people in broad daylight.\n",
      "\n",
      "\n",
      " Why?\n",
      "\n",
      "\n",
      " They don't look their best. They don't look their best they don't look because you have stark shadows sun is up there and it is casting dark shadows on the face it'll make people look the worst possible you look you take pictures in cloudy days or during sunrise or sunset when the light is coming the golden light is coming at you right anyway so those are the things but this is it guys it just as we geeks make a fool of ourselves the first time we buy a fancy camera and lots of lenses and everybody knows all the photographers know in the same way if you don't know your tools well hugging face tools well believe me in the nl community, everybody will know.\n",
      "\n",
      "\n",
      " Right? So don't be that.\n",
      "\n",
      "\n",
      " So today, learn, let's start. We'll give this week to really learning the tools well. Guys, we could have put many tools.\n",
      "\n",
      "\n",
      " Hugging face is the dominant one. You have to pick one tool set properly.\n",
      "\n",
      "\n",
      " Like if I were to talk about cameras, it's a distinction between am I in the Canon club or am I in the Nikon club or am I in the Sony club?\n",
      "\n",
      "\n",
      " It doesn't matter.\n",
      "\n",
      "\n",
      " The ideas are the same.\n",
      "\n",
      "\n",
      " You need to pick one and know that tool set well, right?\n",
      "\n",
      "\n",
      " Because the foundational ideas, the F stops, the aperture, the shutter speed, the depth of field, right?\n",
      "\n",
      "\n",
      " The color, this thing, balance, all those concepts remain invariant. All of these tools just do it slightly differently. In the menu items, it will be shown differently.\n",
      "\n",
      "\n",
      " But ultimately, the foundational concepts are the same and you need to now bridge between your concepts and how these tools implemented.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " And that you have to do, you really have to do.\n",
      "\n",
      "\n",
      " Guys do that.\n",
      "\n",
      "\n",
      " I cannot, there is no way I can overemphasize this thing that you need to know your data well, but you also need to absolutely have details of the API of these tools.\n",
      "\n",
      "\n",
      " One API, pick one. You should not only know some at the basic level of these tools, one API, pick one, you should not only know some at the basic level of Quickstart, you should know the APIs, the signature of the methods, almost reflexively from memory, you should be able to quote. You may not be able to quote all the arguments to a function, but at least the dominant ones that you use, you should be able to remember them.\n",
      "\n",
      "\n",
      " Are we together?\n",
      "\n",
      "\n",
      " And with that comes power.\n",
      "\n",
      "\n",
      " So take that piece as a lesson and make yourself go through all of these. Go through the tutorials, how-to guides, conceptual guides.\n",
      "\n",
      "\n",
      " And finally, the reference.\n",
      "\n",
      "\n",
      " When you go to the reference, let's say, how did I know that the class label has that method into string and string to int?\n",
      "\n",
      "\n",
      " Why?\n",
      "\n",
      "\n",
      " Because I can go to it somewhere in here is a class label. Class.\n",
      "\n",
      "\n",
      " Why am I not able to find it?\n",
      "\n",
      "\n",
      " Control F. Yeah.\n",
      "\n",
      "\n",
      " So we see there's a class label here.\n",
      "\n",
      "\n",
      " Right, expand, where am I? Dataset, oh, somewhere. Okay, so you keep searching for that\n",
      "\n",
      "\n",
      " and And some sooner or later, you'll hit upon it.\n",
      "\n",
      "\n",
      " Builder classes, loading classes.\n",
      "\n",
      "\n",
      " Okay. Yes, here we go.\n",
      "\n",
      "\n",
      " As a class label, right.\n",
      "\n",
      "\n",
      " So I can go into this.\n",
      "\n",
      "\n",
      " It sort of jumped somewhere. I don't know where it jumped, but it was supposed to jump to class labels.\n",
      "\n",
      "\n",
      " Hang on.\n",
      "\n",
      "\n",
      " Why did I not get there?\n",
      "\n",
      "\n",
      " Class labels.\n",
      "\n",
      "\n",
      " Okay. For some reason. Anyway, suffice it to say, I don't know why it is jumping around here.\n",
      "\n",
      "\n",
      " Again, I'll go.\n",
      "\n",
      "\n",
      " Okay, after the lunch break, you'll provide the Jupiter notebook.\n",
      "\n",
      "\n",
      " Yes, all of them. In fact, right now, I'll give some right now.\n",
      "\n",
      "\n",
      " So you can play with it.\n",
      "\n",
      "\n",
      " Okay, okay, class class label.\n",
      "\n",
      "\n",
      " Here we go. Class label, here we go. Now with some luck, class label data properly converted to tenses here, here is this class label, here we go. Now with some luck, class label data properly converted to tensors.\n",
      "\n",
      "\n",
      " Here is this.\n",
      "\n",
      "\n",
      " Class label has names, but let's drill into class labels and hopefully it will take us somewhere.\n",
      "\n",
      "\n",
      " Here we go. It took us to class. Yeah, here we go.\n",
      "\n",
      "\n",
      " The class class label.\n",
      "\n",
      "\n",
      " You see that it contains names. It contains, there are three ways now it will give you\n",
      "\n",
      "\n",
      " sorry\n",
      "\n",
      "\n",
      " it's running a bit slow ah parameters cast into yeah here we go do you see into string right and string to end\n",
      "\n",
      "\n",
      " so how do you remember it after a little while it becomes familiar to you, right? You know, you keep going through that.\n",
      "\n",
      "\n",
      " The other thing that I would suggest is there is a very good book.\n",
      "\n",
      "\n",
      " It's literally called Natural Language Processing with Transformers.\n",
      "\n",
      "\n",
      " It is a good textbook to have as a reference to this course. It's an O'Reilly book, Natural Language Processing with Transformers. Can you paste the link to the book please?\n",
      "\n",
      "\n",
      " Absolutely, let me do that right now. Yeah, yeah, it used to be.\n",
      "\n",
      "\n",
      " I'll bring it over. You guys can look at it.\n",
      "\n",
      "\n",
      " It's on my table in my office.\n",
      "\n",
      "\n",
      " It's a very good book actually. I like it.\n",
      "\n",
      "\n",
      " It is completely Hugging Faces based, mostly natural language processing.\n",
      "\n",
      "\n",
      " Yeah, by the second one. The second one came just two, three months after the first.\n",
      "\n",
      "\n",
      " The distinction is colors. It's colored pages.\n",
      "\n",
      "\n",
      " With transformers, yeah. with Transformers.\n",
      "\n",
      "\n",
      " Yeah.\n",
      "\n",
      "\n",
      " So you can see that it came out in August and I bought it in August.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So usually what happens is that through the community, you know, through the community channels, you always know which books are coming out and when.\n",
      "\n",
      "\n",
      " You end up pre-ordering them.\n",
      "\n",
      "\n",
      " So it's a very good book.\n",
      "\n",
      "\n",
      " I would highly encourage you to do that.\n",
      "\n",
      "\n",
      " It's Hugging Face, almost completely Hugging Face. See when you think of transformers, open source Hugging Face is basically basically. The word hugging face is synonymous with transformers. That is why the library itself is called transformers. So you say pip install transformers for hugging face.\n",
      "\n",
      "\n",
      " So guys, this is it.\n",
      "\n",
      "\n",
      " Let's break for lunch.\n",
      "\n",
      "\n",
      " And where was I?\n",
      "\n",
      "\n",
      " Today's class work is actually go through all the tutorials like tutorials how-to guides etc do it thoroughly because if you don't do it now you're all busy remember your life has your boss your workplace\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " and then your boss is at home right your children your family your parents they all will be there and then you have to sleep and then you have to socialize and watch tv netflix\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so any studies you don't do here the probability that you will get as much time at home is relatively low.\n",
      "\n",
      "\n",
      " So do it here. Okay.\n",
      "\n",
      "\n",
      " So guys, we break for lunch and we meet at 3.17. 3.17.\n",
      "\n",
      "\n",
      " Okay, got it.\n",
      "\n",
      "\n",
      " Let's pause the recording.\n",
      "\n",
      "\n",
      " So welcome back from lunch, folks.\n",
      "\n",
      "\n",
      " We were making our way through the concept of transfer learning. And I think there's a repetition transfer learning and pre trained models are the same file, but we can verify.\n",
      "\n",
      "\n",
      " I have put all the files on the slack channel.\n",
      "\n",
      "\n",
      " So please see that it is there.\n",
      "\n",
      "\n",
      " Oh yes, the, you can ignore one of the files because lesson 08 has been duplicated with two names. So you can ignore it. So we'll move to lesson number nine.\n",
      "\n",
      "\n",
      " And if you look at this, one of the one of the easiest things you can do in hugging faces is obviously the easiest thing you can do is use the pipeline API, isn't it? A pipeline to do this, boom, it's done. Give it input, output comes out.\n",
      "\n",
      "\n",
      " It can't get simpler than that. But the next simplest thing that you can do as you start peeling the onion is to use one of the so-called auto classes. Hugging face gives you a very simple framework.\n",
      "\n",
      "\n",
      " And these auto classes are best represented by this syntax. They will be all auto some class or some task from pre-trained.\n",
      "\n",
      "\n",
      " So let's break it down.\n",
      "\n",
      "\n",
      " When you say from pre-trained you're loading it from a checkpoint isn't it? So you give it the name of a checkpoint and it will load a transformer model.\n",
      "\n",
      "\n",
      " It will first look locally if you happen to have a local copy. If you don't happen to have a local cache of it, then it will download it from the Hugging Face hub. So implicit to this is you can't be running this code if you're flying at 30,000 feet in the air because you might not have the internet connectivity.\n",
      "\n",
      "\n",
      " So then in that case you hope that you have locally cached it. So be aware of that, that it reaches out to the hub to get it.\n",
      "\n",
      "\n",
      " Now we'll take a simple example. We have been dealing with tokenizers and we'll deal with the tokenizers in a moment. Did I cover tokenizers before lunch?\n",
      "\n",
      "\n",
      " I have not yet. A little bit. Okay, we're going to do it a little bit more now.\n",
      "\n",
      "\n",
      " So one example is auto tokenizer. Right, so do you notice that the syntax is auto something from pre-trained? So if you look at this syntax, the syntax is auto.\n",
      "\n",
      "\n",
      " Now the name of the class is really like that.\n",
      "\n",
      "\n",
      " Auto tokenizer from a checkpoint, a BERT based uncaged is the checkpoint, right?\n",
      "\n",
      "\n",
      " And are we looking at this code folks? Is this making sense?\n",
      "\n",
      "\n",
      " So what it does is you get a tokenizer. When you pass in a text to it, what do you expect? You expect tokens to come out.\n",
      "\n",
      "\n",
      " And it is as simple as that.\n",
      "\n",
      "\n",
      " If you look at the element number six or line number six or input six, you notice that we send in a text, which was, and I deliberately sent in a text that is the tokenizer does tokenization it does this to have fun with tokens\n",
      "\n",
      "\n",
      " so there's a lot of redundancy here why did I give a word like that remember we talked about the word pieces and so forth so it's a hint that there is more going on in this sentence\n",
      "\n",
      "\n",
      " but but we'll see that later. At this moment, we'll just look at the auto classes. You print the tokens and here we go. You get the input mass.\n",
      "\n",
      "\n",
      " Now, what are these numbers? Input IDs are what? These are indexes into the vocabulary that this tokenizer is using, right?\n",
      "\n",
      "\n",
      " And that is what it is. Now, what is token ID?\n",
      "\n",
      "\n",
      " And we'll come to this. Forget about the output here.\n",
      "\n",
      "\n",
      " We'll talk about tokenizers in depth.\n",
      "\n",
      "\n",
      " Likewise, if you want to do image processing, now you may say, oh, good grief. If you have done image processing and use the PIL and libraries and many things, you know that there are many, many lines of code you write to do just basic image processing and vectorizing the image. But how simple can it be when all you have to do is say auto image processor and auto image processor from pre-trained and you give it a proper transformer model into which you will feed in. Like what is an image transformer? VITs are obviously one of the classic image transformers, right, the original papers.\n",
      "\n",
      "\n",
      " You can use it for determining whether some image is a cat or a dog.\n",
      "\n",
      "\n",
      " But before you do it, you need to create it into an embedding vector that goes into the transformer, isn't it? That's what we talked in the morning, that they take input as an embedding vector.\n",
      "\n",
      "\n",
      " So you need to create the embedding for that. And then the transformer will take it and it will produce what it's supposed to produce. And remember, what does a transformer head in itself produce?\n",
      "\n",
      "\n",
      " It just produces hidden states or embedding latent vectors or embeddings.\n",
      "\n",
      "\n",
      " And I will use this word sort of interchangeably.\n",
      "\n",
      "\n",
      " And this is it.\n",
      "\n",
      "\n",
      " And here I won't go more into and show a full example.\n",
      "\n",
      "\n",
      " But suffice is to say that you see that this code between tokenizing, which is an NLP task and image processing, which is a computer vision task, how absolutely similar the API looks, isn't it?\n",
      "\n",
      "\n",
      " Very easy, guys, very, very easy.\n",
      "\n",
      "\n",
      " And likewise, you go to the next thing, audio processing. You have sound files, MP3s, you want to process them.\n",
      "\n",
      "\n",
      " What do you need to do?\n",
      "\n",
      "\n",
      " Well, Auto Feature Extractor, which extracts features from audio, is this.\n",
      "\n",
      "\n",
      " You can just use a proper transformer.\n",
      "\n",
      "\n",
      " This one does speech emotion recognition.\n",
      "\n",
      "\n",
      " And then lastly, you could go further.\n",
      "\n",
      "\n",
      " We live in the world of multimodal learning now. you can have, we live in the world of multimodal learning now.\n",
      "\n",
      "\n",
      " So when you feed in data, nothing says that the data cannot come one part from image and another part from text, or one part from image and another part from sound, or you can mix and match and do all sorts of things, right?\n",
      "\n",
      "\n",
      " And so, and by the way, multimodal is very big in the last two sessions of this workshop. We are going to focus on a lot of multimodal learning.\n",
      "\n",
      "\n",
      " If you think about it, all of these DALY, stable diffusion, etc., etc. These are all instances of multimodal learning, but it goes far beyond that.\n",
      "\n",
      "\n",
      " You can do very complicated things when you give input of text and sound and so on and so forth. You can do very complicated things when you give input of text and sound and so on and so forth. And we will see, and you can do the opposite, you know, sound synthesis and or transcription, which will do transcriptions for you.\n",
      "\n",
      "\n",
      " So we'll do a lot of multimodal stuff.\n",
      "\n",
      "\n",
      " They're part of natural language processing.\n",
      "\n",
      "\n",
      " But this is a this is a good first start.\n",
      "\n",
      "\n",
      " You take a LMV model, which is very well known in this space, and you can load it.\n",
      "\n",
      "\n",
      " It will do two things.\n",
      "\n",
      "\n",
      " It is ready to do input processing of images as well as tokenization of text.\n",
      "\n",
      "\n",
      " So think about it, guys.\n",
      "\n",
      "\n",
      " In one line, you have put two domains which used to be separate in AI, computer vision and text processing. And you can see how close they have come with this multimodal learning that you can now, there are checkpoints and models that you can load with just, and processes that you can load with just a line, isn't it? Now you can say, all right, that is for the processors or the, preparing the data to go into the model what about models themselves that that too is very easy so we will take an example suppose you take a checkpoint distilled bird case a distilled bird base uncased now what was distilled based to vet your recollection distilled based is a trip is sort of a smaller version of bird based is a sort of a smaller version of BERT. And how it learns, how do you train a much smaller model to perform almost as well as the big model?\n",
      "\n",
      "\n",
      " We will learn about this.\n",
      "\n",
      "\n",
      " That's one of the loveliest papers.\n",
      "\n",
      "\n",
      " So what you do is you build a model, you make it do predictions, and then you train a student model, and you treat the big model as the teacher and you try to make the student model not learn from the data but learn from the predictions of the teacher model\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so it learns the way the predict the teacher makes predictions and sort of catches up to it and so it has almost the accuracy of the much bigger model.\n",
      "\n",
      "\n",
      " Distilled Bird is a shining example of that.\n",
      "\n",
      "\n",
      " And it does really give you. How does it do that? Oh, we'll learn about that. We'll learn about that.\n",
      "\n",
      "\n",
      " It's good.\n",
      "\n",
      "\n",
      " The architecture is worth learning about.\n",
      "\n",
      "\n",
      " So it's one of the papers we'll cover.\n",
      "\n",
      "\n",
      " So but just assume it can do that.\n",
      "\n",
      "\n",
      " Patrick?\n",
      "\n",
      "\n",
      " So I said said for these multi-modal models, does it still follow the J, Q, and V of attention where like you break down an image to like multiple pixels?\n",
      "\n",
      "\n",
      " Yeah, so that's right.\n",
      "\n",
      "\n",
      " So what happens is not at the level of pixels.\n",
      "\n",
      "\n",
      " So there is a paper which I can be covered. Actually we will cover a lot of papers you'll see and at some point i'll start covering two three papers a day um it is called what is it 16 by 16 pixels is worth a thousand words or something like that so what they do is see what is a word made up of characters so they're saying that we'll take 16 by 16 pixel and consider it a word or a token you got it right from there the moment you realize you get that insight you realize that the whole transformer architecture can now be applied doesn't it because it works on tokens sequence of tokens so you broke the image chopped it off into tokens so it doesn't break away from the original transformer models that were based off of words.\n",
      "\n",
      "\n",
      " Exactly.\n",
      "\n",
      "\n",
      " Just we converted the signal into what fits into that.\n",
      "\n",
      "\n",
      " So what do transformers need? They need a sequence of tokens. So how do you stare at a picture and say how do I make it into a sequence of tokens? What is a token here?\n",
      "\n",
      "\n",
      " You say okay\n",
      "\n",
      "\n",
      " I'll take 16 by 16 pixels and call it a token, right? Because after all a word is made up of characters this is made up of pixels and that's it so they just go through the whole image and this is interesting\n",
      "\n",
      "\n",
      " yeah that's a basic intuition and now when we do the paper they do it a little bit smartly and we'll come to that good quick oh so for like going back to like tokens for words is it randomized as to like where they'll stop the token\n",
      "\n",
      "\n",
      " no no it's not at all not at all in fact that's a very next one so uh hold your breath and you won't have to hold it for too long so here we go you can take a checkpoint and you can say auto model for what purpose let's say that you want to classify sequences sequences are, texts. You want to classify text. A classic example is sentiment analysis. What sentiment is it, right?\n",
      "\n",
      "\n",
      " Or classifying what the topic is. Is it politics? Is it sports? Is it science? What is it?\n",
      "\n",
      "\n",
      " You can use it for that, right?\n",
      "\n",
      "\n",
      " So things like that. So for classification. So that would be sequence classification. So you can say automotor for sequence classification. Pretty much the way you would intuitively think of the sentence, that's the name of the class.\n",
      "\n",
      "\n",
      " On the other hand, if you are using the same checkpoint, because you can use the same transformer for different purposes. Remember, we said that once you have trained a transformer, it can do multiple things.\n",
      "\n",
      "\n",
      " Suppose you want to use it for just token classification. Suppose you want to use it for just token classification.\n",
      "\n",
      "\n",
      " One example would be which part of speech is it? Is it a noun, a verb, an adjective? What is it? Right?\n",
      "\n",
      "\n",
      " So you could do auto model for token classification.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " From pre-trained.\n",
      "\n",
      "\n",
      " Another way to think about it it is is it a named entity or not named entity resolution so if i say uh felix the cat jumped over the wall right or jumped over the berlin wall well berlin wall is fallen but let's imagine right um felix is the named entity it's a person uh well i suppose cat is not none of the words have to be ignored. And then you come to Berlin and you say location, it's an empty entity.\n",
      "\n",
      "\n",
      " So you can imagine that you can construct all sorts of things.\n",
      "\n",
      "\n",
      " So guys, I hope you realize that, excuse me, that auto classes are about the next simplest thing after the pipeline isn't it very easy so we peel one more layer of the onion and you and once you get familiar with it just lovely okay if you want to see how lovely it is uh try writing this entire code in plain pytorch or if you really want to trouble yourself, write it in TensorFlow.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " And see how many lines of code you write.\n",
      "\n",
      "\n",
      " So...\n",
      "\n",
      "\n",
      " Why did you not say that you didn't know?\n",
      "\n",
      "\n",
      " Oh gosh! Now that would be, that would be like...\n",
      "\n",
      "\n",
      " That's in the bicycle analogy we use, you'll have to take ore, you'll have to refine the ore, smelt it, smelt the iron, and build a bicycle from scratch.\n",
      "\n",
      "\n",
      " Okay, so the next one, we have been talking about tokenizers. Let's look into the tokenizers a little bit more in detail.\n",
      "\n",
      "\n",
      " So guys, are you following me? I want to go fast because this is easy stuff, right?\n",
      "\n",
      "\n",
      " By the way, all of these notebooks are on your Slack now.\n",
      "\n",
      "\n",
      " I hope you have taken them from there.\n",
      "\n",
      "\n",
      " So let's take an example.\n",
      "\n",
      "\n",
      " And we'll go into the tokenizer now in real detail, hands-on detail.\n",
      "\n",
      "\n",
      " So I took the same sentence.\n",
      "\n",
      "\n",
      " The tokenizer does tokenization. It does this to have fun with tokens.\n",
      "\n",
      "\n",
      " You'll see why I deliberately wrote that sentence.\n",
      "\n",
      "\n",
      " So how can we do that?\n",
      "\n",
      "\n",
      " We'll use the auto tokenizer, something the auto one of the auto classes.\n",
      "\n",
      "\n",
      " You give it a checkpoint, bird base uncased and there you are.\n",
      "\n",
      "\n",
      " You see how easy it was except that and by the way this is a trick I use. You know, when you print things out like this, sometimes it's not very, like, at least I find it a little harder to read, especially if there are many, these dictionaries have many keys and values are erased.\n",
      "\n",
      "\n",
      " So what I tend to do is I use this trick. I convert it into a Pandas data frame and then read it.\n",
      "\n",
      "\n",
      " I find it a little easier to, I don't know, and of course your own experience may differ, but I find it a little easier to read that these are the inputs, right?\n",
      "\n",
      "\n",
      " So now what has happened?\n",
      "\n",
      "\n",
      " This sentence, let's see how many words there are.\n",
      "\n",
      "\n",
      " One, two, three, four, full stop is five, six, seven, eight, nine, 10, 11, 12, 13, and another full stop, 14. So you would have expected, including punctuations, 14 tokens. And yet we have, oh good grief, 19 tokens. So guess, where are the extra tokens coming from?\n",
      "\n",
      "\n",
      " No? Punctuation?\n",
      "\n",
      "\n",
      " We included that in the 14.\n",
      "\n",
      "\n",
      " Look at the sentence.\n",
      "\n",
      "\n",
      " Start and end. Very good.\n",
      "\n",
      "\n",
      " You need special token CLS for start.\n",
      "\n",
      "\n",
      " Separator for the end of that.\n",
      "\n",
      "\n",
      " What else? Hint.\n",
      "\n",
      "\n",
      " Look at the text that I sent in. What does that text tell you?\n",
      "\n",
      "\n",
      " So look at the results.\n",
      "\n",
      "\n",
      " Do you see 19 or 19204 repeated?\n",
      "\n",
      "\n",
      " And what does it correspond to?\n",
      "\n",
      "\n",
      " 19204 seems to correspond to token. The word token, isn't it?\n",
      "\n",
      "\n",
      " And so what do you, why has... Why is it token?\n",
      "\n",
      "\n",
      " Yeah, so look at that.\n",
      "\n",
      "\n",
      " The tokenizer.\n",
      "\n",
      "\n",
      " So first is the 101 stands for CLS, as you can guess. The second one stands probably for the, and we'll verify whether it's true.\n",
      "\n",
      "\n",
      " Now use your deductive skills.\n",
      "\n",
      "\n",
      " What will the next one stand for?\n",
      "\n",
      "\n",
      " Because it is repeated, it cannot stand for the word tokenizer because tokenizer word happens only once and yet 19-204 is thrice. Now what is thrice in this sentence?\n",
      "\n",
      "\n",
      " The word piece token or the word segment token, token, token. It is there in tokenizer, it is there in tokenization and it is there in tokens.\n",
      "\n",
      "\n",
      " So that gives you a clue why it is good to use word segments because now you can have a smaller vocabulary.\n",
      "\n",
      "\n",
      " We talked about it.\n",
      "\n",
      "\n",
      " So now let's see if our guess is right.\n",
      "\n",
      "\n",
      " This 19.\n",
      "\n",
      "\n",
      " So I saw the mystery now. Look at it. Does it look right? Right?\n",
      "\n",
      "\n",
      " Yeah, it got broken up into the token.\n",
      "\n",
      "\n",
      " And the the birds convention is whenever it puts two hashes.\n",
      "\n",
      "\n",
      " It says it's a suffix. It's a suffix, right? So isers, it's a suffix. Isation is a suffix. Civilization, prioritization.\n",
      "\n",
      "\n",
      " There are many, many words that can end with this token, isn't it? And ends with S, the plural. Surely one word piece that is ubiquitously used is the plural part, isn't it?\n",
      "\n",
      "\n",
      " That is it.\n",
      "\n",
      "\n",
      " It starts with CLS and CEP.\n",
      "\n",
      "\n",
      " And it turns out that different tokenizers, they may have different sort of these special tokens. One may use this and you'll see that it differs.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So intuitively, without knowing all this, you would have expected this.\n",
      "\n",
      "\n",
      " The tokenizer does tokenization dot.\n",
      "\n",
      "\n",
      " But now you know that there is more going on here.\n",
      "\n",
      "\n",
      " So why is it a good idea?\n",
      "\n",
      "\n",
      " We have talked about it, but I'll just review it.\n",
      "\n",
      "\n",
      " Why is it a good idea for ISER, tokenizer, to take token away from the ISA part and keep ISA as a suffix, because we happen to know that there are many, many words that end with ISA, right? A finalizer, finalizers, plural of it, visualizer, visualizers, equalizer, equalizers, and so on and so forth, right?\n",
      "\n",
      "\n",
      " And this is not an exhaustive list.\n",
      "\n",
      "\n",
      " You can go on thinking more and more and more right\n",
      "\n",
      "\n",
      " so we can do a little experiment let's put all of these i created a list of isers you know i created a string which is just made up of isers all sorts of isers right finalizer finalizers etc and let's pass these things through the whole process and see what comes out how How did it break up the word? Do you notice that final, eiser, final, eiser, and es, the plural. And do you notice that it did not create, like even though it created more tokens, it is from a smaller vocabulary, implicitly from a more condensed vocabulary. And that is the whole point of using word segments rather than words themselves. Because remember, English has 5 million words and you could be in quite a bit of trouble if you try to use 5 million words, right? Now, one of the side benefits of using word segments is, remember I said that for words that is not there in your vocabulary at all, you use a special token, like BERT will use UNK, right?\n",
      "\n",
      "\n",
      " UNK stands for unknown.\n",
      "\n",
      "\n",
      " And all the words that it doesn't find in the vocabulary, it will map to that. But what happens is that it's rather hard to construct a word which won't get chopped off into word pieces that are in the vocabulary.\n",
      "\n",
      "\n",
      " And so the number of mappings to unknown decreases because of that. Now it turns out, so we realize that word segments are the building blocks of the vocabulary.\n",
      "\n",
      "\n",
      " And so what happens?\n",
      "\n",
      "\n",
      " Here is your vocabulary. So imagine a little dictionary, not the Oxford dictionary, not the Webster's dictionary, but the Transformers dictionary, right?\n",
      "\n",
      "\n",
      " And its entries are the word pieces.\n",
      "\n",
      "\n",
      " And God knows how in the world would you give the meaning to ISA, but okay, some meaning is given. But what you do instead is for each of this token, instead of meaning, you give a number, a sequential number. Because if you, for example, if nothing else, alphabetically sort the tokens, right, then you can surely attach an index to it. The array, whatever the index thing is, you could do that. Or you could just hand choose and pick some numbers.\n",
      "\n",
      "\n",
      " People apply some conventions.\n",
      "\n",
      "\n",
      " So they have done that.\n",
      "\n",
      "\n",
      " You associate a number. So there is a mapping between each token and a number. The number uniquely gives a token.\n",
      "\n",
      "\n",
      " So long as you maintain the map somewhere, you have your own private dictionary, the tokenizer's dictionary, right? It is as good as Webster's dictionary, except that instead of the meaning of the word, you have a number for it.\n",
      "\n",
      "\n",
      " And once you have a number, what else can you do?\n",
      "\n",
      "\n",
      " Well, remember that words or tokens are categoricals. As I input their categorical variables, they're not a measure. They're not 13.6.\n",
      "\n",
      "\n",
      " So actually that thing that is just the location, if you think of a large array of zeros, and suppose the token ID, the input ID of the token is, let's take an example, 101. So you go to the 101 index in the array, and only that bit you flip up.\n",
      "\n",
      "\n",
      " You set that flag, you make it one.\n",
      "\n",
      "\n",
      " All others are still zero.\n",
      "\n",
      "\n",
      " This is your one-hot encoding, right? This is called the one-hot encoding.\n",
      "\n",
      "\n",
      " You now get, for each word piece, a vector. What is the dimensionality of the vector?\n",
      "\n",
      "\n",
      " It is the cardinality of the vocabulary. If your vocabulary has 1,000 words, then your one-hot encoding will have 1,000 dimensions.\n",
      "\n",
      "\n",
      " Actually, technically, it can have one less dimension, but you don't do that.\n",
      "\n",
      "\n",
      " When we do NLP, you keep it 1,000.\n",
      "\n",
      "\n",
      " You may say that the one last one is redundant. So, for example, if everything is zero, you can say if none if it is none of the words, it must be the word that I left out. The thousandth word or something like that.\n",
      "\n",
      "\n",
      " But you don't do that.\n",
      "\n",
      "\n",
      " You just make it the same size as the vocabulary.\n",
      "\n",
      "\n",
      " And you set a flag at the at the place that you would like to right\n",
      "\n",
      "\n",
      " and oh you're making notes here nice now it turns out that uh oh by the way if you write on this you have to erase also because otherwise i don't know there to be, you have to figure it out.\n",
      "\n",
      "\n",
      " Okay.\n",
      "\n",
      "\n",
      " So now there are many ways to do word pieces. It turns out that when you do multilingual words, then some of the presumptions of English do not apply.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " For example, in English, I said, you presume that, I don't know, words go in a particular way, right, from left to right, and so on and so forth, you start interpreting.\n",
      "\n",
      "\n",
      " But in lowercase, uppercase, yeah, these are assumptions of that.\n",
      "\n",
      "\n",
      " But if you're writing it in some other languages, you need to be very careful. You need to, for example, if you're using Mandarin, Mandarin, some of these assumptions don't work. I don't think there's an upper lower case in Mandarin, is there Dennis? Dennis Hirschhorn I think the only one that's too, that came from like, you know, the hieroglyphics, sort of like capital.\n",
      "\n",
      "\n",
      " Yeah.\n",
      "\n",
      "\n",
      " Like whatever they actually write in the data.\n",
      "\n",
      "\n",
      " Okay. So it's pictographic, right? A house has a symbol and that symbol is it.\n",
      "\n",
      "\n",
      " There's no upper, lower case.\n",
      "\n",
      "\n",
      " Yeah, that's right. Yeah, nice. So there we go.\n",
      "\n",
      "\n",
      " So what happens is you need a different tokenizer that works better for multilingual models, multilingual text.\n",
      "\n",
      "\n",
      " And such a thing is sentence-piece\n",
      "\n",
      "\n",
      " and we'll see it in a moment.\n",
      "\n",
      "\n",
      " Then GPT family of models, they tend to use yet another kind of tokenizer called the BPE tokenizer.\n",
      "\n",
      "\n",
      " So I will show you the result for one of them, I believe for sentence piece and very easy homework for you is just use the BPE and see what output comes out. I'm sure you can do that, just have to substitute BPE there, tokenizer.\n",
      "\n",
      "\n",
      " So now when you give it the tokenizer, now comes the interesting part. Now comes the interesting part.\n",
      "\n",
      "\n",
      " You can give, suppose tokenizer giving it one data point, data instance or datum at a time, one sentence at a time is rather inefficient because the way these things run, especially on the GPU is they're tensors. You can pass a whole batch and it can do the whole matrix computation, forward computation at one go right so why give it one at a time so one easy way is you can give it a list here we are giving it a list and this is just to illustrate the point that it works just fine with this right so suppose you give it more i gave it two to r is human to divine, to learn is to live.\n",
      "\n",
      "\n",
      " And what does it produce?\n",
      "\n",
      "\n",
      " It produces these tokens, those identifiers. Remember those IDs, those numbers in our dictionary, in our tokens dictionary, the numbers that you are hugging face APIs, they call it the input IDs. IDs would have been fine.\n",
      "\n",
      "\n",
      " Input why? Why input IDs? Because they will go into the model.\n",
      "\n",
      "\n",
      " The presumption is tokenization is a pre-processing step to something else that will come after.\n",
      "\n",
      "\n",
      " So that's why you use the word, the prefix input ID. Then comes token type ID\n",
      "\n",
      "\n",
      " I'll mention in a moment.\n",
      "\n",
      "\n",
      " Look at attention mask.\n",
      "\n",
      "\n",
      " Do you realize that of the two sentences, to live is to learn is a shorter sentence?\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " So if you make the encoding of equal length, so I've said padding is equal to true. So I'm saying just for matrix multiplication, just make them both of the same length.\n",
      "\n",
      "\n",
      " If I do that, what do I do about the fact that to learn is to live? And then what do I do?\n",
      "\n",
      "\n",
      " You pad it up with, like you put this special characters saying, ignore, ignore, ignore, but you also do something. And this is where attention is beginning to kick in because you know that you're likely going to run it through a transformer another transformer you're saying you're giving a hint to the transformer that don't even bother paying attention to these tokens make sure that nothing is paying attention to these tokens so these are attention masks you're hiding it from the attention mechanism so when it is zero you're hiding it when it is zero, you're hiding it. When it is one, you're not hiding it.\n",
      "\n",
      "\n",
      " So that is the mystery that explains the mystery of this column.\n",
      "\n",
      "\n",
      " So when I just give it one sentence, of course, you have to, everything is open to attention, isn't it? That's why this attention mask here, all the values are one.\n",
      "\n",
      "\n",
      " Do you see?\n",
      "\n",
      "\n",
      " Now what about this middle column, token type IDs? What does this signify? This, let me explain.\n",
      "\n",
      "\n",
      " now going back to the BERT architecture that we covered last week what do we give we give a pair of sentences isn't it two segments two sentences and so we need to tell that this word belongs to which sentence the first or the second isn't it and how did we do it that was the segment id we call it the segment embedding of the segment id we call it the segment embedding of the segment id and that is what this token type ids is it tells which of the sentences it belongs to and of course you start counting from zero so the first sentence right if you were to give it as a pair the first sentence of course has the id zero isn't it and that's what it is. That's why it's zero. And that's an explanation for that. Oh, what is the question? Sorry.\n",
      "\n",
      "\n",
      " NER is different for question answering than chat GPT-3 or sentiment analysis.\n",
      "\n",
      "\n",
      " I asked this question as a prompt.\n",
      "\n",
      "\n",
      " As a logic, NER is different to sentiment analysis that was discussed before. NER is different for question answering. Okay, let me explain about what NER is.\n",
      "\n",
      "\n",
      " Sorry, Asif, I was typing too quickly.\n",
      "\n",
      "\n",
      " I was also listening to you.\n",
      "\n",
      "\n",
      " So, we were talking about sentiment and analysis, then you brought in named entity recognition, you brought BERT and distal bird, are they better or worse in terms of the specific, which is named entity recognition? We have sentiment analysis and we also have question answering, which kind of chat GPT does.\n",
      "\n",
      "\n",
      " I also apologize in advance for my grammar. I was typing pretty quickly and apparently we can't edit.\n",
      "\n",
      "\n",
      " Okay. So I'll let you, that was my question. I'll leave it there. So let us unpack it. So we have a lot of things playing in that question.\n",
      "\n",
      "\n",
      " First is chat GPT. So let's keep chat GPT aside for a moment because it's a very large language model and there's more happening than just one transformer there. So let's call chat GPT a composite thing and keep it aside.\n",
      "\n",
      "\n",
      " Now let's come to this question named entity recognition and sentiment analysis. See named entity recognition and the other thing was BERT versus Distilbert versus GPT.\n",
      "\n",
      "\n",
      " So let me answer these three things.\n",
      "\n",
      "\n",
      " See, we say that to do named entity recognition or to do sentiment analysis, these are tasks.\n",
      "\n",
      "\n",
      " These are different tasks.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So when you are using a model for one, you explicit. So let's take anyone sentiment analysis internally.\n",
      "\n",
      "\n",
      " You can imagine the following three things to be happening you are tokenizing text some input text you're running it through a transformer whether bird distill bird whatever and then you're taking some output either the average of all the tokens or all the all the hidden states but generally it's not necessary you just take the output of the CLS token remember the word architecture and you feed it to a classifier and say what is it now you may do sentiment analysis as thumbs up thumbs down you may do it in a predict into a set of states sentimental states joy fear anger so on and so forth\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " So that is an example of sequence classification.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " A good class, that a model that you can use is a model that has been trained on classifying sequence or a text, a sentence.\n",
      "\n",
      "\n",
      " On the other hand, when you come to named entity recognition, things get more interesting.\n",
      "\n",
      "\n",
      " And it's very simple as form.\n",
      "\n",
      "\n",
      " You are asking, is this token a named entity? Named entity means person, place, or organization.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " A person is John, a place is Paris, and organization is World Health Organization.\n",
      "\n",
      "\n",
      " Right? Or some company, right? So Support Vectors is an organization, right?\n",
      "\n",
      "\n",
      " The most famous of them.\n",
      "\n",
      "\n",
      " So then it is a classification.\n",
      "\n",
      "\n",
      " If you really think about it, what are you doing?\n",
      "\n",
      "\n",
      " You're looking at every token and asking, is it a person, place or organization or none of the above?\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " That is one way to pose the question. Or another way to pose the question is, is this token a named entity, like person, place or organization? Or is it a verb, adjective, noun, blah, blah, blah. You can classify it into one of those, right? But in any case, it's at a token level classification, right?\n",
      "\n",
      "\n",
      " So those are two different tasks.\n",
      "\n",
      "\n",
      " Now-\n",
      "\n",
      "\n",
      " Asif, sorry to interrupt.\n",
      "\n",
      "\n",
      " Just before we go into the other thing, would name entity recognition, I know we talked about sequence to sequence in another, and I think a class or two classes before. about sequence to sequence in another\n",
      "\n",
      "\n",
      " and I think a class or two classes before.\n",
      "\n",
      "\n",
      " Do you think named entity recognition would still be valid for other languages like you said Hindi or other languages?\n",
      "\n",
      "\n",
      " Absolutely valid.\n",
      "\n",
      "\n",
      " Like for example, if you say, Krishna Delhi ghum ke aaya. Krishna is a name. Delhi is a place. Delhi is a place.\n",
      "\n",
      "\n",
      " And so forth.\n",
      "\n",
      "\n",
      " So the named entity recognition is global. It's a language agnostic. You just need a named entity recognizer that has been trained on that a corpus of that language so long as it has been trained it does a very good job now recently there is also a movement to see if you could do some sort of a zero shot a few shot learning and whatnot right but let's not get into that but in its simplest form so long as long as you have been trained with the corpus and label data, it will work. In any language, it would work.\n",
      "\n",
      "\n",
      " Next, I apologize, Shalini, I don't know Tamil. Otherwise, I would have given you an example in Tamil.\n",
      "\n",
      "\n",
      " Asif, you don't have to apologize.\n",
      "\n",
      "\n",
      " Your Tamil is probably better than mine. Your Tamil is probably better than my mind.\n",
      "\n",
      "\n",
      " Okay. All right.\n",
      "\n",
      "\n",
      " So the next, so that's a distinction between the two.\n",
      "\n",
      "\n",
      " Now comes the question of which model to use. See, there are two aspects to a model.\n",
      "\n",
      "\n",
      " Given the same dataset, what is the accuracy of that? What is the performance of that model? Pick a performance measure.\n",
      "\n",
      "\n",
      " For simplicity, I'll use accuracy though. I hesitate using the word accuracy because people get fixated on accuracy whereas in most real life practical situations it is either the precision or the recall or something else that is more valuable.\n",
      "\n",
      "\n",
      " To illustrate the point, if you have a screening test that checks whether somebody has, name your cancer, prostate cancer, breast cancer, these are two scary things for men and women. So as a screening test, what do you want to do?\n",
      "\n",
      "\n",
      " You know that if it comes out, even if there's a slighter suspicion of cancer, you would rather say yes, a positive indication.\n",
      "\n",
      "\n",
      " Because why?\n",
      "\n",
      "\n",
      " Because if this person doesn't have cancer, the subsequent test will eliminate it.\n",
      "\n",
      "\n",
      " But the last thing you want to do, so the price of a false positive is you'll scare this guy for a day. He'll go home and write some emotional obituaries for himself for something like that right and two weeks later another blood test will show he's totally fine on the other hand if you don't do that if you are false negative the price is catastrophic because a lot of these cancers they're very aggressive and time is of the essence what stage of cancer it has and whether cancer has metastasized determines the survival of this person the way existence so the cost is very high if you miss it the cost is relatively low if you have a false positive right\n",
      "\n",
      "\n",
      " There'll be a little bit of a drama, but that's about it, right?\n",
      "\n",
      "\n",
      " So what do you want? Do you want to have high recall? You want to make sure that you don't mess a person with, at the cost of precision, right? You, a lot of people, you declare you have cancer, right?\n",
      "\n",
      "\n",
      " They go to the subsequent test, half of them come out fine.\n",
      "\n",
      "\n",
      " So remember, which measure you use should be determined by accuracy.\n",
      "\n",
      "\n",
      " And this is very odd, actually.\n",
      "\n",
      "\n",
      " Even with well-known, studied data sets, like the breast cancer data set, which is probably the hello world of classifiers. If you go and look on the wild, most of the Jupiter notebooks and articles that have been written, they will say, hey, here is my classifier to look at breast cancer.\n",
      "\n",
      "\n",
      " And by the way, see how accurate it is. Accuracy is meaningless.\n",
      "\n",
      "\n",
      " Do you realize that?\n",
      "\n",
      "\n",
      " The one measure you should look for is recall in a screening test. On the other hand, if your test is the one that comes right before the surgeon is standing with his operating room door open and his big knife ready, he is looking at you. At that moment, you want to be absolutely sure that this person does have a tumor that is going to cut open and take out. So at that moment, that particular test needs to have what?\n",
      "\n",
      "\n",
      " A high precision.\n",
      "\n",
      "\n",
      " You need to know that if it says you have cancer, you have cancer.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " You're not sending in false positives.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So situation determines which metric you should use.\n",
      "\n",
      "\n",
      " Remember that. So don't get fixated.\n",
      "\n",
      "\n",
      " So with that story aside now, let us take a measure, some metric. I will use accuracy, but not, I don't mean it, some measure.\n",
      "\n",
      "\n",
      " We, so this question comes now to the second part.\n",
      "\n",
      "\n",
      " Given the same data set for a particular task, take a task.\n",
      "\n",
      "\n",
      " Let's say that you take a task sentiment analysis or classification into subjects, politics versus science versus sports for this text or whatever, pick your classification task.\n",
      "\n",
      "\n",
      " Now you look at the measure that you care about.\n",
      "\n",
      "\n",
      " Actually, because I tend to warn people against accuracy, I'll take F1 score.\n",
      "\n",
      "\n",
      " Let's see.\n",
      "\n",
      "\n",
      " The composite of precision.\n",
      "\n",
      "\n",
      " So I have heard, so I know F1 score people, how do I say this nicely? In industry, people understand F1 score than they understand in academia.\n",
      "\n",
      "\n",
      " So can you kind of clarify what your stance is? So just so I can recalibrate what you mean by F1 score. When you do F1 score, so first is F1 score versus accuracy. Why use F1 score? F1 score is, in my view, a better measure when there is no dire reason to use either precision or recoil, right? Because F1 is the harmonic mean of the two. So you have arithmetic mean, you have geometric mean. F1 is simply the harmonic mean of the two.\n",
      "\n",
      "\n",
      " Are we together?\n",
      "\n",
      "\n",
      " So what it means is that if either, see, remember, as a harmonic mean, it has a problem. If either the precision is too low or the recall is too low, it will adversely affect the harmonic mean.\n",
      "\n",
      "\n",
      " Isn't it?\n",
      "\n",
      "\n",
      " So think about it.\n",
      "\n",
      "\n",
      " Let's say the precision is 0.1 but recall is 0.9. 1 over 0.1 is 10, right? And 1 over 0.9 is well 10 over 9.\n",
      "\n",
      "\n",
      " Let's just call it, what should we call it? 10 over 9 is 1.1, let's say, should we call it 10 over 9 is 1.1 let's say right so you are seeing a 10 plus 1.1 right is 11 and so 1 11th what was the score so f1 becomes 1 11 what was the score leaning towards towards the worst number right isn't it So F1 score has one benefit. It sort of overweighs the worst quantity, right? Whichever of the two is worse. It sort of says that this is the, like it sort of gives emphasis to the worst of the two.\n",
      "\n",
      "\n",
      " And therefore it's a useful measure.\n",
      "\n",
      "\n",
      " Accuracy is not, I don't prefer accuracy whenever there is a class imbalance.\n",
      "\n",
      "\n",
      " And let me make this obvious from the same example. See, actually, perhaps I'll tell a different story.\n",
      "\n",
      "\n",
      " You know, before modern medicine, to take on the task, let's say I take a task named anterior cognition.\n",
      "\n",
      "\n",
      " Then you ask this question, condition on this task, which model is better? Then comes, but that question also is conditioned because what corpus of data did you train it with? So suppose you train one, let's say you take Bert, and you train Bert with a smaller corpus, and you train Bert with a larger corpus, right? You train Bert with a corpus of just 1,000 documents.\n",
      "\n",
      "\n",
      " And let's say that the task is sentiment analysis.\n",
      "\n",
      "\n",
      " And you take Bert, and you train it with 100 million, or I don't't know 20,000 or 30,000 data points which do you expect to perform better obviously that particular bird the same architecture the same thing that has been trained on more data isn't it so we are we have to condition on two factors which is the task which is the corpus they were trained on. Now, assuming that they were trained on both, both had the same task, that's a sentiment analysis, and they were trained on the same large corpus.\n",
      "\n",
      "\n",
      " Then the question is, which is better?\n",
      "\n",
      "\n",
      " I mean, BERT, distal BERT, or let's say GPT-2 or something like that.\n",
      "\n",
      "\n",
      " So there, the answer to that is, generally, distilled bird, by its very nature, has learned from bird and approximates its performance. So it will always underperform bird, right?\n",
      "\n",
      "\n",
      " So Asif, I just want to ask.\n",
      "\n",
      "\n",
      " Just hold your thought.\n",
      "\n",
      "\n",
      " One second.\n",
      "\n",
      "\n",
      " Let me give it because a lot of people are waiting for the answer.\n",
      "\n",
      "\n",
      " Distilled BERT is trying to approximate BERT, so it will definitely have a little bit of difficulty catching up to BERT.\n",
      "\n",
      "\n",
      " But there is more to it. So it will be a little bit like.\n",
      "\n",
      "\n",
      " Now the question is between BERT and GPT-2, let's say one of the autoencoder, the decoder side of the models, which will do better. Now, if you take just GPT, so not GPT-2, if you just take GPT, clearly there's evidence that BERT does better than GPT. GPT-2, I don't know exactly what the answer is and how they are comparable, but you can look it up. You can look up what it is.\n",
      "\n",
      "\n",
      " Now there is one more sort of a nuance to it.\n",
      "\n",
      "\n",
      " The nuances, if your data set is smaller, then what should you train? Well, the thing is, if you take a small data set and you train a very large model. Now, what will happen is you'll have massive overfitting.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So you don't want to do that.\n",
      "\n",
      "\n",
      " The bigger the model, the bigger the corpus you need to train that model and still not have overfitting, right? Because more data regularizes a model and complex model have a huge problem with overfitting. So you have to also look at the size of corpus available for you to train the model, right? That is another dimension that you have to take into view.\n",
      "\n",
      "\n",
      " And so with that things all being there, Shalini to your question that and I've kept chat GPT away, I'm just looking at the standard transformers.\n",
      "\n",
      "\n",
      " I would say that a distal bird assuming sufficiently large corpus, assuming that the task is let's say sentiment analysis, a distal bird will slightly lag but not by much the bird BERT, especially. And if you use a bigger BERT, bigger BERT will do better. A BERT, a large, will do better. If you compare that to just GPT, because GPT is one word at a time, sort of a, you know, one token at a time emission kind of thinking, generally, and BERT is bidiredirectional it pays attention to the whole sentence it will tend to do better for this task right so tasks specific the performance difference I'll come to you the next part is chat GPT see what happens is chat GPT is not a model it is a composite it's a pipeline of many things right amongst other things is chat is GPT-4. GPT-4 is a ginormous model of a trillion parameters, which has been trained on huge amounts of data.\n",
      "\n",
      "\n",
      " There is reason to believe that chat GPT at this moment seems to be for general purpose tasks.\n",
      "\n",
      "\n",
      " Let's say the sentiment analysis.\n",
      "\n",
      "\n",
      " I have not seen any objective study, but there is people generally believe that Chad GPT beats BERT.\n",
      "\n",
      "\n",
      " Please do the experiment and look for that evidence. See if it is really true or not.\n",
      "\n",
      "\n",
      " And so task by task, you can go and see whether Chad GPT, I mean, I wouldn't use the word chat GPT, I would use GPT-4, whether GPT-4 beats BERT or not, I would imagine that it does, right, simply because it has been trained for far longer and it has been, it is a much larger model.\n",
      "\n",
      "\n",
      " Okay, so now let me take a question. So for Shalini's question, Shalini, go ahead with your question.\n",
      "\n",
      "\n",
      " No, I think you've explained correctly.\n",
      "\n",
      "\n",
      " Any other questions, I'll post to Slack. But I think you've explained my precise question. So I think the summary is that you're saying that, you know, distal BERT was trained on a larger model. Sorry, BERT is trained on a larger model, but distal BERT is trained on a smaller model, but it's the dataset that matters. You're looking at the F1 score and not accuracy because accuracy is very dependent on the dataset.\n",
      "\n",
      "\n",
      " So that's kind of my summary of where we're taking it.\n",
      "\n",
      "\n",
      " Yeah. That is right.\n",
      "\n",
      "\n",
      " So I get it that for a large model, you need large dataset. Otherwise you won't get it that for a large model, you need large data set. Otherwise, you won't get it.\n",
      "\n",
      "\n",
      " But I think we talked about\n",
      "\n",
      "\n",
      " 5G. And 5G in a large model, you only have small data set. So is that 5G in a remote?\n",
      "\n",
      "\n",
      " Yes. Very good question. Masmi has asked a very interesting question.\n",
      "\n",
      "\n",
      " I'll repeat it for all of you who are remote.\n",
      "\n",
      "\n",
      " She says, I made the statement that large models need a lot of data to regularize them, to prevent overfitting. But what about the fact that I also said that for fine tuning a model, you don't need so much data.\n",
      "\n",
      "\n",
      " Right, it can't do that.\n",
      "\n",
      "\n",
      " So how do you explain this paradox?\n",
      "\n",
      "\n",
      " Anyone would like to take a stab at it before I answer it?\n",
      "\n",
      "\n",
      " How come you're fine-tuning a massive model, but in fine-tuning you don't have an overfitting problem?\n",
      "\n",
      "\n",
      " How did that happen?\n",
      "\n",
      "\n",
      " It doesn't shift the model too much, shift the model too much because it recognizes the weights are already set for a certain structure and a small sample size you know you're not drastically the way the instruction is made it just focuses on that data that is That is right.\n",
      "\n",
      "\n",
      " That's absolutely.\n",
      "\n",
      "\n",
      " So what Patrick said, I'll repeat.\n",
      "\n",
      "\n",
      " He said that in smaller data sets, because you're looking at a model that's been trained, the weights are more or less trained for an adjacent problem. And you're just doing small changes, very small changes, to make it even better on this small data set. So when you patch this, run a few epochs over this small data set, the weights will just slightly perturb it.\n",
      "\n",
      "\n",
      " But regularization has already been taken care of by the big data set that trained or pre-trained this model. Remember, fine tuning is the second half that follows the pre-training. Another way to look at it is that the total data set that this model has seen in its full training journey is the vast data set that pre-trained it plus this fine training. So the total data set size is still vast. And so it is a regularized model from that. So how does it, for example, like if it has trained in this vast data set, let's say for like for Genentech, if we have a smaller data for our clinical trial set, we use it to find different model.\n",
      "\n",
      "\n",
      " How does the model keep it segregated and not dissipate everything else?\n",
      "\n",
      "\n",
      " No, no.\n",
      "\n",
      "\n",
      " What happens is it's a very, so long as the problem is the same, I'll take an example.\n",
      "\n",
      "\n",
      " Suppose you, and this is a real thing, you trained a model to do sentiment analysis. We did this example actually in one of our previous labs, to do sentiment analysis and it is pre-trained on general text.\n",
      "\n",
      "\n",
      " But people in the financial world, they speak a peculiar dialect of their own.\n",
      "\n",
      "\n",
      " They have bears and bulls and all sorts of things, derivatives and whatnot.\n",
      "\n",
      "\n",
      " So what you do is you need to just make it understand that little bit. And so what happens is when you take a small corpus of financial text and run it over that, it has already gotten the gist of the English language, right? So, for example, if it encounters the word, I hate, hate, hate today, right? So, sentiment is pretty negative, right? And financial terms are not, I mean, there's no financial term that will say, oh, this is great, right you need to be uh be more right like you know that if you use the word this company is i think this stock is highly leveraged it's a negative you don't want to buy it\n",
      "\n",
      "\n",
      " no\n",
      "\n",
      "\n",
      " but in english you wouldn't know what leverage means right it's it's right if something is highly leveraged\n",
      "\n",
      "\n",
      " well what is it so you need to give it that little bit of weight change so that it starts recognizing this and when you that's why i use the word minor perturbations you don't see drastic changes you just see fine tuning people use the word fine tuning for that reason and the weight change will only be for your own usage for your own usage because the model that comes out is your model you started with somebody else's model and you tuned it. Now when you save the model, it's your ways.\n",
      "\n",
      "\n",
      " That's it. Okay, guys.\n",
      "\n",
      "\n",
      " So with that, we move forward now to word segments. And we talked about word segments. We talked about the attention mask.\n",
      "\n",
      "\n",
      " We now know all the three components of this. So to summarize, we know what input IDs are.\n",
      "\n",
      "\n",
      " This is the index in the vocabulary. So imagine that there's Webster's dictionary, there's Oxford's dictionary, and then there is the tokenizer's dictionary, right? So where the word pieces have IDs as their meaning, right? So you just look up the ID, which you use for hot encoding, when hot encoding. Token type IDs is not terribly relevant for us here for the use cases we're looking, but it basically says that when you're passing it to a bird-like architecture, is it part of the first sentence or is it part of the second sentence?\n",
      "\n",
      "\n",
      " The third one, attention mask matters.\n",
      "\n",
      "\n",
      " Is this fair game for attention or is it just padding and you need to ignore attention from it?\n",
      "\n",
      "\n",
      " That's all it takes.\n",
      "\n",
      "\n",
      " So we understood the three parts.\n",
      "\n",
      "\n",
      " Attention mask.\n",
      "\n",
      "\n",
      " And... No, no, no.\n",
      "\n",
      "\n",
      " Vocabulary index never changes. It's the weights that change.\n",
      "\n",
      "\n",
      " Remember, if it encounters a word that is not in your vocabulary, it will just map it to you and you and K. Yes. Fix the maximum set of a variety of tokens then is the vocabulary size.\n",
      "\n",
      "\n",
      " That's it.\n",
      "\n",
      "\n",
      " So you always, every tokenizer starts with its decided vocabulary that's it gets mapped and it is not as terrible as it sounds why because the meaning of that word is inferred from the context that's the whole point of attention.\n",
      "\n",
      "\n",
      " So all is not lost.\n",
      "\n",
      "\n",
      " Right? That is it.\n",
      "\n",
      "\n",
      " So let's say that the word zebra was never there. And the sentence is, the zebra ran with all that sounds a lot like horse.\n",
      "\n",
      "\n",
      " That's fine. That's a point.\n",
      "\n",
      "\n",
      " So you notice that even if you give it a nonsensical word, when you use word pieces, like I used a Jabberwocky statement. Jabberwocky is this beautiful poem that as kids we all love, if you read it from it, right?\n",
      "\n",
      "\n",
      " And it says, it starts with, it was Billig in the slithy toes the giant jimble in the way and mimsy with the borogos and so forth and when you read that it sounds very sensible except that when you think of the words none of the words make sense\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so i took jabberwocky's first sentence and tokenize it and one of the interesting things is that when you do word pieces it will actually break it up into things it has so brillic is an unknown word but it broke it up into br because br is the prefix to many words brother and whatnot and uh ill still bill hill mill so it got that and ig I don't know what what ends with egg.\n",
      "\n",
      "\n",
      " Twig. Pig. Twig. Oh, yeah, there we go.\n",
      "\n",
      "\n",
      " So many work with that.\n",
      "\n",
      "\n",
      " Yes.\n",
      "\n",
      "\n",
      " And a slithy again, slit and the hy hy is many, many, many, many, many, many things have suffix and VESE-S. Oh, lots of. A word galore that ends with V-E-S, doesn't it?\n",
      "\n",
      "\n",
      " And so you notice that none of them got mapped to unknown. I give it to you as an exercise to find a word which will get mapped to you and K. Okay.\n",
      "\n",
      "\n",
      " Right.\n",
      "\n",
      "\n",
      " So now, the process of encoding or tokenizing the converse of it is decoding. So when you decode it, you realize that when you decode this, it gets to this, it was billing in the slithy toes sandwiched between the CLS and CEP. CLS is the special token that you'll use, for example, for classifier.\n",
      "\n",
      "\n",
      " CEP is the separator between the two statements for both.\n",
      "\n",
      "\n",
      " Now notice that everything has been lowercase. So it has been lowercase. But that is what this tokenizer does.\n",
      "\n",
      "\n",
      " So you realize that the tokenizer does more than just tokenize the words into pieces.\n",
      "\n",
      "\n",
      " It does some pre-processing. It does some post-processing, isn't it?\n",
      "\n",
      "\n",
      " So let's go into that a little bit more.\n",
      "\n",
      "\n",
      " So when you look at Jabberwocky and you look at the token.\n",
      "\n",
      "\n",
      " Now, one more thing is there.\n",
      "\n",
      "\n",
      " When you tokenize, if you, instead of using the tokenizer as a function itself, callable function, you invoke the tokenize method, it will deliberately give it to you without the paddings, without the CLS and sub-paddings.\n",
      "\n",
      "\n",
      " It's something to know. You notice that it did not produce those things.\n",
      "\n",
      "\n",
      " Yeah, it's one way.\n",
      "\n",
      "\n",
      " Now you can convert tokens to IDs and you can decode it.\n",
      "\n",
      "\n",
      " It will give it back to you the thing as it is.\n",
      "\n",
      "\n",
      " Now, one more thing to do is to, you know, we use the auto class, right? Auto tokenizer.\n",
      "\n",
      "\n",
      " You don't have to.\n",
      "\n",
      "\n",
      " If you really insist, you know what you're talking about, you happen to know through private channel that bird-based un-cased is a checkpoint that used bird. What's that private channel?\n",
      "\n",
      "\n",
      " You go read the documentation on the Hugging Face website.\n",
      "\n",
      "\n",
      " So offline you know that it's a bird model. So then you can explicitly say bird tokenizer. You're tokenizing it for a bird model.\n",
      "\n",
      "\n",
      " You can do that.\n",
      "\n",
      "\n",
      " And pretty much the same thing will happen.\n",
      "\n",
      "\n",
      " Then you see, BERT model will tell you that BERT tokenizer will declare that it is, it tells you a lot about itself. It says, oh, BERT tokenizer uses a vocabulary of 30,000 words, pretty large vocabulary, frankly.\n",
      "\n",
      "\n",
      " And now in tokenizers, there are two things.\n",
      "\n",
      "\n",
      " uses a vocabulary of 30,000 words, pretty large vocabulary, frankly.\n",
      "\n",
      "\n",
      " And now in tokenizers, there are two things. There is a fast tokenizers.\n",
      "\n",
      "\n",
      " Let me leave that as a homework.\n",
      "\n",
      "\n",
      " What is a fast tokenizer?\n",
      "\n",
      "\n",
      " It's a homework for you guys. Post the answers to Slack.\n",
      "\n",
      "\n",
      " Let's see who gets there first.\n",
      "\n",
      "\n",
      " Padding site means you pad it on the right. Like if your sentence is shorter, you pad it with extras on the right.\n",
      "\n",
      "\n",
      " A truncation site also right.\n",
      "\n",
      "\n",
      " So suppose you give a sentence that leads to more than 512 tokens, chop off. Why are we chopping at 512? If you remember the BERT architecture, we said 512 is the token limit that it will take then if you need more tokens use some other transform right use big bird right or something like that so special tokens what are the special tokens it used for unknown tokens it use it uses ank\n",
      "\n",
      "\n",
      " It uses ANK, separator, SAP, padding, PAD, and class token, the first one, CLS, and mask, MSK, mask. That is if you're doing a masking job, mask language modification.\n",
      "\n",
      "\n",
      " So as a code for BERT, it's 512 input and then 512 output? No, no, no, output is up to you.\n",
      "\n",
      "\n",
      " The hidden state for a typical BERT is 768. The hidden state, and we'll see that in a moment.\n",
      "\n",
      "\n",
      " But at this moment, focus only on the input and the tokenizer part.\n",
      "\n",
      "\n",
      " So what is it doing?\n",
      "\n",
      "\n",
      " The tokenizer, if you use for example the XLM Robota base, which is good, quite commonly used for multilingual text.\n",
      "\n",
      "\n",
      " And by the way, multilingual for me is a reality. I tend to, I used to have under me as a leader, many teams. I had a Ukrainian team. I had a Paris team and I have a team in India.\n",
      "\n",
      "\n",
      " And what were some very interesting things you learn.\n",
      "\n",
      "\n",
      " So I would do this daily meetings and people felt very free with me like they would never really took me seriously. So they would, you know, they would get heated conversations and so forth. So with the Ukrainians, what would happen is when they had difficulty explaining, they would suddenly transition into Russian. they would suddenly transition into Russian, right? And then they would look at it, just explained you, right? And I have no clue what they said, you know, and they wouldn't realize that they just spoke something in Russian. So they have a mix of English and Russian. And I had to sort of read between the lines to figure out what they must have said, or they would use a Russian word for something. So I literally had to use my own attention heads to figure out what they must have meant, right?\n",
      "\n",
      "\n",
      " When I would talk to, a similar thing would happen with the French people, right? And then with India, it would be very interesting if two people are debating and they're both Telugu. Before you know it, they're speaking Teluguugu and if those two people are from Kannada they're speaking in that right and suddenly sometimes some peculiar things would happen one guy would suddenly launch into Tamil and then the Kannada would say you know I don't know Tamil in English\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so the conversation would be actually multilingual in my meetings so well if you're parsing a multilingual in my meetings. So well, if you're parsing a multilingual conversation, you do need an auto tokenizer that's multilingual sensitive, right?\n",
      "\n",
      "\n",
      " So you use xlmrover.tabase.\n",
      "\n",
      "\n",
      " It does a pretty good job, but notice that the tokens it is using are, do you notice that?\n",
      "\n",
      "\n",
      " What is it using it for the start of a sentence\n",
      "\n",
      "\n",
      " yes right the html notation almost html markup notation and slashes which is again the end of sentence um that so that is it and then it says any word start is with an underscore\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " and if it doesn't start with the underscore then it probably is just a suffix\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " So every tokenizer will have a different convention.\n",
      "\n",
      "\n",
      " I leave it to you as a homework to find out what sort of tokens that BPE will generate, BPE tokenizer generates, which GPT uses. So now I also said that see tokenizer itself is a pipeline. There is more going on in the tokenizer than you think.\n",
      "\n",
      "\n",
      " So there are four steps in the tokenizer itself is a pipeline.\n",
      "\n",
      "\n",
      " There is more going on in the tokenizer than you think. So there are four steps in the tokenizer.\n",
      "\n",
      "\n",
      " First is basic cleanup. So what's a basic cleanup?\n",
      "\n",
      "\n",
      " String whitespaces, multiple whitespaces you may want to condense into one or something like that.\n",
      "\n",
      "\n",
      " Remove accented characters, lowercase everything.\n",
      "\n",
      "\n",
      " Those are your normalization steps.\n",
      "\n",
      "\n",
      " So D-O-G capital would look the same as D-O-G small case and mixed case and so forth.\n",
      "\n",
      "\n",
      " All of those things you do.\n",
      "\n",
      "\n",
      " Normalization is something that happens from other languages.\n",
      "\n",
      "\n",
      " I don't know what it is.\n",
      "\n",
      "\n",
      " If somebody can explain to me i'll be happy i mean no point in my regurgitating the definition of normalization but apparently the same thing can be written in multiple ways uh in i do know in hindi and sanskrit you have the word the there is a sort of a wobble or not wobble or something not a wobble actually but a r right a half an r you attach now you can attach before the word which will be at the bottom of the character or above the word as a curly crescent and that will come after the word\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " and so there are these peculiarities so I'm assuming that normalization and that will come after the word, right? And so there are these peculiarities. I'm assuming that normalization somehow has to do with that.\n",
      "\n",
      "\n",
      " I don't know, right?\n",
      "\n",
      "\n",
      " If somebody knows what it is can enlighten.\n",
      "\n",
      "\n",
      " So you do all of that cleanups and so forth. The second thing you do is you do pre-tokenization. Simplest way to think of pre-tokenization is you have this sentence. If you're, for English, if you're breaking it up into words, breaking it up into white spaces, right, that's a pretty good way of thinking of pre-tokenization. Or, you know, what, if you use Java, for example, think of the tokenizer of Java, right, word tokenizer of Java, word break, how would it do? It would break it up into words segments or word pieces either sentence p you can use word piece sentence piece bp whatever but word segments i'll use the word word segments breaking it up into logical word parts that are part of the vocabulary so tokenizer is the only one that makes sure that everything is broken down into something in this very well-published tokenizer's dictionary, right? Not Oxford's dictionary or Webster's dictionary, but the tokenizer's dictionary. Its vocabulary must contain it, right? So you should say page 13, there is this token described, right? And it has an index to it, that sort of thing, giving the metaphor of an actual dictionary with pages.\n",
      "\n",
      "\n",
      " And post-processing does what?\n",
      "\n",
      "\n",
      " Oh, you need to still pad it up with something.\n",
      "\n",
      "\n",
      " CLS, CEP, PAD, MASK, and so on and so forth.\n",
      "\n",
      "\n",
      " All of those special tokens that need to be inserted, that will be the post-processing step so tokenizer pipe is a pipeline in itself it goes through four steps are we together guys and enough for tokenizers we'll take a 10 minutes break\n",
      "\n",
      "\n",
      " oh my goodness it's already getting to be five do you guys have have okay no if it is five then we should start the next topic next time go ahead please uh so when you're talking about the multi-language uh tokenizer so how does it differentiate between like a token from like english versus like a token from french or spanish it does it moves you cross train it on corpus that includes both the languages\n",
      "\n",
      "\n",
      " okay\n",
      "\n",
      "\n",
      " so is there like a way for you to specify that i'm going to use these two languages\n",
      "\n",
      "\n",
      " oh yeah yeah\n",
      "\n",
      "\n",
      " you can do that I'm going to use these two languages?\n",
      "\n",
      "\n",
      " Oh yeah, yeah, you can do that.\n",
      "\n",
      "\n",
      " But generally these things are pretty good at even sensing.\n",
      "\n",
      "\n",
      " So one of the NLP tasks is you see character, which language is it? Language detection.\n",
      "\n",
      "\n",
      " Right, and so they're very good at it. Language detection is generally fairly accurate. protection is generally fairly accurate.\n",
      "\n",
      "\n",
      " Okay, guys, I wanted to cover one more thing, the classifier.\n",
      "\n",
      "\n",
      " We will get to the sentence.\n",
      "\n",
      "\n",
      " So what is the journey ahead? Our journey ahead is we are now going to feed this tokenization into a pre-trained classifier. We'll stay with our running example of sentiments.\n",
      "\n",
      "\n",
      " We will see how it is that we can do it, not using the pipeline, but by hand doing the pieces, putting the tokenizer, taking its output, converting it into the PyTorch format, then feeding it into a PyTorch model, like let's say Bird or something like that. Taking the output, then putting our own softmax or some classifier layers, running it through that, seeing the output.\n",
      "\n",
      "\n",
      " Then what happens is, remember, this is one part of the journey.\n",
      "\n",
      "\n",
      " But we are used only pre-trained.\n",
      "\n",
      "\n",
      " There is no fine-tuning here.\n",
      "\n",
      "\n",
      " So the next thing we'll do is we will take the emotions data set and we will now fine-tune BERT. data set and we will now fine-tune bird specifically this whole pipe will fine-tune the model to do a much better job on this particular task emotion detection right and see how well it can do and we will see so to give you a sense we will see that you'll go the pre-train will give you accuracies in the 60s, which is pretty good considering that if you're predicting in six classes, you're gone from somewhere like 16% to 60s, pretty huge lift, but you can do better. When you fine tune, you will go into the 90s, right? And when you go into the 90s, especially considering that there is a class imbalance, you will realize that that's a pretty good lift and that argues in favor of the fact that you should never skip the fine-tuning stage.\n",
      "\n",
      "\n",
      " A lot of people, they just take a pre-trained model and say, I'm done.\n",
      "\n",
      "\n",
      " It's working good now.\n",
      "\n",
      "\n",
      " But it's worth fine-tuning more, right?\n",
      "\n",
      "\n",
      " it's working good enough\n",
      "\n",
      "\n",
      " but it's worth fine-tuning model right that stage is worth doing the second thing we will learn is once we have fine-tuned the model we will look at those cases where the loss was high the prediction loss because see when you predict a number\n",
      "\n",
      "\n",
      " and you compare it with the answer.\n",
      "\n",
      "\n",
      " Your probability says, let's say for anger, it says 0.1. But the answer is anger.\n",
      "\n",
      "\n",
      " So the loss is high. Cross-entropy loss is high there, isn't it? That's the whole point of the cross-entropy loss. So what is cross-entropy, guys? Going back to what I explained about classifiers and this, the cross-entropy loss is a big word, but all it means is it is the level of surprise you have.\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So one easy way to think about it is that think about this.\n",
      "\n",
      "\n",
      " If you know that you think your model is good, presumption model is worse or the radio typically is the weatherman. The weatherman has said it's going to be the chances of rain are less than one percent and you go out and it's pouring.\n",
      "\n",
      "\n",
      " Are you surprised?\n",
      "\n",
      "\n",
      " It's surprising assuming you believe the weatherman which these days is hard to believe, given the California weather. But let's say that you believe it.\n",
      "\n",
      "\n",
      " Are you surprised at the weatherman's prediction now?\n",
      "\n",
      "\n",
      " Right?\n",
      "\n",
      "\n",
      " So let's say that he said one percent chance of rain, it's pouring outside.\n",
      "\n",
      "\n",
      " How much is the surprise?\n",
      "\n",
      "\n",
      " Suppose he had said 99 percent chance that it rains. You went out and it's pouring rain.\n",
      "\n",
      "\n",
      " Are you surprised? You're not surprised.\n",
      "\n",
      "\n",
      " But when he said probability is 0.1 or 0.01, one person, then you have a massive surprise. So you say, well, how much is the surprise? One way to look at it is to say, well, it must be the reciprocal of the probability.\n",
      "\n",
      "\n",
      " Think about it.\n",
      "\n",
      "\n",
      " If the probability is high, predicted probability is high, my surprise is less because it's raining. If the predicted probability was low, now I'm really surprised at the model's prediction. Unpleasantly surprised, isn't it? So 1 over p may be a good way of thinking of surprise. So 1 over p may be a good way of thinking of surprise.\n",
      "\n",
      "\n",
      " But there is a problem with that.\n",
      "\n",
      "\n",
      " The 1 over p, if you think about it, what is the range of values it goes through? It goes from 1 to infinity.\n",
      "\n",
      "\n",
      " You may say that, well, when it said 100% chance of rain and it rains, why am I surprised?\n",
      "\n",
      "\n",
      " Why is the value even 1? I would much prefer if the values was zero. It went from zero to infinity, right?\n",
      "\n",
      "\n",
      " And so there is an easy mathematical trick. Mathematicians will say,\n",
      "\n",
      "\n",
      " oh yeah, that's an easy one.\n",
      "\n",
      "\n",
      " Take the log of it, right? Log of one is zero and log of infinity is still infinity\n",
      "\n",
      "\n",
      " right so log of reciprocal probability is a measure of surprise but you don't use the long word measure of surprise you instead use the word entropy right it's your cross entropy and that is exactly what the cross entropy loss is.\n",
      "\n",
      "\n",
      " You got it?\n",
      "\n",
      "\n",
      " That's why cross entropy loss is the summation over all the probability predictions and how wrong they were.\n",
      "\n",
      "\n",
      " As simple as that. Anyway, it's a recap of that.\n",
      "\n",
      "\n",
      " So today, guys, we didn't reach as far as we did. From next time, we'll have to move a little bit faster.\n",
      "\n",
      "\n",
      " I need to finish this hugging faces and we have a lot of interesting examples.\n",
      "\n",
      "\n",
      " But the point is it is far better to understand deeply and move slowly than to rush ahead.\n",
      "\n",
      "\n",
      " Any other questions, guys?\n",
      "\n",
      "\n",
      " I'm totally open.\n",
      "\n",
      "\n",
      " We have 15 minutes.\n",
      "\n",
      "\n",
      " Go ahead, Sushil.\n",
      "\n",
      "\n",
      " So, the whole of the open user followed by this\n",
      "\n",
      "\n",
      " no no not sentence transformer our transformer remember sentence transform is a special architecture that uses a transform in a in a simian architecture right network two of them put together no ignore sentence transfer just transform\n",
      "\n",
      "\n",
      " yeah\n",
      "\n",
      "\n",
      " so classifier is therefore so let me give you a preview of what it means what we'll come to and we'll repeat it next time so imagine that you know what a classifier is. You have a classifier, you give it an input vector, output will come up. The problem is we don't have an input vector, we have a text.\n",
      "\n",
      "\n",
      " So what is the journey that we need to go through to classify?\n",
      "\n",
      "\n",
      " First thing you need to do, you need to tokenize it and make hot encoding.\n",
      "\n",
      "\n",
      " But then you realize that if you feed it into a direct classifier, your performance will suck because it is not attention. It hasn't gotten the benefit of attention. The semantic awareness is not there. The context is not there.\n",
      "\n",
      "\n",
      " So what do you do?\n",
      "\n",
      "\n",
      " You sandwich and transform a body into it.\n",
      "\n",
      "\n",
      " And it's so turns out that these transformers that are posted on Hugging Pizzles, most of them, they're headless. Remember, they're zombies.\n",
      "\n",
      "\n",
      " So if you want to classify, you need to screen a classifier head on top of the transformer body. So the transformer is sandwiched between two things, tokenizer before it and classifier after it to make the complete pipeline text classification pipeline you got it\n",
      "\n",
      "\n",
      " right\n",
      "\n",
      "\n",
      " so look at the picture on my screen that's what it is there also because even the tokenizers have been trained right so in fact why don't i let you explore the question it's interesting go check it out the code of the tokenizer is available see how it is done\n",
      "\n",
      "\n",
      " okay\n",
      "\n",
      "\n",
      " so i'm trying to fix the we are trying to do a pre-trained model. Yeah, the pre-trained just means that somebody has already created the tokenizer for you. And it's pre-trained in the sense that it is completely adapted to the, for example, if you get bird-based uncased, it is completely adapted to producing an output that can just feed into bird.\n",
      "\n",
      "\n",
      " But that would have assumed some trap tokenism. Yeah, and the transformer assumes the tokenism.\n",
      "\n",
      "\n",
      " All these two have to go together. They have to go together.\n",
      "\n",
      "\n",
      " We can't pick arbitrary some tokenism model and...\n",
      "\n",
      "\n",
      " Yeah, the reason for that is why are the two broken up? The reason they are broken up is one, even though they are producing the same vector, embedding, the same embedding to go into the transformer, remember the fact, or not even embedding, just the hot encoding vector, one hot encoding, they are producing the same.\n",
      "\n",
      "\n",
      " Given BERT, let's say that we are taking BERT case, uncased base model.\n",
      "\n",
      "\n",
      " One tokenizer is good at English. Another tokenizer may be good at Mandarin. Another tokenizer may be good at multilingual. Another may be good at Tamil, right?\n",
      "\n",
      "\n",
      " And that is why you don't want to sandwich the tokenizer. You don't want to put the twoizer. You don't want to put the two together.\n",
      "\n",
      "\n",
      " You have far more flexibility.\n",
      "\n",
      "\n",
      " If you let the transformer just be the transformer and let the right tokenizer bring it the same one-hot encoded vectors that it expects. And it doesn't care how it got that vector because the different tokenizers are better at dealing with different languages.\n",
      "\n",
      "\n",
      " That is the point. Now you got it.\n",
      "\n",
      "\n",
      " And you don't want to create one mega ginormous tokenizer that does everything perfectly.\n",
      "\n",
      "\n",
      " That is it.\n",
      "\n",
      "\n",
      " So these are, see, you're used to Unix, right? What is the basic Unix philosophy?\n",
      "\n",
      "\n",
      " Simple commands, it does one thing well, and then you use the pipe. And you weave the pipes together and before you know it on that one command line, you can achieve what some commercial software does after charging you $99. dollars.\n",
      "\n",
      "\n",
      " That's a basic philosophy.\n",
      "\n",
      "\n",
      " So the whole idea is pipeline, build pipelines out of reusable components.\n",
      "\n",
      "\n",
      " That is why tokenizers are interchangeable based on what corpus, what data you have, use the right tokenizer.\n",
      "\n",
      "\n",
      " Models are interchangeable. But of course, once you pick a tokenizer, you better use the model compatible with it.\n",
      "\n",
      "\n",
      " Right? That's it.\n",
      "\n",
      "\n",
      " The name will be a giveaway. The tokenizer name will start with, like a BERT case, tells you clearly that I'm going to feed into BERT.\n",
      "\n",
      "\n",
      " Right? That's it.\n",
      "\n",
      "\n",
      " Generally, there's checkpoints, right?\n",
      "\n",
      "\n",
      " The names are such that if you use the same name for the tokenizer as well as for the model, you're doing pretty well.\n",
      "\n",
      "\n",
      " How do you? No, no.\n",
      "\n",
      "\n",
      " So the documentation will say if you go to the card of that thing, the tokenizer, it will say that it has been trained for english or for this or it's good for this it will say that and that brings us again the same thing right people are posting a lot of things on hugging phrase doing good diligence and writing good documentation means people will actually use your stuff if you don't document it properly nobody knows what it is for. Give sample code, document it.\n",
      "\n",
      "\n",
      " Okay, that brings me to one thing, guys.\n",
      "\n",
      "\n",
      " This file, the classifier, auto classifier, don't use it.\n",
      "\n",
      "\n",
      " I mean, you can use it, but the last element, I think has a bug at this moment.\n",
      "\n",
      "\n",
      " I need to fix because the things have changed a little bit.\n",
      "\n",
      "\n",
      " But now, this book, the book that I was recommending is there in this, \"'Natural Language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from svlearn.text import ChunkText\n",
    "import os\n",
    "os.chdir('/home/asif/github/llm-bootcamp')\n",
    "\n",
    "chunker = ChunkText()\n",
    "chunks: [str] = chunker.create_chunks(transcript)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print (f'\\n {chunk}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2551246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
