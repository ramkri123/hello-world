{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:53.489230Z",
     "start_time": "2023-03-25T10:55:51.576481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!-- Many of the styles here are inspired by: \n",
       "    https://towardsdatascience.com/10-practical-tips-you-need-to-know-to-personalize-jupyter-notebook-fbd202777e20 \n",
       "       \n",
       "    \n",
       "    On the author's local machine, these exist in the custom.css file. However, in order to keep uniform look and feel, \n",
       "    and at the request of participants, I have added it to this common import-file here.\n",
       "\n",
       "    -->\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Lora:400,700|Montserrat:300\" rel=\"stylesheet\">\n",
       "\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Crimson+Pro&family=Literata&display=swap\" rel=\"stylesheet\">\n",
       "<style>\n",
       "\n",
       "\n",
       "#ipython_notebook::before{\n",
       " content:\"NLP with Transformers\";\n",
       "        color: white;\n",
       "        font-weight: bold;\n",
       "        text-transform: uppercase;\n",
       "        font-family: 'Lora',serif;\n",
       "        font-size:16pt;\n",
       "        margin-bottom:15px;\n",
       "        margin-top:15px;\n",
       "           \n",
       "}\n",
       "body > #header {\n",
       "    #background: #D15555;\n",
       "    background: linear-gradient(to bottom, indianred 0%, #fff 100%);\n",
       "    opacity: 0.8;\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       ".navbar-default .navbar-nav > li > a, #kernel_indicator {\n",
       "    color: white;\n",
       "    transition: all 0.25s;\n",
       "    font-size:10pt;\n",
       "    font-family: sans-serif;\n",
       "    font-weight:normal;\n",
       "}\n",
       ".navbar-default {\n",
       "    padding-left:100px;\n",
       "    background: none;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "\n",
       "body > menubar-container {\n",
       "    background-color: wheat;\n",
       "}\n",
       "#ipython_notebook img{                                                                                        \n",
       "    display:block; \n",
       "    \n",
       "    background: url(\"https://www.supportvectors.com/wp-content/uploads/2016/03/logo-poster-smaller.png\") no-repeat;\n",
       "    background-size: contain;\n",
       "   \n",
       "    padding-left: 600px;\n",
       "    padding-right: 100px;\n",
       "    \n",
       "    -moz-box-sizing: border-box;\n",
       "    box-sizing: border-box;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "body {\n",
       " #font-family:  'Literata', serif;\n",
       "    font-family:'Lora', san-serif;\n",
       "    text-align: justify;\n",
       "    font-weight: 400;\n",
       "    font-size: 12pt;\n",
       "}\n",
       "\n",
       "iframe{\n",
       "    width:100%;\n",
       "    min-height:600px;\n",
       "}\n",
       "\n",
       "h1, h2, h3, h4, h5, h6 {\n",
       "# font-family: 'Montserrat', sans-serif;\n",
       " font-family:'Lora', serif;\n",
       " font-weight: 200;\n",
       " text-transform: uppercase;\n",
       " color: #EC7063 ;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: #000080;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       "#notebook_name {\n",
       "    font-weight: 600;\n",
       "    font-size:20pt;\n",
       "    text-variant:uppercase;\n",
       "    color: wheat; \n",
       "    margin-right:20px;\n",
       "    margin-left:-500px;\n",
       "}\n",
       "#notebook_name:hover {\n",
       "background-color: salmon;\n",
       "}\n",
       "\n",
       "\n",
       ".dataframe { /* dataframe atau table */\n",
       "    background: white;\n",
       "    box-shadow: 0px 1px 2px #bbb;\n",
       "}\n",
       ".dataframe thead th, .dataframe tbody td {\n",
       "    text-align: center;\n",
       "    padding: 1em;\n",
       "}\n",
       "\n",
       ".checkpoint_status, .autosave_status {\n",
       "    color:wheat;\n",
       "}\n",
       "\n",
       ".output {\n",
       "    align-items: center;\n",
       "}\n",
       "\n",
       "div.cell {\n",
       "    transition: all 0.25s;\n",
       "    border: none;\n",
       "    position: relative;\n",
       "    top: 0;\n",
       "}\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border: none;\n",
       "    background: transparent;\n",
       "    box-shadow: 0 6px 18px #aaa;\n",
       "    z-index: 10;\n",
       "    top: -10px;\n",
       "}\n",
       ".CodeMirror pre, .CodeMirror-dialog, .CodeMirror-dialog .CodeMirror-search-field, .terminal-app .terminal {\n",
       "    font-family: 'Hack' , serif; \n",
       "    font-weight: 500;\n",
       "    font-size: 14pt;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "<center><img src=\"https://d4x5p7s4.rocketcdn.me/wp-content/uploads/2016/03/logo-poster-smaller.png\"/> </center>\n",
       "<div style=\"color:#aaa;font-size:8pt\">\n",
       "<hr/>\n",
       "&copy; SupportVectors. All rights reserved. <blockquote>This notebook is the intellectual property of SupportVectors, and part of its training material. \n",
       "Only the participants in SupportVectors workshops are allowed to study the notebooks for educational purposes currently, but is prohibited from copying or using it for any other purposes without written permission.\n",
       "\n",
       "<b> These notebooks are chapters and sections from Asif Qamar's textbook that he is writing on Data Science. So we request you to not circulate the material to others.</b>\n",
       " </blockquote>\n",
       " <hr/>\n",
       "</div>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run supportvectors-common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T23:28:41.781265Z",
     "start_time": "2021-12-15T23:28:41.750084Z"
    }
   },
   "source": [
    "# Semantic search\n",
    "\n",
    "Traditionally, one would search through a corpus of documents using a keywords-based search engine like Lucene, Solr, ElasticSearch, etc. While the technology has matured, the basic underlying approach behind keyword search engines is to maintain an *inverted-index* mapping keywords to a list of documents that contain them, with associated relevances.\n",
    "\n",
    "In general, the keywords-based search approach has been quite successful over the years, and have matured with added features and linguistic capabilities.\n",
    "\n",
    "However, this approach has had its limitations. The principal cause of it goes to the fact that when we enter keywords, it is a human tendency to describe the intent of what we are looking for. For example, if we enter \"breakfast places\", we implicitly also mean restaurants, cafe, etc that serve items appropriate for breakfast. There may be a restaurant described as a shop for expresso, or crepe, that a keywords-search will likely miss, since its keywords do not match the query terms. And yet, we would hope to see it near the top of the search results.\n",
    "\n",
    "Semantic search is an NLP approach largely relying on deep-neural networks, and in particular, the transformers that make it possible to more closely infer the human intent behind the search terms, the relationship between the words, and the underlying context. It allows for entire sentences -- and even paragraphs -- describing what the searcher's intent is, and retrieves results more relevant or aligned to it.\n",
    "\n",
    "## How would we do this NLP task with AI?\n",
    "\n",
    "Let us represent the functional behavior we expect: \n",
    "\n",
    "\n",
    "![](images/semantic-search-functionality.png)\n",
    "\n",
    "\n",
    "### Magic happens: breaking it down into steps\n",
    "\n",
    "We recall that machine-learning algorithms work with vectors ($\\mathbf{X}$) representation of data.\n",
    "\n",
    "So the first order of business would be to map each of the document texts $D_i$ to its corresponding vector $X_i$ in an appropriate $d$-dimensional space, $\\mathbb{R}^d$, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "D_i \\longrightarrow X_i \\in \\mathbb{R}^d\n",
    "\\end{equation}\n",
    "\n",
    "This resulting vectors are called **sentence embeddings**. Once these embeddings are for each of the documents, we can store the collection of tuples $[<D_1, X_1>, <D_2, X_2>, ..., <D_n, X_n>]$. Here each tuple corresponds to a document and its sentence embedding.\n",
    "\n",
    "This collection of tuples, therefore, becomes our **search index**.\n",
    "\n",
    "### Search\n",
    "\n",
    "Now, when the user described what she is looking for, we consider the entire text as a \"sentence\".\n",
    "<p>\n",
    "<div class=\"alert-box alert-warning\" style=\"padding-top:30px\">\n",
    "   \n",
    "<b >Caveat Emptor</b>\n",
    "\n",
    "> Note that we have a rather relaxed definition of a *sentence* in NLP: it diverges from a grammmatical definition of a sentence somewhat.  For example, in the English language, we would consider a sentence to be terminated with a punctuation, such as a period, question-mark or exclamation. However, in NLP, we loosely consider the entire text -- whether it is just a word, or a few keywords, or an english sentence, or a few sentences together -- as one **sentence** for the purposes of natual language processing task.\n",
    "    \n",
    "<p>\n",
    "</div>\n",
    "    \n",
    "Therefore, it is common to consider an entire document text as a *sentence* if the text is relatively short. Alternatively, it is partitioned into smaller chunks (of say 512-tokens each), and each such chunk is considered an NLP *sentence*.\n",
    "\n",
    "Since we consider the entire query text as a sentence, we can map it to its **sentence embedding vector**, ${Q}$.\n",
    "\n",
    "#### Vector Similarity\n",
    "Once we have this, we simply need to compare the query vector ${Q}$ with each of the document vectors $X_i$, and sort the document vectors in descending order of similarity.\n",
    "\n",
    "The rest is trivial: pick the top-k  in the sorted document vectors list. Then for each vector, look up its corresponding document, and return the list as sorted search result of relevant document.\n",
    "\n",
    "We expect that these documents will exhibit high semantic similarity with the search query, assuming that the search index did contain such documents.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png\">\n",
    "    <caption> Semantic similarity as vector proximity in the embedding space. <br>\n",
    "    (Figure source: Sbert.net documentation).\n",
    "    </caption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### Similarity measures\n",
    "\n",
    "The sentence embedding vectors typically exist in very large dimensional space (e.g., 300 dimensions). In such large dimensional spaces, the notion of euclidean distance is not as effective. Therefore, it is far more common to use one of the two below measures for vector similarity:\n",
    "\n",
    "* **dot-product**, the (inner) dot-product between the embedding vectors.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{dot-similarity} = \\langle X_i, X_j \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "* **cosine-similarity**, the $\\cos \\left(\\theta_{ij}\\right)$ gives degree of directional alignment between the vectors, but ignores their magnitudes. Here, $\\theta_{ij}$ is the angle between $X_i$ and $X_j$ (embedding) vectors.\n",
    "\n",
    "\\begin{equation} \n",
    "\\text{cosine-similarity} = \\frac{\\langle X_i, X_j \\rangle} {\\| X_i \\| \\| X_j \\|}\n",
    "\\end{equation}\n",
    "\n",
    "<div class=\"alert-box alert-info\" style=\"padding-top:30px\">\n",
    "   \n",
    "**Important**\n",
    "    \n",
    ">  Sentence transformer models trained with cosine-similarity tend to favor the shorter document texts in the search results, whereas the models trained on the dot-product similarity tend to favor longer texts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric vs asymmetric search\n",
    "\n",
    "One of the technical aspects to be careful of is the relative textual length of the query sentence compared to the actual documents. Different sentence-transformer models have been trained specifically for each of these use-cases. \n",
    "\n",
    "* **symmetric search** when we expect the query-sentence to be approximately the same length as the document sentences.\n",
    "\n",
    "* **asymmetric search** when we expect the document texts to be significantly larger in length to the query sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load an appropriate model\n",
    "\n",
    "Let us consider the use-case where we are searching through some reasonably large documents. In such a case, it would be appropriate to use an asymmetric-search model. \n",
    "\n",
    "Let us consider an asymmetric model trained with *cosine-similarity* as the distance measure. In particular, let us use one of the below models:\n",
    "\n",
    "* `\n",
    "\n",
    "\n",
    "We load the model with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:56.147887Z",
     "start_time": "2023-03-25T10:55:53.491388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686fa9368dfd4d6595c967a454ed7ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17a6bb672a54bc8a5008a46c2d1078a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe7ac5cb4f44e0db0a63da154a6f5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d65693814ab4b31ac21f59abdcf33d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a513d958c94397a67b702160a21f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/545 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84074c31e1044f6ba83cdf1368a388b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb1da171eae458180d18cc980b9d41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/319 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896dd54bc100408fad4824d95c1ec4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38624d81b1f847dc826cf80a584907e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b27e4b72f5e49c48353d2a719106e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714c590b298045e28082cc67539571a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL = 'msmarco-distilbert-base-v4'\n",
    "embedder = SentenceTransformer(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a toy corpus\n",
    "\n",
    "Let us now load a toy corpus of some simple, long texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:56.204028Z",
     "start_time": "2023-03-25T10:55:56.149807Z"
    }
   },
   "outputs": [],
   "source": [
    "%run NLP-Lesson-01___search-corpus.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search index of sentence embeddings\n",
    "\n",
    "Let us now create the search index of sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.047653Z",
     "start_time": "2023-03-25T10:55:56.205787Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = embedder.encode(sentences, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we chose to get the embeddings as `pytorch` tensors -- this will help us later in doing high-performance searches over the GPU/TPU hardware. What do these embeddings look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.055548Z",
     "start_time": "2023-03-25T10:55:58.050080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T15:19:59.964384Z",
     "start_time": "2023-03-18T15:19:59.950392Z"
    }
   },
   "source": [
    "Clearly, there are 16 embeddings, each of a 768 dimensional vector. Let us glance at a sentence, and its embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.112601Z",
     "start_time": "2023-03-25T10:55:58.059746Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "’Twas brillig, and the slithy toves\n",
      "      Did gyre and gimble in the wabe:\n",
      "All mimsy were the borogoves,\n",
      "      And the mome raths outgrabe.\n",
      "\n",
      "“Beware the Jabberwock, my son!\n",
      "      The jaws that bite, the claws that catch!\n",
      "Beware the Jubjub bird, and shun\n",
      "      The frumious Bandersnatch!”\n",
      "\n",
      "He took his vorpal sword in hand;\n",
      "      Long time the manxome foe he sought—\n",
      "So rested he by the Tumtum tree\n",
      "      And stood awhile in thought.\n",
      "\n",
      "And, as in uffish thought he stood,\n",
      "      The Jabberwock, with eyes of flame,\n",
      "Came whiffling through the tulgey wood,\n",
      "      And burbled as it came!\n",
      "\n",
      "One, two! One, two! And through and through\n",
      "      The vorpal blade went snicker-snack!\n",
      "He left it dead, and with its head\n",
      "      He went galumphing back.\n",
      "\n",
      "“And hast thou slain the Jabberwock?\n",
      "      Come to my arms, my beamish boy!\n",
      "O frabjous day! Callooh! Callay!”\n",
      "      He chortled in his joy.\n",
      "\n",
      "’Twas brillig, and the slithy toves\n",
      "      Did gyre and gimble in the wabe:\n",
      "All mimsy were the borogoves,\n",
      "      And the mome raths outgrabe.\n",
      "\n",
      "  tensor([ 1.7917e-02,  3.2575e-01, -5.2674e-01, -4.5594e-01, -1.9075e-01,\n",
      "         1.2122e-01,  6.5697e-01,  3.6187e-01, -3.1310e-01,  4.4082e-01,\n",
      "         1.4342e-02,  2.6244e-01,  5.3394e-01,  1.0590e-01, -4.5185e-01,\n",
      "         1.2903e-01, -6.4802e-02, -1.5091e-01,  7.3926e-01, -5.1614e-01,\n",
      "         2.7852e-01,  2.7217e-02,  1.5634e-01,  3.4818e-01,  5.5325e-01,\n",
      "         3.8035e-01,  1.8087e-01,  4.8250e-01, -2.6185e-01, -1.3253e-01,\n",
      "        -3.6970e-01,  3.3219e-01,  1.6746e-01,  2.1852e-02,  2.8724e-01,\n",
      "         7.9400e-02, -3.2609e-01, -2.0288e-01, -5.9856e-01,  8.8381e-02,\n",
      "         2.5152e-01,  3.5598e-01,  4.8215e-02, -7.2605e-02,  1.9663e-01,\n",
      "        -3.8163e-02,  4.1078e-01,  1.4597e-01,  3.5012e-01, -4.9210e-01,\n",
      "         4.4801e-01,  9.7348e-03,  5.4350e-01, -2.7849e-01, -2.8385e-01,\n",
      "        -7.2805e-02,  2.6930e-01, -9.2224e-03,  2.5226e-01,  3.9880e-01,\n",
      "        -4.8472e-01, -5.0370e-01,  2.1162e-01,  4.4003e-01, -5.7341e-01,\n",
      "         3.1301e-01, -1.4083e-01,  2.2410e-01, -4.4437e-01, -1.8528e-01,\n",
      "         1.4232e-01, -4.4814e-01, -5.3232e-02,  1.1122e-01, -3.7606e-02,\n",
      "        -3.6722e-01, -7.7720e-03, -4.3455e-01, -1.4202e-01,  1.2595e-01,\n",
      "        -7.6096e-02, -2.7487e-01,  2.0302e-01,  4.0621e-01, -4.9197e-02,\n",
      "        -7.9096e-01,  2.9097e-01,  6.0802e-01, -3.7323e-01,  2.4245e-01,\n",
      "        -6.5203e-02, -5.9707e-01, -1.9272e-01,  3.3305e-01, -2.1034e-01,\n",
      "         2.2604e-01, -5.0660e-01,  2.1707e-01,  1.8715e-01,  3.8112e-01,\n",
      "         2.5473e-01,  1.6069e-01,  2.1520e-01,  2.9177e-01, -4.9131e-01,\n",
      "         4.0516e-02, -2.2139e-01, -4.2589e-02,  1.2716e-01,  1.5987e-01,\n",
      "         2.5544e-01, -2.9770e-01, -1.3194e-01,  1.5549e-02,  5.0884e-01,\n",
      "         4.0749e-01, -2.1452e-01, -3.0160e-01, -3.1604e-01, -1.7183e-01,\n",
      "         8.1062e-01, -5.2953e-02,  2.7273e-01, -5.1342e-01,  4.2006e-01,\n",
      "        -3.6531e-01, -7.3516e-02, -1.2260e-02, -1.7089e-01, -1.3319e-01,\n",
      "         1.3879e-01,  9.8377e-01,  3.1317e-02,  1.2674e-01,  6.2551e-02,\n",
      "         1.0961e-02,  5.3247e-01, -1.7293e-01, -4.7855e-01, -1.8883e-01,\n",
      "        -3.1886e-02,  2.3250e-02, -6.8862e-02,  6.1711e-01,  3.1547e-01,\n",
      "        -1.0787e-01, -4.6169e-01, -3.2052e-01, -9.7469e-02,  5.3235e-01,\n",
      "        -1.9616e-01,  3.4646e-02, -3.6577e-01,  2.9087e-01,  2.2671e-01,\n",
      "         2.1960e-01,  1.8154e-01,  4.0864e-01,  3.7546e-01,  1.1521e-01,\n",
      "        -3.8926e-01, -1.4973e-01,  7.6049e-02, -6.7651e-02,  1.7376e-01,\n",
      "        -1.3123e-01, -3.1742e-01, -2.8624e-01, -1.5841e-02, -1.4196e-01,\n",
      "         1.0881e-01, -5.8047e-01,  4.4365e-01,  2.4115e-01,  5.4083e-02,\n",
      "        -5.3161e-01,  1.6474e-01, -2.9610e-01, -4.0890e-01, -2.1082e-01,\n",
      "        -5.2382e-01, -5.5760e-01,  1.2509e-01, -1.3824e-02,  4.2349e-02,\n",
      "         2.1822e-01, -7.5218e-02, -2.3979e-01, -5.7186e-01,  8.4367e-02,\n",
      "        -1.2576e-01, -4.2887e-02,  5.5928e-01,  1.0654e-01,  8.9992e-02,\n",
      "        -2.3590e-01,  1.9082e-01, -5.8835e-01,  1.7601e-01, -3.4115e-01,\n",
      "         2.2821e-02, -5.3441e-02, -3.5236e-01,  1.1238e-01,  6.1241e-02,\n",
      "         1.2119e-01, -3.2559e-01,  2.0884e-01,  3.3278e-02,  9.3341e-03,\n",
      "         4.4278e-01, -1.8672e-01, -1.2111e-01, -5.6136e-02, -1.2680e-01,\n",
      "         1.6150e-01,  2.3153e-01,  1.8705e-01,  1.1947e-01, -2.5572e-01,\n",
      "        -9.2670e-01, -6.9634e-02,  8.8565e-02, -5.9111e-01,  2.7490e-01,\n",
      "         8.0624e-01, -3.2596e-01, -6.9462e-02, -6.4609e-01,  1.6149e-01,\n",
      "        -3.6439e-01,  3.9385e-02, -7.1675e-02,  1.9714e-01,  4.1333e-01,\n",
      "        -2.1130e-01, -2.6286e-01, -2.9745e-01, -1.6346e-01, -1.1724e-01,\n",
      "        -4.7544e-01, -4.3545e-01,  3.7929e-02,  3.0695e-01, -1.3773e-02,\n",
      "        -2.0251e-02,  4.0708e-01, -1.1445e-03,  3.1819e-01,  1.8085e-01,\n",
      "         4.6539e-01,  2.1454e-02, -4.0807e-02, -2.8014e-01, -2.7622e-01,\n",
      "         6.6179e-02, -1.8675e-01, -7.5373e-01,  2.1418e-02,  3.1014e-01,\n",
      "         3.9646e-01,  4.0270e-01,  4.3578e-01,  1.8946e-01, -5.6522e-02,\n",
      "         7.1969e-01,  2.6049e-01,  1.7067e-01,  2.4791e-01, -2.3705e-01,\n",
      "        -5.5931e-02,  1.7301e-02,  1.6413e-01,  4.0594e-01,  1.6504e-01,\n",
      "         7.5399e-02, -1.1570e-01, -2.4189e-01, -7.6809e-02,  6.3858e-02,\n",
      "        -3.0081e-01,  7.4117e-01, -6.9915e-01, -1.4235e-01, -1.9157e-01,\n",
      "        -8.2040e-02,  5.7404e-01,  3.2158e-01,  2.7205e-01, -2.1700e-01,\n",
      "         2.9987e-02,  7.9279e-01,  6.1427e-01, -5.1989e-02, -5.8224e-01,\n",
      "        -8.6446e-04,  1.5717e-01,  1.0860e-01,  2.2685e-01,  3.0562e-01,\n",
      "        -2.3346e-01,  1.5908e-01, -5.7816e-01,  1.6771e-01, -3.8282e-01,\n",
      "        -2.4983e-01, -1.9433e-01, -1.5392e-01, -1.7070e+00,  2.6598e-01,\n",
      "         1.1801e-01, -2.2074e-01,  6.3294e-01,  3.6595e-01, -9.0836e-02,\n",
      "         1.1078e-01,  5.9200e-03, -1.8070e-02, -2.4574e-01, -5.4478e-01,\n",
      "         6.0092e-01, -2.2873e-01,  2.3063e-01, -6.8054e-01, -2.9629e-01,\n",
      "        -3.2019e-01,  3.4550e-01, -6.7519e-01,  1.9262e-01,  2.3377e-04,\n",
      "         9.4906e-02, -5.4170e-01, -2.9485e-01,  1.7788e-01,  4.3582e-01,\n",
      "        -1.5583e-01, -4.0829e-02,  1.7331e-01, -2.2900e-01,  4.2604e-01,\n",
      "         2.2739e-01,  2.9902e-02,  1.2950e-02, -3.8811e-02,  5.0931e-01,\n",
      "        -6.9911e-02, -6.8597e-02, -1.0675e-01, -6.8135e-03,  2.3347e-01,\n",
      "        -5.0954e-01,  2.2618e-01,  3.1839e-01, -4.5075e-02, -5.2840e-01,\n",
      "        -3.6181e-01,  6.9245e-01, -4.7528e-03, -3.2137e-01, -2.2106e-01,\n",
      "        -2.1965e-01, -1.4922e-01,  1.5394e-01, -3.2444e-02, -1.9035e-01,\n",
      "        -7.6166e-03,  5.0208e-02, -5.5047e-01, -3.2600e-02,  2.8282e-01,\n",
      "         2.7266e-02, -2.8430e-01,  4.8848e-02,  3.3134e-01,  3.7561e-02,\n",
      "         2.4394e-01,  8.3382e-03, -1.2271e-02, -1.4590e-02, -3.1226e-01,\n",
      "        -4.0161e-02, -5.0416e-01,  3.6569e-01,  1.0153e+00,  1.9811e-02,\n",
      "         7.8787e-04,  2.7361e-01, -4.2070e-01, -3.8222e-01, -4.2397e-01,\n",
      "         2.5626e-01, -5.2781e-01, -3.4484e-01, -5.6268e-01,  1.2262e-01,\n",
      "        -3.9935e-01,  5.1999e-02, -5.5470e-01,  8.0129e-02, -2.2590e-02,\n",
      "        -7.6713e-01,  7.7694e-01,  3.2428e-01,  1.9949e-01,  5.5646e-01,\n",
      "         2.2863e-02,  6.8620e-02,  4.1802e-01,  3.9969e-01, -6.4930e-02,\n",
      "        -3.4573e-02, -7.8673e-02,  4.0344e-01, -1.6961e-01, -2.0253e-03,\n",
      "        -3.9754e-02, -2.6265e-01,  2.7204e-01,  1.5871e-02, -2.3830e-01,\n",
      "         1.8041e-01,  5.5454e-01, -5.0322e-01, -2.1440e-01, -3.6038e-01,\n",
      "        -2.8141e-01,  2.3926e-01,  1.5952e-01,  1.3700e-01,  1.9691e-01,\n",
      "         1.0487e-01,  1.2944e-01,  1.6427e-01, -4.3116e-01, -1.4120e-01,\n",
      "        -1.9606e-01, -2.3791e-01, -2.8282e-01, -3.0180e-01, -2.0097e-01,\n",
      "        -2.8438e-01, -7.9645e-02,  4.1995e-01,  5.7060e-02, -7.0792e-01,\n",
      "        -6.1364e-01,  2.9859e-01, -3.2533e-01, -7.5629e-02, -3.6611e-02,\n",
      "         4.6359e-01,  2.8782e-01,  3.4186e-02,  3.3679e-01,  1.0766e-01,\n",
      "        -1.0526e-01, -2.7351e-01,  3.6651e-01, -5.9686e-01,  5.0702e-02,\n",
      "         2.0827e-01, -3.3547e-01,  1.3994e-01,  7.5079e-02, -2.9489e-01,\n",
      "        -1.8911e-01,  4.0929e-01,  5.2650e-01, -8.5071e-02,  3.8760e-01,\n",
      "         2.3798e-01,  7.0253e-01,  1.3223e-01, -4.7596e-01,  1.5318e-01,\n",
      "        -2.2118e-01, -2.0028e-02, -1.9628e-01, -2.6742e-01,  5.7637e-02,\n",
      "        -1.6170e-01,  8.3750e-01, -3.9614e-01,  4.1609e-01, -3.2807e-02,\n",
      "         5.1721e-02, -4.4778e-01,  3.5607e-01,  1.9781e-01,  3.0608e-01,\n",
      "        -4.2100e-01,  4.2799e-01,  3.2833e-01, -1.2995e-01, -2.2661e-01,\n",
      "        -1.9074e-01, -5.5467e-01,  2.6481e-01, -2.7469e-01,  5.6599e-02,\n",
      "         4.0208e-01, -1.7044e-01, -5.2993e-01,  6.1364e-02, -2.6243e-01,\n",
      "        -2.6288e-02, -2.4756e-01, -7.5067e-02,  4.9017e-01, -2.1801e-01,\n",
      "        -1.2747e-01, -3.4201e-02, -2.5892e-01, -3.5188e-01, -6.6999e-02,\n",
      "        -4.2172e-01,  1.4296e-01, -4.1183e-01, -9.4315e-04, -6.4984e-01,\n",
      "        -1.2443e-01, -2.2977e-02,  3.1080e-01, -5.3202e-01,  9.2309e-02,\n",
      "         5.1738e-01,  1.6357e-01,  4.1306e-02, -7.2864e-02, -1.8374e-01,\n",
      "        -9.1456e-02,  8.7033e-02, -5.2140e-01,  8.4393e-01, -2.6657e-01,\n",
      "         4.1124e-01,  2.2357e-01, -1.9583e-01,  3.4919e-01, -6.0420e-01,\n",
      "        -3.4275e-01,  1.7922e-01,  7.1702e-01, -6.5519e-02,  3.3226e-01,\n",
      "         1.9494e-01,  3.7074e-01,  4.8286e-01,  8.6542e-01,  1.9256e-01,\n",
      "         1.8796e-02,  8.1855e-01, -4.3893e-01,  2.1546e-01, -3.0848e-01,\n",
      "         2.5845e-01, -2.6430e-01,  2.7694e-01,  1.3315e-01,  9.5527e-01,\n",
      "         1.5193e-01, -1.4842e-01,  3.9241e-01,  3.5782e-01, -5.9792e-01,\n",
      "         1.1726e-01,  1.3109e-01, -3.5268e-01, -3.0282e-01,  4.1248e-02,\n",
      "         3.6656e-01, -1.8282e-02,  9.7258e-03, -2.4284e-01,  1.1514e-01,\n",
      "         1.3559e-01, -1.8612e-01,  3.1848e-01,  1.9259e-01, -3.0652e-01,\n",
      "        -5.2179e-01, -1.8819e-01, -1.0093e-01,  2.0414e-01, -3.2664e-01,\n",
      "        -3.6530e-01,  1.5873e-01, -5.5727e-01, -1.9240e-01,  4.5441e-02,\n",
      "        -1.7976e-01, -3.1875e-01, -7.1592e-01,  2.6830e-02,  2.9293e-01,\n",
      "         2.5408e-02, -9.0681e-02, -3.0221e-01,  1.2896e-01, -4.5249e-02,\n",
      "        -2.7396e-01,  2.4500e-02,  1.0130e-01,  1.0020e-01,  2.7701e-01,\n",
      "        -3.0467e-01, -1.0260e-01,  2.8172e-01,  2.3085e-02,  5.7835e-02,\n",
      "         6.0795e-01, -6.0961e-01, -2.4893e-02, -5.0576e-02,  5.5405e-02,\n",
      "         1.2906e-02, -2.0817e-01,  2.4534e-01, -2.2500e-01,  2.9426e-02,\n",
      "        -2.7480e-01,  7.4730e-01,  1.8980e-01,  2.5081e-01,  1.5851e-01,\n",
      "        -4.0235e-01, -1.6969e-01, -3.7136e-01,  7.6890e-02, -8.5676e-01,\n",
      "         1.2352e-01, -2.4541e-01,  1.3273e-01,  1.0321e-03, -1.5147e-01,\n",
      "         2.7888e-02, -2.3935e-02, -3.5993e-01, -3.2951e-01,  2.2789e-01,\n",
      "         2.5615e-02, -4.4394e-02, -1.2734e-01,  5.5925e-01,  2.4509e-01,\n",
      "        -3.9563e-01,  4.0246e-01,  8.5918e-02,  2.3930e-01,  5.5555e-02,\n",
      "         1.5976e-01,  3.1305e-01, -6.9706e-02, -2.4159e-01,  5.3633e-01,\n",
      "         2.5338e-02,  2.1870e-01,  7.3817e-02, -5.9284e-01,  3.9119e-01,\n",
      "        -2.6954e-01, -3.6438e-03,  4.2445e-01, -3.6671e-01, -2.2968e-01,\n",
      "         2.2761e-01,  2.0409e-01,  9.7982e-02,  3.0123e-01,  4.2016e-01,\n",
      "        -2.9227e-01,  1.8621e-01, -7.0845e-02,  5.2205e-02, -6.8265e-01,\n",
      "         2.2897e-01, -1.5891e-02,  2.4238e-01,  1.0389e+00,  6.2297e-01,\n",
      "         3.1147e-01, -3.1041e-01, -6.5775e-01,  8.6018e-02, -3.3830e-02,\n",
      "        -7.7502e-02, -1.6662e-01, -3.8203e-01,  5.9171e-02,  3.4376e-01,\n",
      "        -1.3236e-01,  3.9207e-01,  4.5186e-01, -2.7197e-01, -4.3461e-02,\n",
      "        -1.0712e-02,  1.9661e-01, -3.6862e-01,  3.2772e-02, -3.8254e-01,\n",
      "        -2.6337e-01,  7.3939e-02, -2.2048e-01, -3.3578e-01,  3.9128e-01,\n",
      "         5.8405e-01, -9.8725e-02,  6.2789e-01,  3.0186e-01,  2.6917e-01,\n",
      "        -6.3461e-01,  2.4154e-01,  4.7397e-01,  3.4264e-01,  6.6855e-02,\n",
      "        -1.0252e-01,  8.5943e-02, -1.4061e-01,  5.3254e-01,  5.5319e-01,\n",
      "        -3.1107e-03, -5.7683e-02, -9.4840e-02, -5.5374e-01, -1.0431e-01,\n",
      "        -6.4141e-01, -3.6586e-01,  3.1125e-02, -4.1912e-01, -8.1661e-02,\n",
      "         1.0081e-01,  4.0771e-02, -1.4054e-01, -1.4227e-01,  1.3890e-01,\n",
      "         3.7653e-01,  1.2723e-01, -4.6420e-01,  1.1746e-01,  7.3600e-02,\n",
      "         5.3363e-01, -6.5410e-01,  4.5491e-01,  3.2288e-01, -4.9433e-01,\n",
      "        -2.1253e-01,  2.3369e-01, -7.7339e-01,  5.9050e-01, -4.0324e-01,\n",
      "         3.8141e-01,  3.2117e-01, -9.5685e-01, -4.8296e-01,  5.1497e-01,\n",
      "        -2.8550e-02,  1.2886e-01, -2.1897e-02,  1.9965e-02,  5.5359e-02,\n",
      "        -4.9937e-01,  4.3293e-01,  2.4391e-01,  2.6040e-01, -4.2517e-02,\n",
      "         5.2210e-01,  4.4372e-01,  8.3467e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (f'{sentences[0]}  {embeddings[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, search for something!\n",
    "\n",
    "Let us find the closest match to the the query: \"a friendship with animals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.126169Z",
     "start_time": "2023-03-25T10:55:58.114669Z"
    }
   },
   "outputs": [],
   "source": [
    "query_text = \"a friendship with animals\"\n",
    "query = embedder.encode(query_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.134626Z",
     "start_time": "2023-03-25T10:55:58.127819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 7, 'score': 0.2550491690635681},\n",
       "  {'corpus_id': 8, 'score': 0.23244604468345642},\n",
       "  {'corpus_id': 5, 'score': 0.21887624263763428}]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "search_results = util.semantic_search(query, embeddings, top_k = 3)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:55:58.139477Z",
     "start_time": "2023-03-25T10:55:58.136192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Search Rank: 0, Relevance score: 0.2550491690635681 \n",
      "\n",
      "Golden retrievers are not bred to be guard dogs, and considering the size of their hearts and their irrepressible joy and life, they are less likely to bite than to bark, less likely to bark than to lick a hand in greeting. In spite of their size, they think they are lap dogs, and in spite of being dogs, they think they’re also human, and nearly every human they meet is judged to have the potential to be a boon companion who might at any moment, cry, “Let’s go!” and lead them on a great adventure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search Rank: 1, Relevance score: 0.23244604468345642 \n",
      "\n",
      "If you’re lucky, a golden retriever will come into your life, steal your heart, and change everything\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search Rank: 2, Relevance score: 0.21887624263763428 \n",
      "If a man aspires towards a righteous life, his first act of abstinence is from injury to animals.\n"
     ]
    }
   ],
   "source": [
    "for index, result in enumerate(search_results[0]):\n",
    "    print('-'*80)\n",
    "    print(f'Search Rank: {index}, Relevance score: {result[\"score\"]} ')\n",
    "    print(sentences[result['corpus_id']])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-18T15:38:16.470854Z",
     "start_time": "2023-03-18T15:38:16.463869Z"
    }
   },
   "source": [
    "## Visual Search\n",
    "\n",
    "Let us now search by giving it an image of what we are looking for. How can we do this?\n",
    "\n",
    "We need to translate an image into an embedding vector, in a semantically relevant manner. If we were to be able to do it, we can then search through our document embeddings in essentially  the same manner as we did for textual search.\n",
    "\n",
    "We start by getting an image from the web:\n",
    "\n",
    "<figure style=\"padding:30px\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/At_Sticker_Lumis_Golden.JPG/1920px-At_Sticker_Lumis_Golden.JPG\">\n",
    "    <caption> A golden retriever image from wikipedia </caption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:56:13.761887Z",
     "start_time": "2023-03-25T10:55:58.141135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8176dea9d1f94556a4ee4d28db1b88a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b05a642351744dc8e361df91f2952bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a812fd43754774b93c18adb757b441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/1.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09a78395ffc40c4b51bfb3937b702fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c96f3f1272499cb9b1a58eeda64b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 0_CLIPModel/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05791e73c1d64117984dd698fb664eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fa87a3311443ee81dd0dcb52616dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/604 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf940dd8f99b45af9bd7d7f4a973ba84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 0_CLIPModel/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa01ab22191e47d59d460a447c5cc7ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d4bd2f5eeb40e7b07eec5c57281647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)LIPModel/config.json:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Create the model\n",
    "CLIP_MODEL = 'clip-ViT-B-32'\n",
    "embedder = SentenceTransformer(CLIP_MODEL)\n",
    "\n",
    "# We need to take smaller text, for CLIP to work. (current limitation)\n",
    "short_sentences = [\n",
    "    'A smiling dog', 'House with a chimney', 'Car on a highway',\n",
    "    'Elephant in the field', 'Attention is all you need',\n",
    "    'Golden retrievers are little children', 'A cat on the window sill',\n",
    "    'For the love of these furry dogs'\n",
    "]\n",
    "# Create the search index of embeddings\n",
    "embeddings = embedder.encode(short_sentences, convert_to_tensor=True)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image as query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:56:14.445088Z",
     "start_time": "2023-03-25T10:56:13.763934Z"
    }
   },
   "outputs": [],
   "source": [
    "query = embedder.encode(Image.open('images/dog.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:56:14.451712Z",
     "start_time": "2023-03-25T10:56:14.446700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "search_results = util.semantic_search(query, embeddings, top_k = 3)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:56:14.457464Z",
     "start_time": "2023-03-25T10:56:14.453522Z"
    }
   },
   "outputs": [],
   "source": [
    "for index, result in enumerate(search_results[0]):\n",
    "    print('-'*80)\n",
    "    print(f'Search Rank: {index}, Relevance score: {result[\"score\"]} ')\n",
    "    print(short_sentences[result['corpus_id']])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text as query\n",
    "\n",
    "We can also do the converse: give a text query, and retrieve all images that match. \n",
    "\n",
    "<div class=\"alert-box alert-info\" style=\"padding-top:30px\">\n",
    "   \n",
    "**Attribution**\n",
    "    \n",
    ">  The following example is derived from, and inspired by, the sample jupyter notebook posted at:  https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/image-search/Image_Search.ipynb\n",
    "</div>\n",
    "\n",
    "Let us first download a collection of photos from the Unsplash website of free available photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:58:10.401909Z",
     "start_time": "2023-03-25T10:56:14.458799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taken almost verbatim from the above-mentioned resource.\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "img_folder = 'photos/'\n",
    "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "    \n",
    "    photo_filename = 'unsplash-25k-photos.zip'\n",
    "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
    "        \n",
    "    #Extract all images\n",
    "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
    "            zf.extract(member, img_folder)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the search index of these images\n",
    "\n",
    "Let us now create a search index of these images as a collection of sentence embeddings. Since it is rather computationally expensive and time-consuming to create these embeddings, let us also store it for repeated use. \n",
    "\n",
    "Of-course, this implies that the next time you run this cell, it will retrieve the pre-computed embeddings, rather than recreate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:58:18.022242Z",
     "start_time": "2023-03-25T10:58:10.404001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Once again, this code is taken from the sample notebook mentioned above.\n",
    "\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "use_precomputed_embeddings = True\n",
    "\n",
    "if use_precomputed_embeddings:\n",
    "    emb_filename = 'unsplash-25k-photos-embeddings.pkl'\n",
    "    if not os.path.exists(emb_filename):  #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/' + emb_filename,\n",
    "                      emb_filename)\n",
    "\n",
    "    with open(emb_filename, 'rb') as fIn:\n",
    "        img_names, img_emb = pickle.load(fIn)\n",
    "    print(\"Images:\", len(img_names))\n",
    "else:\n",
    "    img_names = list(glob.glob('unsplash/photos/*.jpg'))\n",
    "    print(\"Images:\", len(img_names))\n",
    "    img_emb = model.encode([Image.open(filepath) for filepath in img_names],\n",
    "                           batch_size=128,\n",
    "                           convert_to_tensor=True,\n",
    "                           show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image search with prompts\n",
    "\n",
    "Now we can search in a manner exactly the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T10:58:22.299246Z",
     "start_time": "2023-03-25T10:58:18.024233Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipyplot\n",
    "import ipyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T11:28:52.954612Z",
     "start_time": "2023-03-25T11:28:52.948357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let us define a search function\n",
    "from typing import Union\n",
    "def search(query: str, is_image: bool = False, k:int = 8):    \n",
    "    raw = Image.open(query) if is_image else query\n",
    "    query_emb = model.encode([raw], convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    # Then, we use the util.semantic_search function, which computes the cosine-similarity\n",
    "    # between the query embedding and all image embeddings.\n",
    "    # It then returns the top_k highest ranked images, which we output\n",
    "    hits = util.semantic_search(query_emb, img_emb, top_k=k)[0]\n",
    "    \n",
    "    print(\"Query:\")\n",
    "    display(raw) if is_image else display(query)\n",
    "    images = [os.path.join(img_folder, img_names[hit['corpus_id']]) for hit in hits]\n",
    "    ipyplot.plot_images(images, max_images=30, img_width=150, show_url=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T11:27:57.952317Z",
     "start_time": "2023-03-25T11:27:57.907369Z"
    }
   },
   "outputs": [],
   "source": [
    "search('To love a dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T11:27:58.718632Z",
     "start_time": "2023-03-25T11:27:58.679588Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search('House near a lake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T11:27:59.401008Z",
     "start_time": "2023-03-25T11:27:59.361939Z"
    }
   },
   "outputs": [],
   "source": [
    "search('jumping dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T11:28:00.721925Z",
     "start_time": "2023-03-25T11:28:00.682287Z"
    }
   },
   "outputs": [],
   "source": [
    "search('Pathways through the forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Perform a search, where the query is an image, and the results are images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T11:29:21.325381Z",
     "start_time": "2023-03-25T11:29:20.589066Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dog = 'images/dog.jpeg'\n",
    "search(dog, k=10, is_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    "Further reading resources associated with the two topics: sentence-transformers and approximate nearest neighbor searches.\n",
    "<p>\n",
    "\n",
    "* <a href=\"https://sbert.net/\">Sentence Transformers</a> \n",
    "\n",
    "* The original paper that introduced sentence-transformers: <a href=\"https://arxiv.org/abs/1908.10084\">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a>\n",
    "\n",
    "* Faiss blog at facebook <a href=\"https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\"> Faiss: A library for efficient similarity search </a>\n",
    "\n",
    "* A gentler introduction to Faiss: <a href=\"https://github.com/facebookresearch/faiss/wiki/\"> Faiss wiki </a>\n",
    "\n",
    "* Faiss <a href=\"https://github.com/facebookresearch/faiss\">The Faiss github repository </a>\n",
    "\n",
    "* Approximate nearest neighbor search with ScaNN <a href=\"https://github.com/google-research/google-research/tree/master/scann\"> Scann github repository</a>\n",
    "\n",
    "* Scann research paper: <a href=\"https://arxiv.org/abs/1908.10396\">  Accelerating Large-Scale Inference with Anisotropic Vector Quantization </a>\n",
    "    \n",
    "* Topic modeling: <a href=\"https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\">Topic modeling with BERT </a>\n",
    "    \n",
    "* To2Vec research paper: <a  href=\"https://arxiv.org/abs/2008.09470\">Top2Vec: Distributed Representations of Topics</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T13:00:54.733152Z",
     "start_time": "2023-03-25T13:00:49.387287Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install bertopic  #Do it only once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T13:01:16.377528Z",
     "start_time": "2023-03-25T13:01:15.035991Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_20newsgroups\n\u001b[1;32m      4\u001b[0m docs \u001b[38;5;241m=\u001b[39m fetch_20newsgroups(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m,  remove\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfooters\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquotes\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[1;32m      7\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic()\n\u001b[1;32m      8\u001b[0m topics, probs \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mfit_transform(docs)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "# Fetch the famous 20-newsgroup data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T12:59:55.911255Z",
     "start_time": "2023-03-25T12:59:18.653491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.14.1-py2.py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: plotly>=4.7.0 in /home/asif/.local/lib/python3.9/site-packages (from bertopic) (5.13.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from bertopic) (1.21.6)\n",
      "Collecting hdbscan>=0.8.29\n",
      "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: umap-learn>=0.5.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from bertopic) (4.64.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/asif/anaconda3/lib/python3.9/site-packages (from bertopic) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /home/asif/anaconda3/lib/python3.9/site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from bertopic) (2.2.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in /home/asif/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.32)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/asif/.local/lib/python3.9/site-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
      "Requirement already satisfied: sentencepiece in /home/asif/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.97)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.27.1)\n",
      "Requirement already satisfied: torchvision in /home/asif/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.15.1)\n",
      "Requirement already satisfied: nltk in /home/asif/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/asif/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.8)\n",
      "Requirement already satisfied: numba>=0.49 in /home/asif/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: filelock in /home/asif/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/asif/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/asif/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.3.0)\n",
      "Requirement already satisfied: requests in /home/asif/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /home/asif/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (60.10.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/asif/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/asif/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.10.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.7.91)\n",
      "Requirement already satisfied: jinja2 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
      "Requirement already satisfied: networkx in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/asif/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (11.7.101)\n",
      "Requirement already satisfied: wheel in /home/asif/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/asif/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.26.0)\n",
      "Requirement already satisfied: lit in /home/asif/anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (15.0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/asif/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.11.10)\n",
      "Requirement already satisfied: click in /home/asif/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/asif/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/asif/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/asif/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/asif/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/asif/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/asif/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/asif/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (1.2.1)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp39-cp39-linux_x86_64.whl size=744970 sha256=3854e88e44fd1ee04498a8a25a722dc4514916ba336b2ed496085368ad8acc93\n",
      "  Stored in directory: /home/asif/.cache/pip/wheels/05/6f/88/1a4c04276b98306f00217a1e300e6ba0252c6aa4f7616067ae\n",
      "Successfully built hdbscan\n",
      "Installing collected packages: hdbscan, bertopic\n",
      "Successfully installed bertopic-0.14.1 hdbscan-0.8.29\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T12:58:10.409671Z",
     "start_time": "2023-03-25T12:58:04.372337Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8404/1982228763.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mumap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mumap_\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m umap_embeddings = UMAP(n_neighbors=15, \n\u001b[0m\u001b[1;32m      3\u001b[0m                             \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             metric='cosine').fit_transform(embeddings)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "from umap import umap_ as UMAP\n",
    "umap_embeddings = UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-25T12:50:14.327044Z",
     "start_time": "2023-03-25T12:50:14.309680Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8404/888121535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n\u001b[1;32m      3\u001b[0m                           \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           cluster_selection_method='eom').fit(umap_embeddings)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "import hdbscan\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
